{"cells":[{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.linalg.Vectors\nval denseVec = Vectors.dense(1.0, 2.0, 3.0)\nval size = 3\nval idx = Array(1,2) // locations in vector\nval values = Array(2.0,3.0)\nval sparseVec = Vectors.sparse(size, idx, values)\nsparseVec.toDense\ndenseVec.toSparse\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["\nfrom pyspark.ml.linalg import Vectors\ndenseVec = Vectors.dense(1.0, 2.0, 3.0)\nsize = 3\nidx = [1, 2] # locations in vector\nvalues = [2.0, 3.0]\nsparseVec = Vectors.sparse(size, idx, values)\n# sparseVec.toDense() # these two don't work, not sure why\n# denseVec.toSparse() # will debug later"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%scala\nvar df = spark.read.json(\"/FileStore/tables/ixxwee3q1497648769294/part_r_00000_f5c243b9_a015_4a3b_a4a8_eca00f80f04c-a8b89.json\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df = spark.read.json(\"/FileStore/tables/ixxwee3q1497648769294/part_r_00000_f5c243b9_a015_4a3b_a4a8_eca00f80f04c-a8b89.json\")\ndf.orderBy(\"value2\").show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sh cd ..\ncd ..\ncd dbfs/FileStore/tables/t4wp8ci61497648354438\nls"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%scala\nval libsvmData = spark.read.format(\"libsvm\")\n.load(\"FileStore/tables/t4wp8ci61497648354438/sample_libsvm_data.txt\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["libsvmData = spark.read.format(\"libsvm\")\\\n.load(\"FileStore/tables/t4wp8ci61497648354438/sample_libsvm_data.txt\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.RFormula\nval supervised = new RFormula()\n.setFormula(\"lab ~ . + color:value1 + color:value2\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.ml.feature import RFormula\nsupervised = RFormula()\\\n.setFormula(\"lab ~ . + color:value1 + color:value2\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%scala\nval fittedRF = supervised.fit(df)\nval preparedDF = fittedRF.transform(df)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["fittedRF = supervised.fit(df)\npreparedDF = fittedRF.transform(df)\npreparedDF.show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%scala\nval Array(train, test) = preparedDF.randomSplit(Array(0.7, 0.3))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["train, test = preparedDF.randomSplit([0.7, 0.3])"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.classification.LogisticRegression\nval lr = new LogisticRegression()\n.setLabelCol(\"label\")\n.setFeaturesCol(\"features\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression()\\\n.setLabelCol(\"label\")\\\n.setFeaturesCol(\"features\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%scala\nval fittedLR = lr.fit(train)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["fittedLR = lr.fit(train)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["fittedLR.transform(train).select(\"label\", \"prediction\").show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%scala\nval Array(train, test) = df.randomSplit(Array(0.7, 0.3))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["train, test = df.randomSplit([0.7, 0.3])"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%scala\nval rForm = new RFormula()\nval lr = new LogisticRegression()\n.setLabelCol(\"label\")\n.setFeaturesCol(\"features\")\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["rForm = RFormula()\n\nlr = LogisticRegression()\\\n.setLabelCol(\"label\")\\\n.setFeaturesCol(\"features\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.Pipeline\nval stages = Array(rForm, lr)\nval pipeline = new Pipeline().setStages(stages)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nstages = [rForm, lr]\npipeline = Pipeline().setStages(stages)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.tuning.ParamGridBuilder\nval params = new ParamGridBuilder()\n.addGrid(rForm.formula, Array(\n\"lab ~ . + color:value1\",\n\"lab ~ . + color:value1 + color:value2\"))\n.addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n.addGrid(lr.regParam, Array(0.1, 2.0))\n.build()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\nparams = ParamGridBuilder()\\\n.addGrid(rForm.formula, [\n\"lab ~ . + color:value1\",\n\"lab ~ . + color:value1 + color:value2\"])\\\n.addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n.addGrid(lr.regParam, [0.1, 2.0])\\\n.build()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nval evaluator = new BinaryClassificationEvaluator()\n.setMetricName(\"areaUnderROC\")\n.setRawPredictionCol(\"prediction\")\n.setLabelCol(\"label\")"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator()\\\n.setMetricName(\"areaUnderROC\")\\\n.setRawPredictionCol(\"prediction\")\\\n.setLabelCol(\"label\")"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.tuning.TrainValidationSplit\nval tvs = new TrainValidationSplit()\n.setTrainRatio(0.75) // also the default.\n.setEstimatorParamMaps(params)\n.setEstimator(pipeline)\n.setEvaluator(evaluator)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["from pyspark.ml.tuning import TrainValidationSplit\ntvs = TrainValidationSplit()\\\n.setTrainRatio(0.75)\\\n.setEstimatorParamMaps(params)\\\n.setEstimator(pipeline)\\\n.setEvaluator(evaluator)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["%scala\nval tvsFitted = tvs.fit(train)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["tvsFitted = tvs.fit(train)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["evaluator.evaluate(tvsFitted.transform(test))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.PipelineModel\nimport org.apache.spark.ml.classification.LogisticRegressionModel\nval trainedPipeline = tvsFitted.bestModel.asInstanceOf[PipelineModel]\nval TrainedLR = trainedPipeline.stages(1)\n.asInstanceOf[LogisticRegressionModel]\nval summaryLR = TrainedLR.summary\nsummaryLR.objectiveHistory"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%scala\ntvsFitted.write.overwrite().save(\"/tmp/modelLocation\")"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.tuning.TrainValidationSplitModel\nval model = TrainValidationSplitModel.load(\"/tmp/modelLocation\")\nmodel.transform(test)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%scala\nval sales = spark.read.format(\"csv\")\n.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.load(\"/FileStore/tables/1x42p66f1497639007910/*.csv\")\n.coalesce(5)\n.where(\"Description IS NOT NULL\")\nval fakeIntDF = spark.read.parquet(\"/FileStore/tables/w90rmm721497649161288/part_00000_ce2a44c8_feb4_4369_a2c3_4bf2f0e63b07_c000_gz-d65b9.parquet\")\nvar simpleDF = spark.read.json(\"/FileStore/tables/ixxwee3q1497648769294/part_r_00000_f5c243b9_a015_4a3b_a4a8_eca00f80f04c-a8b89.json\")\nval scaleDF = spark.read.parquet(\"/FileStore/tables/48se0cwp1497649233518/part_00000_cd03406a_cc9b_42b0_9299_1e259fdd9382_c000_gz-2e0be.parquet\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["sales = spark.read.format(\"csv\")\\\n.option(\"header\", \"true\")\\\n.option(\"inferSchema\", \"true\")\\\n.load(\"/FileStore/tables/1x42p66f1497639007910/*.csv\")\\\n.coalesce(5)\\\n.where(\"Description IS NOT NULL\")\nfakeIntDF = spark.read.parquet(\"/FileStore/tables/w90rmm721497649161288/part_00000_ce2a44c8_feb4_4369_a2c3_4bf2f0e63b07_c000_gz-d65b9.parquet\")\nsimpleDF = spark.read.json(\"/FileStore/tables/ixxwee3q1497648769294/part_r_00000_f5c243b9_a015_4a3b_a4a8_eca00f80f04c-a8b89.json\")\nscaleDF = spark.read.parquet(\"/FileStore/tables/48se0cwp1497649233518/part_00000_cd03406a_cc9b_42b0_9299_1e259fdd9382_c000_gz-2e0be.parquet\")\nsales.cache()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.Tokenizer\nval tkn = new Tokenizer().setInputCol(\"Description\")\ntkn.transform(sales).show()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.StandardScaler\nval ss = new StandardScaler().setInputCol(\"features\")\nss.fit(scaleDF).transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.RFormula\nval supervised = new RFormula()\n.setFormula(\"lab ~ . + color:value1 + color:value2\")\nsupervised.fit(simpleDF).transform(simpleDF).show()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["from pyspark.ml.feature import RFormula\nsupervised = RFormula()\\\n.setFormula(\"lab ~ . + color:value1 + color:value2\")\nsupervised.fit(simpleDF).transform(simpleDF).show()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.SQLTransformer\nval basicTransformation = new SQLTransformer()\n.setStatement(\"\"\"\nSELECT sum(Quantity), count(*), CustomerID\nFROM __THIS__\nGROUP BY CustomerID\n\"\"\")\nbasicTransformation.transform(sales).show()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["from pyspark.ml.feature import SQLTransformer\nbasicTransformation = SQLTransformer()\\\n.setStatement(\"\"\"\nSELECT sum(Quantity), count(*), CustomerID\nFROM __THIS__\nGROUP BY CustomerID\n\"\"\")\nbasicTransformation.transform(sales).show()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.VectorAssembler\nval va = new VectorAssembler()\n.setInputCols(Array(\"int1\", \"int2\", \"int3\"))\nva.transform(fakeIntDF).show()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nva = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\nva.transform(fakeIntDF).show()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.Tokenizer\nval tkn = new Tokenizer()\n.setInputCol(\"Description\")\n.setOutputCol(\"DescriptionOut\")\nval tokenized = tkn.transform(sales)\ntokenized.show()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer\ntkn = Tokenizer()\\\n.setInputCol(\"Description\")\\\n.setOutputCol(\"DescriptionOut\")\ntokenized = tkn.transform(sales)\ntokenized.show()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.RegexTokenizer\nval rt = new RegexTokenizer()\n.setInputCol(\"Description\")\n.setOutputCol(\"DescriptionOut\")\n.setPattern(\" \") // starting simple\n.setToLowercase(true)\nrt.transform(sales).show()"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer\nrt = RegexTokenizer()\\\n.setInputCol(\"Description\")\\\n.setOutputCol(\"DescriptionOut\")\\\n.setPattern(\" \")\\\n.setToLowercase(True)\nrt.transform(sales).show()"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.StopWordsRemover\nval englishStopWords = StopWordsRemover\n.loadDefaultStopWords(\"english\")\nval stops = new StopWordsRemover()\n.setStopWords(englishStopWords)\n.setInputCol(\"DescriptionOut\")\nstops.transform(tokenized).show()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\nenglishStopWords = StopWordsRemover\\\n.loadDefaultStopWords(\"english\")\nstops = StopWordsRemover()\\\n.setStopWords(englishStopWords)\\\n.setInputCol(\"DescriptionOut\")\nstops.transform(tokenized).show()"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.NGram\nval unigram = new NGram()\n.setInputCol(\"DescriptionOut\")\n.setN(1)\nval bigram = new NGram()\n.setInputCol(\"DescriptionOut\")\n.setN(2)\nunigram.transform(tokenized).show()\nbigram.transform(tokenized).show()"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.CountVectorizer\nval cv = new CountVectorizer()\n.setInputCol(\"DescriptionOut\")\n.setOutputCol(\"countVec\")\n.setVocabSize(500)\n.setMinTF(1)\n.setMinDF(2)\nval fittedCV = cv.fit(tokenized)\nfittedCV.transform(tokenized).show()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer\ncv = CountVectorizer()\\\n.setInputCol(\"DescriptionOut\")\\\n.setOutputCol(\"countVec\")\\\n.setVocabSize(500)\\\n.setMinTF(1)\\\n.setMinDF(2)\nfittedCV = cv.fit(tokenized)\nfittedCV.transform(tokenized).show()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["%scala\nval tfIdfIn = tokenized\n.where(\"array_contains(DescriptionOut, 'red')\")\n.select(\"DescriptionOut\")\n.limit(10)\ntfIdfIn.show(false)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["tfIdfIn = tokenized\\\n.where(\"array_contains(DescriptionOut, 'red')\")\\\n.select(\"DescriptionOut\")\\\n.limit(10)\ntfIdfIn.show(10, False)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.{HashingTF, IDF}\nval tf = new HashingTF()\n.setInputCol(\"DescriptionOut\")\n.setOutputCol(\"TFOut\")\n.setNumFeatures(10000)\nval idf = new IDF()\n.setInputCol(\"TFOut\")\n.setOutputCol(\"IDFOut\")\n.setMinDocFreq(2)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF\ntf = HashingTF()\\\n.setInputCol(\"DescriptionOut\")\\\n.setOutputCol(\"TFOut\")\\\n.setNumFeatures(10000)\nidf = IDF()\\\n.setInputCol(\"TFOut\")\\\n.setOutputCol(\"IDFOut\")\\\n.setMinDocFreq(2)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["%scala\nidf.fit(tf.transform(tfIdfIn))\n.transform(tf.transform(tfIdfIn))\n.show(false)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["idf.fit(tf.transform(tfIdfIn))\\\n.transform(tf.transform(tfIdfIn))\\\n.show(10, False)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["%scala\nval contDF = spark.range(500)\n.selectExpr(\"cast(id as double)\")"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["contDF = spark.range(500)\\\n.selectExpr(\"cast(id as double)\")"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.Bucketizer\nval bucketBorders = Array(-1.0, 5.0, 10.0, 250.0, 600.0)\nval bucketer = new Bucketizer()\n.setSplits(bucketBorders)\n.setInputCol(\"id\")\nbucketer.transform(contDF).show()"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["from pyspark.ml.feature import Bucketizer\nbucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\nbucketer = Bucketizer()\\\n.setSplits(bucketBorders)\\\n.setInputCol(\"id\")\nbucketer.transform(contDF).show()"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.QuantileDiscretizer\nval bucketer = new QuantileDiscretizer()\n.setNumBuckets(5)\n.setInputCol(\"id\")\nval fittedBucketer = bucketer.fit(contDF)\nfittedBucketer.transform(contDF).show()"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["from pyspark.ml.feature import QuantileDiscretizer\nbucketer = QuantileDiscretizer()\\\n.setNumBuckets(5)\\\n.setInputCol(\"id\")\nfittedBucketer = bucketer.fit(contDF)\nfittedBucketer.transform(contDF).show()"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.Normalizer\nval taxicab = new Normalizer()\n.setP(1)\n.setInputCol(\"features\")\ntaxicab.transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["from pyspark.ml.feature import Normalizer\ntaxicab = Normalizer()\\\n.setP(1)\\\n.setInputCol(\"features\")\ntaxicab.transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.StandardScaler\nval sScaler = new StandardScaler()\n.setInputCol(\"features\")\nsScaler.fit(scaleDF).transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.MinMaxScaler\nval minMax = new MinMaxScaler()\n.setMin(5)\n.setMax(10)\n.setInputCol(\"features\")\nval fittedminMax = minMax.fit(scaleDF)\nfittedminMax.transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["from pyspark.ml.feature import MinMaxScaler\nminMax = MinMaxScaler()\\\n.setMin(5)\\\n.setMax(10)\\\n.setInputCol(\"features\")\nfittedminMax = minMax.fit(scaleDF)\nfittedminMax.transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.MaxAbsScaler\nval maScaler = new MaxAbsScaler()\n.setInputCol(\"features\")\nval fittedmaScaler = maScaler.fit(scaleDF)\nfittedmaScaler.transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.ElementwiseProduct\nimport org.apache.spark.ml.linalg.Vectors\nval scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\nval scalingUp = new ElementwiseProduct()\n.setScalingVec(scaleUpVec)\n.setInputCol(\"features\")\nscalingUp.transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["from pyspark.ml.feature import ElementwiseProduct\nfrom pyspark.ml.linalg import Vectors\nscaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\nscalingUp = ElementwiseProduct()\\\n.setScalingVec(scaleUpVec)\\\n.setInputCol(\"features\")\nscalingUp.transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.StringIndexer\nval labelIndexer = new StringIndexer()\n.setInputCol(\"lab\")\n.setOutputCol(\"labelInd\")\nval idxRes = labelIndexer.fit(simpleDF).transform(simpleDF)\nidxRes.show()"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\nlabelIndexer = StringIndexer()\\\n.setInputCol(\"lab\")\\\n.setOutputCol(\"labelInd\")\nidxRes = labelIndexer.fit(simpleDF).transform(simpleDF)\nidxRes.show()"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["%scala\nval valIndexer = new StringIndexer()\n.setInputCol(\"value1\")\n.setOutputCol(\"valueInd\")\nvalIndexer.fit(simpleDF).transform(simpleDF).show()"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["valIndexer = StringIndexer()\\\n.setInputCol(\"value1\")\\\n.setOutputCol(\"valueInd\")\nvalIndexer.fit(simpleDF).transform(simpleDF).show()"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["%scala\nvalIndexer.setHandleInvalid(\"skip\")\nvalIndexer.fit(simpleDF).setHandleInvalid(\"skip\")"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.IndexToString\nval labelReverse = new IndexToString()\n.setInputCol(\"labelInd\")\nlabelReverse.transform(idxRes).show()"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["from pyspark.ml.feature import IndexToString\nlabelReverse = IndexToString()\\\n.setInputCol(\"labelInd\")\nlabelReverse.transform(idxRes).show()"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.VectorIndexer\nimport org.apache.spark.ml.linalg.Vectors\nval idxIn = spark.createDataFrame(Seq(\n(Vectors.dense(1, 2, 3),1),\n(Vectors.dense(2, 5, 6),2),\n(Vectors.dense(1, 8, 9),3)\n)).toDF(\"features\", \"label\")\nval indxr = new VectorIndexer()\n.setInputCol(\"features\")\n.setOutputCol(\"idxed\")\n.setMaxCategories(2)\nindxr.fit(idxIn).transform(idxIn).show"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["from pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.linalg import Vectors\nidxIn = spark.createDataFrame([\n(Vectors.dense(1, 2, 3),1),\n(Vectors.dense(2, 5, 6),2),\n(Vectors.dense(1, 8, 9),3)\n]).toDF(\"features\", \"label\")\nindxr = VectorIndexer()\\\n.setInputCol(\"features\")\\\n.setOutputCol(\"idxed\")\\\n.setMaxCategories(2)\nindxr.fit(idxIn).transform(idxIn).show"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["%scala\nval labelIndexer = new StringIndexer()\n.setInputCol(\"color\")\n.setOutputCol(\"colorInd\")\nval colorLab = labelIndexer.fit(simpleDF).transform(simpleDF)"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["labelIndexer = StringIndexer()\\\n.setInputCol(\"color\")\\\n.setOutputCol(\"colorInd\")\ncolorLab = labelIndexer.fit(simpleDF).transform(simpleDF)"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.OneHotEncoder\nval ohe = new OneHotEncoder()\n.setInputCol(\"colorInd\")\nohe.transform(colorLab).show()"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\nohe = OneHotEncoder()\\\n.setInputCol(\"colorInd\")\nohe.transform(colorLab).show()"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.PCA\nval pca = new PCA()\n.setInputCol(\"features\")\n.setK(2)\npca.fit(scaleDF).transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["from pyspark.ml.feature import PCA\npca = PCA()\\\n.setInputCol(\"features\")\\\n.setK(2)\npca.fit(scaleDF).transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.PolynomialExpansion\nval pe = new PolynomialExpansion()\n.setInputCol(\"features\")\n.setDegree(2)\npe.transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["from pyspark.ml.feature import PolynomialExpansion\npe = PolynomialExpansion()\\\n.setInputCol(\"features\")\\\n.setDegree(2)\npe.transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.ChiSqSelector\nval prechi = fittedCV.transform(tokenized)\n.where(\"CustomerId IS NOT NULL\")\nval chisq = new ChiSqSelector()\n.setFeaturesCol(\"countVec\")\n.setLabelCol(\"CustomerID\")\n.setNumTopFeatures(2)\nchisq.fit(prechi).transform(prechi).show()"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["from pyspark.ml.feature import ChiSqSelector\nprechi = fittedCV.transform(tokenized)\\\n.where(\"CustomerId IS NOT NULL\")\nchisq = ChiSqSelector()\\\n.setFeaturesCol(\"countVec\")\\\n.setLabelCol(\"CustomerID\")\\\n.setNumTopFeatures(2)\nchisq.fit(prechi).transform(prechi).show()"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["%scala\nval fittedPCA = pca.fit(scaleDF)\nfittedPCA.write.overwrite().save(\"/tmp/fittedPCA\")"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["%scala\nval loadedPCA = PCA.load(\"/tmp/fittedPCA\")\nloadedPCA.transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.UnaryTransformer\nimport org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable, Identifiable}\nimport org.apache.spark.sql.types.{ArrayType, StringType, DataType}\nimport org.apache.spark.ml.param.{IntParam, ParamValidators}\nclass MyTokenizer(override val uid: String)\nextends UnaryTransformer[String, Seq[String], MyTokenizer] with DefaultParamsWritable {\ndef this() = this(Identifiable.randomUID(\"myTokenizer\"))\nval maxWords: IntParam = new IntParam(this, \"maxWords\", \"The max number of words to return.\",\nParamValidators.gtEq(0))\ndef setMaxWords(value: Int): this.type = set(maxWords, value)\ndef getMaxWords: Integer = $(maxWords)\noverride protected def createTransformFunc: String => Seq[String] = (inputString: String) => {\ninputString.split(\"\\\\s\").take($(maxWords))\n}\noverride protected def validateInputType(inputType: DataType): Unit = {\nrequire(inputType == StringType, s\"Bad input type: $inputType. Requires String.\")\n}\noverride protected def outputDataType: DataType = new ArrayType(StringType, true)\n}\n// this will allow you to read it back in by using this object.\nobject MyTokenizer extends DefaultParamsReadable[MyTokenizer]\nval myT = new MyTokenizer()\n.setInputCol(\"someCol\")\n.setMaxWords(2)\ndisplay(myT.transform(Seq(\"hello world. This text won't show.\").toDF(\"someCol\")))\nmyT.write.overwrite().save(\"/tmp/something\")"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["%scala\nval sales = spark.read.format(\"csv\")\n.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.load(\"/FileStore/tables/1x42p66f1497639007910/*.csv\")\n.coalesce(5)\n.where(\"Description IS NOT NULL\")\nval fakeIntDF = spark.read.parquet(\"/FileStore/tables/w90rmm721497649161288/part_00000_ce2a44c8_feb4_4369_a2c3_4bf2f0e63b07_c000_gz-d65b9.parquet\")\nvar simpleDF = spark.read.json(\"/FileStore/tables/ixxwee3q1497648769294/part_r_00000_f5c243b9_a015_4a3b_a4a8_eca00f80f04c-a8b89.json\")\nval scaleDF = spark.read.parquet(\"/FileStore/tables/48se0cwp1497649233518/part_00000_cd03406a_cc9b_42b0_9299_1e259fdd9382_c000_gz-2e0be.parquet\")"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["sales = spark.read.format(\"csv\")\\\n.option(\"header\", \"true\")\\\n.option(\"inferSchema\", \"true\")\\\n.load(\"/FileStore/tables/1x42p66f1497639007910/*.csv\")\\\n.coalesce(5)\\\n.where(\"Description IS NOT NULL\")\nfakeIntDF = spark.read.parquet(\"/FileStore/tables/w90rmm721497649161288/part_00000_ce2a44c8_feb4_4369_a2c3_4bf2f0e63b07_c000_gz-d65b9.parquet\")\nsimpleDF = spark.read.json(\"/FileStore/tables/ixxwee3q1497648769294/part_r_00000_f5c243b9_a015_4a3b_a4a8_eca00f80f04c-a8b89.json\")\nscaleDF = spark.read.parquet(\"/FileStore/tables/48se0cwp1497649233518/part_00000_cd03406a_cc9b_42b0_9299_1e259fdd9382_c000_gz-2e0be.parquet\")\nsales.cache()"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.Tokenizer\nval tkn = new Tokenizer().setInputCol(\"Description\")\ntkn.transform(sales).show()"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.StandardScaler\nval ss = new StandardScaler().setInputCol(\"features\")\nss.fit(scaleDF).transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.RFormula\nval supervised = new RFormula()\n.setFormula(\"lab ~ . + color:value1 + color:value2\")\nsupervised.fit(simpleDF).transform(simpleDF).show()"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["from pyspark.ml.feature import RFormula\nsupervised = RFormula()\\\n.setFormula(\"lab ~ . + color:value1 + color:value2\")\nsupervised.fit(simpleDF).transform(simpleDF).show()"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.SQLTransformer\nval basicTransformation = new SQLTransformer()\n.setStatement(\"\"\"\nSELECT sum(Quantity), count(*), CustomerID\nFROM __THIS__\nGROUP BY CustomerID\n\"\"\")\nbasicTransformation.transform(sales).show()"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["from pyspark.ml.feature import SQLTransformer\nbasicTransformation = SQLTransformer()\\\n.setStatement(\"\"\"\nSELECT sum(Quantity), count(*), CustomerID\nFROM __THIS__\nGROUP BY CustomerID\n\"\"\")\nbasicTransformation.transform(sales).show()"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.VectorAssembler\nval va = new VectorAssembler()\n.setInputCols(Array(\"int1\", \"int2\", \"int3\"))\nva.transform(fakeIntDF).show()"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nva = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\nva.transform(fakeIntDF).show()"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.Tokenizer\nval tkn = new Tokenizer()\n.setInputCol(\"Description\")\n.setOutputCol(\"DescriptionOut\")\nval tokenized = tkn.transform(sales)\ntokenized.show()"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer\ntkn = Tokenizer()\\\n.setInputCol(\"Description\")\\\n.setOutputCol(\"DescriptionOut\")\ntokenized = tkn.transform(sales)\ntokenized.show()"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.RegexTokenizer\nval rt = new RegexTokenizer()\n.setInputCol(\"Description\")\n.setOutputCol(\"DescriptionOut\")\n.setPattern(\" \") // starting simple\n.setToLowercase(true)\nrt.transform(sales).show()"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer\nrt = RegexTokenizer()\\\n.setInputCol(\"Description\")\\\n.setOutputCol(\"DescriptionOut\")\\\n.setPattern(\" \")\\\n.setToLowercase(True)\nrt.transform(sales).show()"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.StopWordsRemover\nval englishStopWords = StopWordsRemover\n.loadDefaultStopWords(\"english\")\nval stops = new StopWordsRemover()\n.setStopWords(englishStopWords)\n.setInputCol(\"DescriptionOut\")\nstops.transform(tokenized).show()"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\nenglishStopWords = StopWordsRemover\\\n.loadDefaultStopWords(\"english\")\nstops = StopWordsRemover()\\\n.setStopWords(englishStopWords)\\\n.setInputCol(\"DescriptionOut\")\nstops.transform(tokenized).show()"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.NGram\nval unigram = new NGram()\n.setInputCol(\"DescriptionOut\")\n.setN(1)\nval bigram = new NGram()\n.setInputCol(\"DescriptionOut\")\n.setN(2)\nunigram.transform(tokenized).show()\nbigram.transform(tokenized).show()"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.CountVectorizer\nval cv = new CountVectorizer()\n.setInputCol(\"DescriptionOut\")\n.setOutputCol(\"countVec\")\n.setVocabSize(500)\n.setMinTF(1)\n.setMinDF(2)\nval fittedCV = cv.fit(tokenized)\nfittedCV.transform(tokenized).show()"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer\ncv = CountVectorizer()\\\n.setInputCol(\"DescriptionOut\")\\\n.setOutputCol(\"countVec\")\\\n.setVocabSize(500)\\\n.setMinTF(1)\\\n.setMinDF(2)\nfittedCV = cv.fit(tokenized)\nfittedCV.transform(tokenized).show()"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["%scala\nval tfIdfIn = tokenized\n.where(\"array_contains(DescriptionOut, 'red')\")\n.select(\"DescriptionOut\")\n.limit(10)\ntfIdfIn.show(false)"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":["tfIdfIn = tokenized\\\n.where(\"array_contains(DescriptionOut, 'red')\")\\\n.select(\"DescriptionOut\")\\\n.limit(10)\ntfIdfIn.show(10, False)"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.{HashingTF, IDF}\nval tf = new HashingTF()\n.setInputCol(\"DescriptionOut\")\n.setOutputCol(\"TFOut\")\n.setNumFeatures(10000)\nval idf = new IDF()\n.setInputCol(\"TFOut\")\n.setOutputCol(\"IDFOut\")\n.setMinDocFreq(2)"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF\ntf = HashingTF()\\\n.setInputCol(\"DescriptionOut\")\\\n.setOutputCol(\"TFOut\")\\\n.setNumFeatures(10000)\nidf = IDF()\\\n.setInputCol(\"TFOut\")\\\n.setOutputCol(\"IDFOut\")\\\n.setMinDocFreq(2)\n"],"metadata":{},"outputs":[],"execution_count":121},{"cell_type":"code","source":["%scala\nidf.fit(tf.transform(tfIdfIn))\n.transform(tf.transform(tfIdfIn))\n.show(false)\n"],"metadata":{},"outputs":[],"execution_count":122},{"cell_type":"code","source":["idf.fit(tf.transform(tfIdfIn))\\\n.transform(tf.transform(tfIdfIn))\\\n.show(10, False)"],"metadata":{},"outputs":[],"execution_count":123},{"cell_type":"code","source":["%scala\nval contDF = spark.range(500)\n.selectExpr(\"cast(id as double)\")"],"metadata":{},"outputs":[],"execution_count":124},{"cell_type":"code","source":["contDF = spark.range(500)\\\n.selectExpr(\"cast(id as double)\")"],"metadata":{},"outputs":[],"execution_count":125},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.Bucketizer\nval bucketBorders = Array(-1.0, 5.0, 10.0, 250.0, 600.0)\nval bucketer = new Bucketizer()\n.setSplits(bucketBorders)\n.setInputCol(\"id\")\nbucketer.transform(contDF).show()"],"metadata":{},"outputs":[],"execution_count":126},{"cell_type":"code","source":["from pyspark.ml.feature import Bucketizer\nbucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\nbucketer = Bucketizer()\\\n.setSplits(bucketBorders)\\\n.setInputCol(\"id\")\nbucketer.transform(contDF).show()"],"metadata":{},"outputs":[],"execution_count":127},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.QuantileDiscretizer\nval bucketer = new QuantileDiscretizer()\n.setNumBuckets(5)\n.setInputCol(\"id\")\nval fittedBucketer = bucketer.fit(contDF)\nfittedBucketer.transform(contDF).show()"],"metadata":{},"outputs":[],"execution_count":128},{"cell_type":"code","source":["from pyspark.ml.feature import QuantileDiscretizer\nbucketer = QuantileDiscretizer()\\\n.setNumBuckets(5)\\\n.setInputCol(\"id\")\nfittedBucketer = bucketer.fit(contDF)\nfittedBucketer.transform(contDF).show()"],"metadata":{},"outputs":[],"execution_count":129},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.Normalizer\nval taxicab = new Normalizer()\n.setP(1)\n.setInputCol(\"features\")\ntaxicab.transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":130},{"cell_type":"code","source":["from pyspark.ml.feature import Normalizer\ntaxicab = Normalizer()\\\n.setP(1)\\\n.setInputCol(\"features\")\ntaxicab.transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":131},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.StandardScaler\nval sScaler = new StandardScaler()\n.setInputCol(\"features\")\nsScaler.fit(scaleDF).transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":132},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.MinMaxScaler\nval minMax = new MinMaxScaler()\n.setMin(5)\n.setMax(10)\n.setInputCol(\"features\")\nval fittedminMax = minMax.fit(scaleDF)\nfittedminMax.transform(scaleDF).show(false)\n"],"metadata":{},"outputs":[],"execution_count":133},{"cell_type":"code","source":["from pyspark.ml.feature import MinMaxScaler\nminMax = MinMaxScaler()\\\n.setMin(5)\\\n.setMax(10)\\\n.setInputCol(\"features\")\nfittedminMax = minMax.fit(scaleDF)\nfittedminMax.transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":134},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.MaxAbsScaler\nval maScaler = new MaxAbsScaler()\n.setInputCol(\"features\")\nval fittedmaScaler = maScaler.fit(scaleDF)\nfittedmaScaler.transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":135},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.ElementwiseProduct\nimport org.apache.spark.ml.linalg.Vectors\nval scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\nval scalingUp = new ElementwiseProduct()\n.setScalingVec(scaleUpVec)\n.setInputCol(\"features\")\nscalingUp.transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":136},{"cell_type":"code","source":["from pyspark.ml.feature import ElementwiseProduct\nfrom pyspark.ml.linalg import Vectors\nscaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\nscalingUp = ElementwiseProduct()\\\n.setScalingVec(scaleUpVec)\\\n.setInputCol(\"features\")\nscalingUp.transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":137},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.StringIndexer\nval labelIndexer = new StringIndexer()\n.setInputCol(\"lab\")\n.setOutputCol(\"labelInd\")\nval idxRes = labelIndexer.fit(simpleDF).transform(simpleDF)\nidxRes.show()"],"metadata":{},"outputs":[],"execution_count":138},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\nlabelIndexer = StringIndexer()\\\n.setInputCol(\"lab\")\\\n.setOutputCol(\"labelInd\")\nidxRes = labelIndexer.fit(simpleDF).transform(simpleDF)\nidxRes.show()"],"metadata":{},"outputs":[],"execution_count":139},{"cell_type":"code","source":["%scala\nval valIndexer = new StringIndexer()\n.setInputCol(\"value1\")\n.setOutputCol(\"valueInd\")\nvalIndexer.fit(simpleDF).transform(simpleDF).show()"],"metadata":{},"outputs":[],"execution_count":140},{"cell_type":"code","source":["valIndexer = StringIndexer()\\\n.setInputCol(\"value1\")\\\n.setOutputCol(\"valueInd\")\nvalIndexer.fit(simpleDF).transform(simpleDF).show()"],"metadata":{},"outputs":[],"execution_count":141},{"cell_type":"code","source":["%scala\nvalIndexer.setHandleInvalid(\"skip\")\nvalIndexer.fit(simpleDF).setHandleInvalid(\"skip\")"],"metadata":{},"outputs":[],"execution_count":142},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.IndexToString\nval labelReverse = new IndexToString()\n.setInputCol(\"labelInd\")\nlabelReverse.transform(idxRes).show()"],"metadata":{},"outputs":[],"execution_count":143},{"cell_type":"code","source":["from pyspark.ml.feature import IndexToString\nlabelReverse = IndexToString()\\\n.setInputCol(\"labelInd\")\nlabelReverse.transform(idxRes).show()"],"metadata":{},"outputs":[],"execution_count":144},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.VectorIndexer\nimport org.apache.spark.ml.linalg.Vectors\nval idxIn = spark.createDataFrame(Seq(\n(Vectors.dense(1, 2, 3),1),\n(Vectors.dense(2, 5, 6),2),\n(Vectors.dense(1, 8, 9),3)\n)).toDF(\"features\", \"label\")\nval indxr = new VectorIndexer()\n.setInputCol(\"features\")\n.setOutputCol(\"idxed\")\n.setMaxCategories(2)\nindxr.fit(idxIn).transform(idxIn).show"],"metadata":{},"outputs":[],"execution_count":145},{"cell_type":"code","source":["from pyspark.ml.feature import VectorIndexer\nfrom pyspark.ml.linalg import Vectors\nidxIn = spark.createDataFrame([\n(Vectors.dense(1, 2, 3),1),\n(Vectors.dense(2, 5, 6),2),\n(Vectors.dense(1, 8, 9),3)\n]).toDF(\"features\", \"label\")\nindxr = VectorIndexer()\\\n.setInputCol(\"features\")\\\n.setOutputCol(\"idxed\")\\\n.setMaxCategories(2)\nindxr.fit(idxIn).transform(idxIn).show"],"metadata":{},"outputs":[],"execution_count":146},{"cell_type":"code","source":["%scala\nval labelIndexer = new StringIndexer()\n.setInputCol(\"color\")\n.setOutputCol(\"colorInd\")\nval colorLab = labelIndexer.fit(simpleDF).transform(simpleDF)"],"metadata":{},"outputs":[],"execution_count":147},{"cell_type":"code","source":["labelIndexer = StringIndexer()\\\n.setInputCol(\"color\")\\\n.setOutputCol(\"colorInd\")\ncolorLab = labelIndexer.fit(simpleDF).transform(simpleDF)"],"metadata":{},"outputs":[],"execution_count":148},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.OneHotEncoder\nval ohe = new OneHotEncoder()\n.setInputCol(\"colorInd\")\nohe.transform(colorLab).show()"],"metadata":{},"outputs":[],"execution_count":149},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\nohe = OneHotEncoder()\\\n.setInputCol(\"colorInd\")\nohe.transform(colorLab).show()"],"metadata":{},"outputs":[],"execution_count":150},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.PCA\nval pca = new PCA()\n.setInputCol(\"features\")\n.setK(2)\npca.fit(scaleDF).transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":151},{"cell_type":"code","source":["from pyspark.ml.feature import PCA\npca = PCA()\\\n.setInputCol(\"features\")\\\n.setK(2)\npca.fit(scaleDF).transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":152},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.PolynomialExpansion\nval pe = new PolynomialExpansion()\n.setInputCol(\"features\")\n.setDegree(2)\npe.transform(scaleDF).show(false)"],"metadata":{},"outputs":[],"execution_count":153},{"cell_type":"code","source":["from pyspark.ml.feature import PolynomialExpansion\npe = PolynomialExpansion()\\\n.setInputCol(\"features\")\\\n.setDegree(2)\npe.transform(scaleDF).show()"],"metadata":{},"outputs":[],"execution_count":154},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.ChiSqSelector\nval prechi = fittedCV.transform(tokenized)\n.where(\"CustomerId IS NOT NULL\")\nval chisq = new ChiSqSelector()\n.setFeaturesCol(\"countVec\")\n.setLabelCol(\"CustomerID\")\n.setNumTopFeatures(2)\nchisq.fit(prechi).transform(prechi).show()"],"metadata":{},"outputs":[],"execution_count":155},{"cell_type":"code","source":["from pyspark.ml.feature import ChiSqSelector\nprechi = fittedCV.transform(tokenized)\\\n.where(\"CustomerId IS NOT NULL\")\nchisq = ChiSqSelector()\\\n.setFeaturesCol(\"countVec\")\\\n.setLabelCol(\"CustomerID\")\\\n.setNumTopFeatures(2)\nchisq.fit(prechi).transform(prechi).show()"],"metadata":{},"outputs":[],"execution_count":156},{"cell_type":"code","source":["%scala\nval fittedPCA = pca.fit(scaleDF)\nfittedPCA.write.overwrite().save(\"/tmp/fittedPCA\")"],"metadata":{},"outputs":[],"execution_count":157},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.UnaryTransformer\nimport org.apache.spark.ml.util.{DefaultParamsReadable, DefaultParamsWritable, Identifiable}\nimport org.apache.spark.sql.types.{ArrayType, StringType, DataType}\nimport org.apache.spark.ml.param.{IntParam, ParamValidators}\nclass MyTokenizer(override val uid: String)\nextends UnaryTransformer[String, Seq[String], MyTokenizer] with DefaultParamsWritable {\ndef this() = this(Identifiable.randomUID(\"myTokenizer\"))\nval maxWords: IntParam = new IntParam(this, \"maxWords\", \"The max number of words to return.\",\nParamValidators.gtEq(0))\ndef setMaxWords(value: Int): this.type = set(maxWords, value)\n  def getMaxWords: Integer = $(maxWords)\noverride protected def createTransformFunc: String => Seq[String] = (inputString: String) => {\ninputString.split(\"\\\\s\").take($(maxWords))\n}\noverride protected def validateInputType(inputType: DataType): Unit = {\nrequire(inputType == StringType, s\"Bad input type: $inputType. Requires String.\")\n}\noverride protected def outputDataType: DataType = new ArrayType(StringType, true)\n}\n// this will allow you to read it back in by using this object.\nobject MyTokenizer extends DefaultParamsReadable[MyTokenizer]\nval myT = new MyTokenizer()\n.setInputCol(\"someCol\")\n.setMaxWords(2)\ndisplay(myT.transform(Seq(\"hello world. This text won't show.\").toDF(\"someCol\")))\nmyT.write.overwrite().save(\"/tmp/something\")"],"metadata":{},"outputs":[],"execution_count":158},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":159}],"metadata":{"name":"Definitive Draft Chapter 14-16","notebookId":2060825016293726},"nbformat":4,"nbformat_minor":0}
