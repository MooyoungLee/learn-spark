{"cells":[{"cell_type":"markdown","source":["# Install TensorFlow with an init script.\n\nInit scripts are global to the databricks cluster (which is separate from the spark cluster).  \n\nYou save an init script to the databricks file system in a subdirectory bearing the cluster name.  \n\nWhen you launch a cluster with that name, databricks looks for the folder in order to run the scripts there.\n\n## Create the init-script\n\nBecause this script will update the global databricks environment, it can be run from any notebook, attached to any cluster."],"metadata":{}},{"cell_type":"code","source":["# The name of the cluster on which to install TensorFlow:\nclusterName = \"tensorflow-cpu\"\n\n# TensorFlow binary URL\ntfBinaryUrl = \"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\"\n\n# Create the script tempalte, then render it using format.\nscript = \"\"\"#!/usr/bin/env bash\n\nset -ex\n\necho \"**** Installing GPU-enabled TensorFlow *****\"\n\npip install {tfBinaryUrl}\n\"\"\".format(tfBinaryUrl = tfBinaryUrl)\n\n# Write the script to the global environment\ndbutils.fs.mkdirs(\"dbfs:/databricks/init/\")\ndbutils.fs.put(\"dbfs:/databricks/init/%s/install-tensorflow-gpu.sh\" % clusterName, script, True)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["Now that the template exists we can create a cluster called `tensorflow-cpu`, since that is the name of the folder where we put the script.\n\nOnce the `tensorflow-cpu` cluster launches, attach this notebook to it and see if tensorflow is available by trying to import it.\n\nIf you are already running on the cluster where you want to install tensorflow, restart it, then attach this notebook to it when it finishes restarting."],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\nprint tf.__version__"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Now you should have a working multi-node Spark cluster.  This means that not only will python work, but Spark will work too.  This step is not necessary in the community edition because all community edition clusters are single-node, with the driver and the worker on the same node.\n\nA single-node professional cluster only includes the driver, there is no way to install the worker on the same node using a professional account."],"metadata":{}},{"cell_type":"markdown","source":["# Continue to Notebook 6\n\nThis notebook only needs to run once to setup the cluster.  Now you can do the exercises in Notebook 11_Use_TF.py."],"metadata":{}}],"metadata":{"name":"10b-TF-setup-pro","notebookId":3737261653513236},"nbformat":4,"nbformat_minor":0}
