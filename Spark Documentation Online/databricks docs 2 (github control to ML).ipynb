{"cells":[{"cell_type":"code","source":["dbutils.widgets.text(\"foo\", \"fooDefault\", \"fooEmptyLabel\")\nprint dbutils.widgets.get(\"foo\")\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%scala\n// Example 1 - returning data through temporary tables.\n// You can only return one string using dbutils.notebook.exit(), but since called notebooks reside in the same JVM, you can\n// return a name referencing data stored in a temporary table.\n\n/** In callee notebook */\nsc.parallelize(1 to 5).toDF().registerTempTable(\"my_data\")\ndbutils.notebook.exit(\"my_data\")\n\n/** In caller notebook */\nval returned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\ndisplay(table(returned_table))\n"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%scala\n// Example 2 - returning data through DBFS.\n// For larger datasets, you can write the results to DBFS and then return the DBFS path of the stored data.\n\n/** In callee notebook */\ndbutils.fs.rm(\"/tmp/results/my_data\", recurse=true)\nsc.parallelize(1 to 5).toDF().write.parquet(\"dbfs:/tmp/results/my_data\")\ndbutils.notebook.exit(\"dbfs:/tmp/results/my_data\")\n\n/** In caller notebook */\nval returned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\ndisplay(sqlContext.read.parquet(returned_table))\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%scala\n// Example 3 - returning JSON data.\n// To return multiple values, you can use standard JSON libraries to serialize and deserialize results.\n\n/** In callee notebook */\n\n// Import jackson json libraries\nimport com.fasterxml.jackson.module.scala.DefaultScalaModule\nimport com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\nimport com.fasterxml.jackson.databind.ObjectMapper\n\n// Create a json serializer\nval jsonMapper = new ObjectMapper with ScalaObjectMapper\njsonMapper.registerModule(DefaultScalaModule)\n\n// Exit with json\ndbutils.notebook.exit(jsonMapper.writeValueAsString(Map(\"status\" -> \"OK\", \"table\" -> \"my_data\")))\n\n/** In caller notebook */\nval result = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\nprintln(jsonMapper.readValue[Map[String, String]](result))\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%scala\n// Errors in workflows thrown a WorkflowException. For now, WorkflowException provides a simple\n// message of why the run failed. We are evaluating a more comprehensive errors API - please\n// file/upvote a feature request at feedback.databricks.com.\nimport com.databricks.WorkflowException\n\n// Since dbutils.notebook.run() is just a function call, you can retry failures using standard Scala try-catch\n// control flow. Here we show an example of retrying a notebook a number of times.\ndef runRetry(notebook: String, timeout: Int, args: Map[String, String] = Map.empty, maxTries: Int = 3): String = {\n  var numTries = 0\n  while (true) {\n    try {\n      return dbutils.notebook.run(notebook, timeout, args)\n    } catch {\n      case e: WorkflowException if numTries < maxTries =>\n        println(\"Error, retrying: \" + e)\n    }\n    numTries += 1\n  }\n  \"\" // not reached\n}\n\nrunRetry(\"LOCATION_OF_CALLEE_NOTEBOOK\", timeout = 60, maxTries = 5)\n"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%python\n\n# Example 1 - returning data through temporary tables.\n# You can only return one string using dbutils.notebook.exit(), but since called notebooks reside in the same JVM, you can\n# return a name referencing data stored in a temporary table.\n\n## In callee notebook\nsqlContext.range(5).toDF(\"value\").registerTempTable(\"my_data\")\ndbutils.notebook.exit(\"my_data\")\n\n## In caller notebook\nreturned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\ndisplay(table(returned_table))\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%python\n\n# Example 2 - returning data through DBFS.\n# For larger datasets, you can write the results to DBFS and then return the DBFS path of the stored data.\n\n## In callee notebook\ndbutils.fs.rm(\"/tmp/results/my_data\", recurse=True)\nsqlContext.range(5).toDF(\"value\").write.parquet(\"dbfs:/tmp/results/my_data\")\ndbutils.notebook.exit(\"dbfs:/tmp/results/my_data\")\n\n## In caller notebook\nreturned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\ndisplay(sqlContext.read.parquet(returned_table))\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%python\n\n# Example 3 - returning JSON data.\n# To return multiple values, you can use standard JSON libraries to serialize and deserialize results.\n\n## In callee notebook\nimport json\ndbutils.notebook.exit(json.dumps({\n  \"status\": \"OK\",\n  \"table\": \"my_data\"\n}))\n\n## In caller notebook\nresult = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\nprint json.loads(result)\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%python\n\n# Errors in workflows thrown a WorkflowException. For now, WorkflowException provides a simple\n# message of why the run failed. We are evaluating a more comprehensive errors API - please\n# file/upvote a feature request at feedback.databricks.com.\n\ndef run_with_retry(notebook, timeout, args = {}, max_retries = 3):\n  num_retries = 0\n  while True:\n    try:\n      return dbutils.notebook.run(notebook, timeout, args)\n    except Exception as e:\n      if num_retries > max_retries:\n        raise e\n      else:\n        print \"Retrying error\", e\n        num_retries += 1\n\nrun_with_retry(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60, max_retries = 5)\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%scala\n\n// We define a class\ncase class TestKey(id: Long, str: String)\n\ndefined class TestKey\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%scala\nval rdd = sc.parallelize(Array((TestKey(1L, \"abd\"), \"dss\"), (TestKey(2L, \"ggs\"), \"dse\"), (TestKey(1L, \"abd\"), \"qrf\")))\nrdd.groupByKey().collect"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%scala\npackage com.databricks.example\n\ncase class TestKey(id: Long, str: String)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%scala\nimport com.databricks.example\n\nval rdd = sc.parallelize(Array(\n  (example.TestKey(1L, \"abd\"), \"dss\"), (example.TestKey(2L, \"ggs\"), \"dse\"), (example.TestKey(1L, \"abd\"), \"qrf\")))\nrdd.groupByKey().collect"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%scala\npackage x.y.z\n\nval aNumber = 5 // won't work\n\ndef functionThatWillNotWork(a: Int): Int = a + 1"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%scala\nsqlContext.read.format(\"com.databricks.spark.csv\")\n  .option(\"header\",\"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\")\n  .registerTempTable(\"diamonds\")\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["dbutils.widgets.dropdown(\"X123\", \"1\", [str(x) for x in range(1, 10)])\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["dbutils.widgets.dropdown(\"1\", \"1\", [str(x) for x in range(1, 10)], \"hello this is a widget\")\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["dbutils.widgets.dropdown(\"x123123\", \"1\", [str(x) for x in range(1, 10)], \"hello this is a widget\")\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["dbutils.widgets.dropdown(\"x1232133123\", \"1\", [str(x) for x in range(1, 10)], \"hello this is a widget 2\")\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["dbutils.widgets.help(\"dropdown\")\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["dbutils.widgets.dropdown(\"X\", \"1\", [str(x) for x in range(1, 10)])\n"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["dbutils.widgets.get(\"X\")\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["dbutils.widgets.remove(\"X\")\n"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["dbutils.widgets.removeAll()\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%sql\nCREATE WIDGET TEXT y DEFAULT \"10\"\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%sql\nCREATE WIDGET DROPDOWN cuts DEFAULT \"Good\" CHOICES select distinct cut from diamonds\n"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sql\nselect count(*) as numChocies, getArgument(\"cuts\") as cuts from diamonds where cut = getArgument(\"cuts\")\n"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%sql\nselect count(*) as numChocies, getArgument(\"cuts\") as cuts from diamonds where cut = getArgument(\"cuts\")\n"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%sql\nREMOVE WIDGET cuts\n"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%sql\nselect * from diamonds where cut like '%$cuts%'\n"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["from pyspark.sql import Row\n\narray = [Row(key=\"a\", group=\"vowels\", value=1),\n         Row(key=\"b\", group=\"consonants\", value=2),\n         Row(key=\"c\", group=\"consonants\", value=3),\n         Row(key=\"d\", group=\"consonants\", value=4),\n         Row(key=\"e\", group=\"vowels\", value=5)]\ndataframe = sqlContext.createDataFrame(sc.parallelize(array))\n\ndisplay(dataframe)\n"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["%r\nfit <- lm(Petal.Length ~., data = iris)\nlayout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page\nplot(fit)\n"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["%r\nlibrary(ggplot2)\nggplot(diamonds, aes(carat, price, color = color, group = 1)) + geom_point(alpha = 0.3) + stat_smooth()\n"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["%r\nlibrary(lattice)\nxyplot(price ~ carat | cut, diamonds, scales = list(log = TRUE), type = c(\"p\", \"g\", \"smooth\"), ylab = \"Log price\")\n"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%r\ninstall.packages(\"DandEFA\", repos = \"http://cran.us.r-project.org\")\nlibrary(DandEFA)\ndata(timss2011)\ntimss2011 <- na.omit(timss2011)\ndandpal <- rev(rainbow(100, start = 0, end = 0.2))\nfacl <- factload(timss2011,nfac=5,method=\"prax\",cormeth=\"spearman\")\ndandelion(facl,bound=0,mcex=c(1,1.2),palet=dandpal)\nfacl <- factload(timss2011,nfac=8,method=\"mle\",cormeth=\"pearson\")\ndandelion(facl,bound=0,mcex=c(1,1.2),palet=dandpal)\n"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["%scala\ncase class MyCaseClass(key: String, group: String, value: Int)\nval dataframe = sc.parallelize(Array(MyCaseClass(\"f\", \"consonants\", 1),\n       MyCaseClass(\"g\", \"consonants\", 2),\n       MyCaseClass(\"h\", \"consonants\", 3),\n       MyCaseClass(\"i\", \"vowels\", 4),\n       MyCaseClass(\"j\", \"consonants\", 5))\n).toDS()\n\ndisplay(dataframe)\n"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%scala\n// in Spark > 2.X\ndataframe.createOrReplaceTempView(\"someTableName\")\n"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["%sql\nSELECT * FROM someTableName\n"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["from bokeh.plotting import figure\nfrom bokeh.embed import components, notebook_div, file_html\nfrom bokeh.resources import CDN\n\n# prepare some data\nx = [1, 2, 3, 4, 5]\ny = [6, 7, 2, 4, 5]\n\n# create a new plot with a title and axis labels\np = figure(title=\"simple line example\", x_axis_label='x', y_axis_label='y')\n\n# add a line renderer with legend and line thickness\np.line(x, y, legend=\"Temp.\", line_width=2)\n\n# create an html document that embeds the Bokeh plot\nhtml = file_html(p, CDN, \"my plot1\")\n\n# display this html\ndisplayHTML(html)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["from pyspark.sql import Row\n\narray = map(lambda x: Row(key=\"k_%04d\" % x, value = x), range(1, 5001))\nlargeDataFrame = sqlContext.createDataFrame(sc.parallelize(array))\nlargeDataFrame.registerTempTable(\"largeTable\")\ndisplay(sqlContext.sql(\"select * from largeTable\"))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# Click on the Plot Options Button...to see how this pivot table was configured.\nfrom pyspark.sql import Row\n\nlargePivotSeries = map(lambda x: Row(key=\"k_%03d\" % (x % 200), series_grouping = \"group_%d\" % (x % 3), value = x), range(1, 5001))\nlargePivotDataFrame = sqlContext.createDataFrame(sc.parallelize(largePivotSeries))\nlargePivotDataFrame.registerTempTable(\"table_to_be_pivoted\")\ndisplay(sqlContext.sql(\"select * from table_to_be_pivoted\"))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["%sql select key, series_grouping, sum(value) from table_to_be_pivoted group by key, series_grouping order by key, series_grouping"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["from pyspark.sql import Row\nsalesEntryDataFrame = sqlContext.createDataFrame(sc.parallelize([\n  Row(category=\"fruits_and_vegetables\", product=\"apples\", year=2012, salesAmount=100.50),\n  Row(category=\"fruits_and_vegetables\", product=\"oranges\", year=2012, salesAmount=100.75),\n  Row(category=\"fruits_and_vegetables\", product=\"apples\", year=2013, salesAmount=200.25),\n  Row(category=\"fruits_and_vegetables\", product=\"oranges\", year=2013, salesAmount=300.65),\n  Row(category=\"fruits_and_vegetables\", product=\"apples\", year=2014, salesAmount=300.65),\n  Row(category=\"fruits_and_vegetables\", product=\"oranges\", year=2015, salesAmount=100.35),\n  Row(category=\"butcher_shop\", product=\"beef\", year=2012, salesAmount=200.50),\n  Row(category=\"butcher_shop\", product=\"chicken\", year=2012, salesAmount=200.75),\n  Row(category=\"butcher_shop\", product=\"pork\", year=2013, salesAmount=400.25),\n  Row(category=\"butcher_shop\", product=\"beef\", year=2013, salesAmount=600.65),\n  Row(category=\"butcher_shop\", product=\"beef\", year=2014, salesAmount=600.65),\n  Row(category=\"butcher_shop\", product=\"chicken\", year=2015, salesAmount=200.35),\n  Row(category=\"misc\", product=\"gum\", year=2012, salesAmount=400.50),\n  Row(category=\"misc\", product=\"cleaning_supplies\", year=2012, salesAmount=400.75),\n  Row(category=\"misc\", product=\"greeting_cards\", year=2013, salesAmount=800.25),\n  Row(category=\"misc\", product=\"kitchen_utensils\", year=2013, salesAmount=1200.65),\n  Row(category=\"misc\", product=\"cleaning_supplies\", year=2014, salesAmount=1200.65),\n  Row(category=\"misc\", product=\"cleaning_supplies\", year=2015, salesAmount=400.35)\n]))\nsalesEntryDataFrame.registerTempTable(\"test_sales_table\")\ndisplay(sqlContext.sql(\"select * from test_sales_table\"))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["%sql select * from test_sales_table"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%sql select * from test_sales_table"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["from pyspark.sql import Row\nstateRDD = sqlContext.createDataFrame(sc.parallelize([\n  Row(state=\"MO\", value=1), Row(state=\"MO\", value=10),\n  Row(state=\"NH\", value=4),\n  Row(state=\"MA\", value=8),\n  Row(state=\"NY\", value=4),\n  Row(state=\"CA\", value=7)\n]))\nstateRDD.registerTempTable(\"test_state_table\")\ndisplay(sqlContext.sql(\"Select * from test_state_table\"))"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["from pyspark.sql import Row\nworldRDD = sqlContext.createDataFrame(sc.parallelize([\n  Row(country=\"USA\", value=1000),\n  Row(country=\"JPN\", value=23),\n  Row(country=\"GBR\", value=23),\n  Row(country=\"FRA\", value=21),\n  Row(country=\"TUR\", value=3)\n]))\ndisplay(worldRDD)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["from pyspark.sql import Row\nscatterPlotRDD = sqlContext.createDataFrame(sc.parallelize([\n  Row(key=\"k1\", a=0.2, b=120, c=1), Row(key=\"k1\", a=0.4, b=140, c=1), Row(key=\"k1\", a=0.6, b=160, c=1), Row(key=\"k1\", a=0.8, b=180, c=1),\n  Row(key=\"k2\", a=0.2, b=220, c=1), Row(key=\"k2\", a=0.4, b=240, c=1), Row(key=\"k2\", a=0.6, b=260, c=1), Row(key=\"k2\", a=0.8, b=280, c=1),\n  Row(key=\"k1\", a=1.8, b=120, c=1), Row(key=\"k1\", a=1.4, b=140, c=1), Row(key=\"k1\", a=1.6, b=160, c=1), Row(key=\"k1\", a=1.8, b=180, c=1),\n  Row(key=\"k2\", a=1.8, b=220, c=2), Row(key=\"k2\", a=1.4, b=240, c=2), Row(key=\"k2\", a=1.6, b=260, c=2), Row(key=\"k2\", a=1.8, b=280, c=2),\n  Row(key=\"k1\", a=2.2, b=120, c=1), Row(key=\"k1\", a=2.4, b=140, c=1), Row(key=\"k1\", a=2.6, b=160, c=1), Row(key=\"k1\", a=2.8, b=180, c=1),\n  Row(key=\"k2\", a=2.2, b=220, c=3), Row(key=\"k2\", a=2.4, b=240, c=3), Row(key=\"k2\", a=2.6, b=260, c=3), Row(key=\"k2\", a=2.8, b=280, c=3)\n]))\ndisplay(scatterPlotRDD)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["import numpy as np\nimport math\n\n# Create data points for scatter plot\nnp.random.seed(0)\npoints = sc.parallelize(range(0,1000)).map(lambda x: (x/100.0, 4 * math.sin(x/100.0) + np.random.normal(4,1))).toDF()\ndisplay(points)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["from pyspark.sql import Row\n# Hover over the entry in the histogram to read off the exact valued plotted.\nhistogramRDD = sqlContext.createDataFrame(sc.parallelize([\n  Row(key1=\"a\", key2=\"x\", val=0.2), Row(key1=\"a\", key2=\"x\", val=0.4), Row(key1=\"a\", key2=\"x\", val=0.6), Row(key1=\"a\", key2=\"x\", val=0.8), Row(key1=\"a\", key2=\"x\", val=1.0), \n  Row(key1=\"b\", key2=\"z\", val=0.2), Row(key1=\"b\", key2=\"x\", val=0.4), Row(key1=\"b\", key2=\"x\", val=0.6), Row(key1=\"b\", key2=\"y\", val=0.8), Row(key1=\"b\", key2=\"x\", val=1.0), \n  Row(key1=\"a\", key2=\"x\", val=0.2), Row(key1=\"a\", key2=\"y\", val=0.4), Row(key1=\"a\", key2=\"x\", val=0.6), Row(key1=\"a\", key2=\"x\", val=0.8), Row(key1=\"a\", key2=\"x\", val=1.0), \n  Row(key1=\"b\", key2=\"x\", val=0.2), Row(key1=\"b\", key2=\"x\", val=0.4), Row(key1=\"b\", key2=\"x\", val=0.6), Row(key1=\"b\", key2=\"z\", val=0.8), Row(key1=\"b\", key2=\"x\", val=1.0)]))\ndisplay(histogramRDD)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["from pyspark.sql import Row\nquantileSeries = map(lambda x: Row(key=\"key_%01d\" % (x % 4), grouping=\"group_%01d\" % (x % 3), otherField=x, value=x*x), range(1, 5001))\nquantileSeriesRDD = sqlContext.createDataFrame(sc.parallelize(quantileSeries))\ndisplay(quantileSeriesRDD)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["from pyspark.sql import Row\nqqPlotSeries = map(lambda x: Row(key=\"key_%03d\" % (x % 5), grouping=\"group_%01d\" % (x % 3), value=x, value_squared=x*x), range(1, 5001))\nqqPlotRDD = sqlContext.createDataFrame(sc.parallelize(qqPlotSeries))\ndisplay(qqPlotRDD)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["from pyspark.sql import Row\nimport random\n# Hovering over the Box will display the exact median value.\nboxSeries = map(lambda x: Row(key=\"key_%01d\" % (x % 2), grouping=\"group_%01d\" % (x % 3), value=random.randint(0, x)), range(1, 5001))\nboxSeriesRDD = sqlContext.createDataFrame(sc.parallelize(boxSeries))\ndisplay(boxSeriesRDD)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["%scala\ncase class MapEntry(key: String, value: Int)\nval largeSeries = for (x <- 1 to 5000) yield MapEntry(\"k_%04d\".format(x), x)\nval largeDataFrame = sc.parallelize(largeSeries).toDF()\nlargeDataFrame.registerTempTable(\"largeTable\")\ndisplay(sqlContext.sql(\"select * from largeTable\"))"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["%scala\n// Click on the Plot Options Button...to see how this pivot table was configured.\n// NOTE how Pivot Tables are highlighted in green to distinguish them from regular charts.\ncase class PivotEntry(key: String, series_grouping: String, value: Int)\nval largePivotSeries = for (x <- 1 to 5000) yield PivotEntry(\"k_%03d\".format(x % 200),\"group_%01d\".format(x % 3), x)\nval largePivotDataFrame = sc.parallelize(largePivotSeries).toDF()\nlargePivotDataFrame.registerTempTable(\"table_to_be_pivoted\")\ndisplay(sqlContext.sql(\"select * from table_to_be_pivoted\"))"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["%sql select key, series_grouping, sum(value) from table_to_be_pivoted group by key, series_grouping order by key, series_grouping\n\n"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["%scala\ncase class QQPlotEntry(key: String, grouping: String, value: Int, value_squared: Int)\nval qqPlotSeries = for (x <- 1 to 5000) yield QQPlotEntry(\"k_%03d\".format(x % 5),\"group_%01d\".format(x % 3), x, x*x)\nval qqPlotRDD = sc.parallelize(qqPlotSeries).toDF()\ndisplay(qqPlotRDD)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["displayHTML(\"<h3>You can view HTML code in notebooks.</h3>\")"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["displayHTML(\"\"\"<svg width=\"100\" height=\"100\">\n   <circle cx=\"50\" cy=\"50\" r=\"40\" stroke=\"green\" stroke-width=\"4\" fill=\"yellow\" />\n   Sorry, your browser does not support inline SVG.\n</svg>\"\"\")"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["%scala\n// Change these colors to your favorites to change the D3 visualization.\nval colorsRDD = sc.parallelize(Array((197,27,125), (222,119,174), (241,182,218), (253,244,239), (247,247,247), (230,245,208), (184,225,134), (127,188,65), (77,146,33)))\nval colors = colorsRDD.collect()\n"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["%scala\ndisplayHTML(s\"\"\"\n<!DOCTYPE html>\n<meta charset=\"utf-8\">\n<style>\n\npath {\n  fill: yellow;\n  stroke: #000;}\n\ncircle {\n  fill: #fff;\n  stroke: #000;\n  pointer-events: none;}\n\n.PiYG .q0-9{fill:rgb${colors(0)}}\n.PiYG .q1-9{fill:rgb${colors(1)}}\n.PiYG .q2-9{fill:rgb${colors(2)}}\n.PiYG .q3-9{fill:rgb${colors(3)}}\n.PiYG .q4-9{fill:rgb${colors(4)}}\n.PiYG .q5-9{fill:rgb${colors(5)}}\n.PiYG .q6-9{fill:rgb${colors(6)}}\n.PiYG .q7-9{fill:rgb${colors(7)}}\n.PiYG .q8-9{fill:rgb${colors(8)}}\n\n</style>\n<body>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.6/d3.min.js\"></script>\n<script>\n\nvar width = 960,\n    height = 500;\n\nvar vertices = d3.range(100).map(function(d) {\n  return [Math.random() * width, Math.random() * height];});\n\nvar svg = d3.select(\"body\").append(\"svg\")\n    .attr(\"width\", width)\n    .attr(\"height\", height)\n    .attr(\"class\", \"PiYG\")\n    .on(\"mousemove\", function() { vertices[0] = d3.mouse(this); redraw(); });\n\nvar path = svg.append(\"g\").selectAll(\"path\");\n\nsvg.selectAll(\"circle\")\n    .data(vertices.slice(1))\n  .enter().append(\"circle\")\n    .attr(\"transform\", function(d) { return \"translate(\" + d + \")\"; })\n    .attr(\"r\", 2);\n\nredraw();\n\nfunction redraw() {\n  path = path.data(d3.geom.delaunay(vertices).map(function(d) { return \"M\" + d.join(\"L\") + \"Z\"; }), String);\n  path.exit().remove();\n  path.enter().append(\"path\").attr(\"class\", function(d, i) { return \"q\" + (i % 9) + \"-9\"; }).attr(\"d\", String);}\n\n</script>\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["%sh apt-get --yes install pandoc"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["%r\ninstall.packages(\"htmlwidgets\")"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["%r\nlibrary(htmlwidgets)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["%r\n## Make sure you use HTTPS\ndatabricksURL <- \"https://community.cloud.databricks.com\""],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["%r\ndb_html_print <- function(x, ..., view = interactive()) {\n  fileName <- paste(tempfile(), \".html\", sep=\"\")\n  htmlwidgets::saveWidget(x, file = fileName)\n  \n  randomFileName = paste0(floor(runif(1, 0, 10^12)), \".html\")\n  baseDir <- \"/dbfs/FileStore/rwidgets/\"\n  dir.create(baseDir)\n  internalFile = paste0(baseDir, randomFileName)\n  externalFile = paste0(databricksURL, \"/files/rwidgets/\", randomFileName)\n  system(paste(\"cp\", fileName, internalFile))\n  displayHTML(externalFile)\n}\nR.utils::reassignInPackage(\"print.htmlwidget\", pkgName = \"htmlwidgets\", value = db_html_print)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["%r\ninstall.packages(\"dygraphs\", repos=\"http://cloud.r-project.org\")"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["%r\nlibrary(dygraphs)\ndygraph(cbind(mdeaths, fdeaths))"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["%r\ninstall.packages(\"plotly\")"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["%r\nlibrary(plotly)\nlibrary(ggplot2)\np <- plot_ly(midwest, x = ~percollege, color = ~state, type = \"box\")\np"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["%r\ninstall.packages(\"ggplot2\")\nupdateR()"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":73}],"metadata":{"name":"databricks docs 2 (github control to ML)","notebookId":4128180761115748},"nbformat":4,"nbformat_minor":0}
