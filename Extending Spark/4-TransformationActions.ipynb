{"cells":[{"cell_type":"markdown","source":["In our notebook we have access to sqlContext"],"metadata":{}},{"cell_type":"code","source":["sqlContext"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["Spark Context"],"metadata":{}},{"cell_type":"code","source":["sc"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["And SparkSession"],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["We can use the Spark Context to create a small python range that will provide a return type of `DataFrame`."],"metadata":{}},{"cell_type":"code","source":["firstDataFrame = spark.range(1000000)\nprint firstDataFrame"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["You might have expected the last command to print the values of `firstDataFrame`.  But the `range` command is a **transformation** operation, and Spark will wait until the values are needed before it calculates them.  Spark will know a value is needed when you execute an **action** operation.  \n\n`show()` is an **action**:"],"metadata":{}},{"cell_type":"code","source":["firstDataFrame.show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["To show you the values in the DataFrame, Spark must know the values first.  When you call `show()` you force Spark to execute all the transformations needed in order to know the values in the DataFrame, and Spark will automatically do this at the right time.\n\nHere are some simple examples of transformations and actions.\n\nTransformations (lazy) - select, distinct, groupBy, sum, orderBy, filter, limit\nActions - show, count, collect, save"],"metadata":{}},{"cell_type":"markdown","source":["Try another transformation.  Tell Spark that we want to create a `secondDataFrame` by multiplying 2 by each value in the `firstDataFrame`."],"metadata":{}},{"cell_type":"code","source":["# select the ID column values and multiply them by 2\nsecondDataFrame = firstDataFrame.selectExpr(\"(id * 2) as value\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["`take` is another action.  This action selects a certain number of elements from the beginning of a dataframe.  Try to take five values from each DataFrame."],"metadata":{}},{"cell_type":"code","source":["# take the first 5 values that we have in our firstDataFrame, use print to see the results\nprint firstDataFrame.take(10)\n# take the first 5 values that we have in our secondDataFrame, use print to see the results\nprint secondDataFrame.take(10)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Spark separates operations into **transformations** (which are evaluated only when needed) and **actions** (which are evaluated immediately) to support an optimization called pipelining.  This lets you decide what you want the data to look like, while leaving it up to Spark to find the most efficient way to calculate your results.\n\n![transformations and actions](http://training.databricks.com/databricks_guide/gentle_introduction/pipeline.png)"],"metadata":{}}],"metadata":{"name":"4-TransformationActions","notebookId":3737261653513047},"nbformat":4,"nbformat_minor":0}
