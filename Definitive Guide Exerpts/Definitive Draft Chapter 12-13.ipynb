{"cells":[{"cell_type":"code","source":["%scala\nval myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\nval words = spark.sparkContext.parallelize(myCollection, 2)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%scala\nwords.mapPartitions(part => Iterator[Int](1)).sum()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%scala\ndef indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = {\nwithinPartIterator.toList.map(value => s\"Partition: $partitionIndex => $value\").iterator\n}\nwords.mapPartitionsWithIndex(indexedFunc).collect()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%scala\nwords.foreachPartition { iter =>\nimport java.io._\nimport scala.util.Random\nval randomFileName = new Random().nextInt()\nval pw = new PrintWriter(new File(s\"/tmp/random-file-${randomFileName}.txt\"))\nwhile (iter.hasNext) {\npw.write(iter.next())\n}\npw.close()\n}"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%scala\nsc.parallelize(Seq(\"Hello\", \"World\"), 2).glom().collect()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%scala\nwords\n.map(word => (word.toLowerCase, 1))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%scala\nwords\n.keyBy(word => word.toLowerCase.toSeq(0))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%scala\nwords\n.map(word => (word.toLowerCase.toSeq(0), word))\n.mapValues(word => word.toUpperCase)\n.collect()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%scala\nwords\n.map(word => (word.toLowerCase.toSeq(0), word))\n.flatMapValues(word => word.toUpperCase)\n.collect()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%scala\nwords\n.map(word => (word.toLowerCase.toSeq(0), word))\n.keys\n.collect()\nwords\n.map(word => (word.toLowerCase.toSeq(0), word))\n.values\n.collect()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%scala\nwords\n.map(word => (word.toLowerCase, 1))\n.lookup(\"spark\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%scala\nval chars = words\n.flatMap(word => word.toLowerCase.toSeq)\nval KVcharacters = chars\n.map(letter => (letter, 1))\ndef maxFunc(left:Int, right:Int) = math.max(left, right)\ndef addFunc(left:Int, right:Int) = left + right\nval nums = sc.parallelize(1 to 30, 5)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%scala\nKVcharacters.countByKey()\nval timeout = 1000L //milliseconds\nval confidence = 0.95\nKVcharacters.countByKeyApprox(timeout, confidence)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%scala\nKVcharacters\n.groupByKey()\n.map(row => (row._1, row._2.reduce(addFunc)))\n.collect()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%scala\nKVcharacters.reduceByKey(addFunc).collect()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%scala\nnums.aggregate(0)(maxFunc, addFunc)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%scala\nnums.treeAggregate(0)(maxFunc, addFunc)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%scala\nKVcharacters.aggregateByKey(0)(addFunc, maxFunc).collect()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%scala\nval valToCombiner = (value:Int) => List(value)\nval mergeValuesFunc = (vals:List[Int], valToAppend:Int) => valToAppend :: vals\nval mergeCombinerFunc = (vals1:List[Int], vals2:List[Int]) => vals1 ::: vals2\n// not we define these as function variables\nval outputPartitions = 6\nKVcharacters\n.combineByKey(\nvalToCombiner,\nmergeValuesFunc,\nmergeCombinerFunc,\noutputPartitions)\n.collect()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%scala\nKVcharacters\n.foldByKey(0)(addFunc)\n.collect()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%scala\nval distinctChars = words\n.flatMap(word => word.toLowerCase.toSeq)\n.distinct\n.collect()\nimport scala.util.Random\nval sampleMap = distinctChars.map(c => (c, new Random().nextDouble())).toMap\nwords\n.map(word => (word.toLowerCase.toSeq(0), word))\n.sampleByKey(true, sampleMap, 6L)\n.collect()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%scala\nwords\n.map(word => (word.toLowerCase.toSeq(0), word))\n.sampleByKeyExact(true, sampleMap, 6L)\n.collect()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%scala\nimport scala.util.Random\nval distinctChars = words\n.flatMap(word => word.toLowerCase.toSeq)\n.distinct\nval charRDD = distinctChars.map(c => (c, new Random().nextDouble()))\nval charRDD2 = distinctChars.map(c => (c, new Random().nextDouble()))\nval charRDD3 = distinctChars.map(c => (c, new Random().nextDouble()))\ncharRDD.cogroup(charRDD2, charRDD3).take(5)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["%scala\nval keyedChars = sc.parallelize(distinctChars.map(c => (c, new Random().nextDouble())))\nval outputPartitions = 10\nKVcharacters.join(keyedChars).count()\nKVcharacters.join(keyedChars, outputPartitions).count()\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%scala\nval numRange = sc.parallelize(0 to 9, 2)\nwords.zip(numRange).collect()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%scala\nwords.coalesce(1)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%scala\nwords.repartition(10)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%scala\nval df = spark.read\n.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.csv(\"dbfs:/mnt/defg/streaming/*.csv\")\nval rdd = df.coalesce(10).rdd"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%scala\nrdd.map(r => r(6)).take(5).foreach(println)\nval keyedRDD = rdd.keyBy(row => row(6).asInstanceOf[Double])\nimport org.apache.spark.{HashPartitioner}\nkeyedRDD\n.partitionBy(new HashPartitioner(10))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%scala\nimport org.apache.spark.{Partitioner}\nclass DomainPartitioner extends Partitioner {\ndef numPartitions = 20\ndef getPartition(key: Any): Int = {\n(key.asInstanceOf[Double] / 1000).toInt\n}\n}\nval res = keyedRDD\n.partitionBy(new DomainPartitioner)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["%scala\nres\n.glom()\n.collect()\n.map(arr => {\nif (arr.length > 0) {\narr.map(_._2(6)).toSet.toSeq.length\n}\n})"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["%scala\nkeyedRDD.repartitionAndSortWithinPartitions(new DomainPartitioner)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["%scala\nclass SomeClass extends Serializable {\nvar someValue = 0\ndef setSomeValue(i:Int) = {\nsomeValue = i\nthis\n}\n}\nsc.parallelize(1 to 10).map(num => new SomeClass().setSomeValue(num))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["%scala\nval myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\nval words = spark.sparkContext.parallelize(myCollection, 2)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["my_collection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\nwords = spark.sparkContext.parallelize(my_collection, 2)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%scala\nval supplementalData = Map(\n\"Spark\" -> 1000,\n\"Definitive\" -> 200,\n  \"Big\" -> -300,\n\"Simple\" -> 100\n)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["%scala\nval suppBroadcast = spark.sparkContext.broadcast(supplementalData)\nsuppBroadcast.value\nval suppWords = words.map(word => (word, suppBroadcast.value.getOrElse(word, 0)))\nsuppWords.sortBy(wordPair => wordPair._2).collect()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%scala\ncase class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)\nval flights = spark.read\n.parquet(\"/mnt/defg/chapter-1-data/parquet/2010-summary.parquet/\")\n.as[Flight]"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["%scala\nimport org.apache.spark.util.LongAccumulator\nval accUnnamed = new LongAccumulator\nsc.register(accUnnamed)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["%scala\nval accChina = new LongAccumulator\nsc.register(accChina, \"China\")\nval accChina2 = sc.longAccumulator(\"China\")"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["%scala\ndef accChinaFunc(flight_row: Flight) = {\nval destination = flight_row.DEST_COUNTRY_NAME\nval origin = flight_row.ORIGIN_COUNTRY_NAME\nif (destination == \"China\") {\naccChina.add(flight_row.count.toLong)\n}\nif (origin == \"China\") {\naccChina.add(flight_row.count.toLong)\n}\n}"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["%scala\nflights.foreach(flight_row => accChinaFunc(flight_row))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["%scala\naccChina.value"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["%scala\nimport scala.collection.mutable.ArrayBuffer\nval arr = ArrayBuffer[BigInt]()\nimport scala.collection.mutable.ArrayBuffer\nimport org.apache.spark.util.AccumulatorV2\nclass EvenAccumulator extends AccumulatorV2[BigInt, BigInt] {\nprivate var num:BigInt = 0\ndef reset(): Unit = {\nthis.num = 0\n}\ndef add(intValue: BigInt): Unit = {\nif (intValue % 2 == 0) {\nthis.num += intValue\n}\n}\ndef merge(other: AccumulatorV2[BigInt,BigInt]): Unit = {\nthis.num += other.value\n}\ndef value():BigInt = {\nthis.num\n}\ndef copy(): AccumulatorV2[BigInt,BigInt] = {\nnew EvenAccumulator\n}\ndef isZero():Boolean = {\nthis.num == 0\n}\n}\nval acc = new EvenAccumulator\nval newAcc = sc.register(acc, \"evenAcc\")\nacc.value\nflights.foreach(flight_row => acc.add(flight_row.count))\nacc.value"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":45}],"metadata":{"name":"Definitive Draft Chapter 12-13","notebookId":2060825016293679},"nbformat":4,"nbformat_minor":0}
