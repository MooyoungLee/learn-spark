{"cells":[{"cell_type":"markdown","source":["## Apache Spark MLLib\nLet's return to the farmer's market dataset and use Spark to explore the hypothesis:\n* The number of farmer's markets in a given zip code can be predicted from the income and taxes paid in a given area.\n\nThere are serveral steps to this process:\n1. **Part One - Load and prepare the data** \n  * Verify and/or load table data\n  * Prepare the data by aggregating, grouping and counting table data values\n  * Join data from the prepared tables\n  * Convert 'null' values in the joined data to '0'\n2. **Part Two - Use the Spark ML Library**\n  * Create and display a vector with the the features you'd like to explore in a scatterplot\n  * Split the dataset into testing and training sets, cache both and call an action to load the cache\n  * Create a linear regression model and fit the model with your training data\n  * Use your model by calling predict on it\n  * Evaluate and update your model  \n  * Train and use the most optimal model"],"metadata":{}},{"cell_type":"markdown","source":["### Part One - Load and Prepare the data \n* Load the table `cleaned_taxes` into a dataframe (created in previous exercise)"],"metadata":{}},{"cell_type":"code","source":["cleanedTaxes = sqlContext.sql(\"SELECT * FROM cleaned_taxes\")\ncleanedTaxes.show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["NOTE: If the table did NOT load, then run the next couple of cells to re-load the data."],"metadata":{}},{"cell_type":"code","source":["# taxes2013 = spark.read\\\n#   .option(\"header\", \"true\")\\\n#   .csv(\"dbfs:/databricks-datasets/data.gov/irs_zip_code_data/data-001/2013_soi_zipcode_agi.csv\")\n# taxes2013.createOrReplaceTempView(\"taxes2013\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# %sql\n# DROP TABLE IF EXISTS cleaned_taxes;\n\n# CREATE TABLE cleaned_taxes AS\n# SELECT \n#   state, \n#   int(zipcode / 10) as zipcode,\n#   int(mars1) as single_returns,\n#   int(mars2) as joint_returns,\n#   int(numdep) as numdep,\n#   double(A02650) as total_income_amount,\n#   double(A00300) as taxable_interest_amount,\n#   double(a01000) as net_capital_gains,\n#   double(a00900) as biz_net_income\n# FROM taxes2013"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["* Load the market dataset to a permanent table named `markets`"],"metadata":{}},{"cell_type":"code","source":["markets = spark.read\\\n  .option(\"header\", \"true\")\\\n  .csv(\"dbfs:/databricks-datasets/data.gov/farmers_markets_geographic_data/data-001/market_data.csv\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["* Use `sum` to aggreggate all the columns in the `cleanedTaxes` dataset -- NOTE: Some data will be nonsense (i.e.summing zipcode) but other data could become useful features (i.e. summing AGI in the zipcode).\n* Group the `cleanedTaxes` dataframe by zipcode, then `sum` to aggregate across all columns. \n* Save the resulting dataframe in `summedTaxes`\n* `show` the `summedTaxes` dataframe"],"metadata":{}},{"cell_type":"code","source":["summedTaxes = cleanedTaxes\\\n  .groupBy(\"zipcode\")\\\n  .sum()\n  \nsummedTaxes.show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Group the market data into buckets and count the number of farmer's markets in each bucket.\n\n* Use `selectExpr` to transform the market data into labels that identify which zip group they belong to (we used `int(zip/10)` to group the tax data) call the new value `zipcode`.  `selectExpr` is short for \"Select Expression\" and can process similar operations to SQL statements.\n* Group by the `zipcode` you just created, then `count` the groups.\n* Use another `selectExpr` to transform the data, you only need to keep the `count` and the `zipcode as zip`.\n* Store the results in a new dataset called `cleanedMarkets`.\n* `show` `cleanedMarkets`"],"metadata":{}},{"cell_type":"code","source":["cleanedMarkets = markets\\\n  .selectExpr(\"*\", \"int(zip / 10) as zipcode\")\\\n  .groupBy(\"zipcode\")\\\n  .count()\\\n  .selectExpr(\"double(count) as count\", \"zipcode as zip\")\n\ndisplay(cleanedMarkets)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Join the two cleaned datasets into one dataset for analysis.\n\n* Outer join `cleanedMarkets` to `summedTaxes` using `zip` and `zipcode` as the join variable.\n* Name the resulting dataset `joined`."],"metadata":{}},{"cell_type":"code","source":["joined = cleanedMarkets\\\n  .join(summedTaxes, cleanedMarkets[\"zip\"] == summedTaxes[\"zipcode\"], \"outer\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["* `display` the `joined` data - do you see the 'null' values?"],"metadata":{}},{"cell_type":"code","source":["display(joined)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["MLLib doesn't allow null values.  These values came up as `null` in the join because there were no farmer's markets in that zip code \"basket\".  It makes sense to replace the `null` values with zeros.\n* Use the `na` prefix to `fill` the empty cells with `0`.\n* Name the resulting dataset `prepped` and `display` it."],"metadata":{}},{"cell_type":"code","source":["prepped = joined.na.fill(0)\ndisplay(prepped)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Part Two -Use MLLib with Spark\n* Put all the features into a single vector.  \n* Create an array to list the names of all the **non-feature** columns: `zip`, `zipcode`, `count`, call it `nonFeatureCols`.\n* Create a list of names called `featureCols` which excludes the columns in `nonFeatureCols`.\n* `print` the `featureCols`."],"metadata":{}},{"cell_type":"code","source":["nonFeatureCols = {'zip', 'zipcode', 'count'}\nfeatureCols = [column for column in prepped.columns if column not in nonFeatureCols]\nprint(featureCols)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["* Use the `VectorAssembler` from `pyspark.ml.feature` to add a `features` vector to the `prepped` dataset.\n* Call the new dataset `finalPrep`, then `display` only the `zipcode` and `features` from `finalPrep`."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(\n    inputCols=[column for column in featureCols],\n    outputCol='features')\n\nfinalPrep = assembler.transform(prepped)\ndisplay(finalPrep.select('zipcode', 'features'))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["* Display the feature columns graphed out against each other as a scatter plot (hint: exclude `zip`, `zipcode` and `features` using `drop`)"],"metadata":{}},{"cell_type":"code","source":["display(finalPrep.drop(\"zip\").drop(\"zipcode\").drop(\"features\"))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["* Split the `finalPrep` data set into training and testing subsets.  The sets should be randomly selected, 70 percent of the samples should go into the `training` set, and 30 percent should go into the `test` set.\n* Cache `training` and `test`.\n* Perform an action such as `count` to populate the cache."],"metadata":{}},{"cell_type":"code","source":["(training, test) = finalPrep.randomSplit((0.7, 0.3))\n\ntraining.cache()\ntest.cache()\n\nprint(training.count())\nprint(test.count())"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["Spark MLLib supports both `regressors` and `classifiers`, in this example you will use linear regression.  Once you create the `regressor` you will train it, and it will return a `Model`. The `Model` will be the object you use to make predictions.\n\n* Create an instance of the `LinearRegression` algorithm called `lrModel`:\n* Set the label column to \"count\"\n* Set the features column to \"features\"\n* Set the \"ElasticNetParam\" to 0.5 (this controlls the mix of l1 and l2 regularization--we'll just use an equal amount of each)\n* Print the results of calling `explainParams` on `lrModel`.  This will show you all the possible parameters, and whether or not you have customized them."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\n\nlrModel = LinearRegression()\\\n  .setLabelCol(\"count\")\\\n  .setFeaturesCol(\"features\")\\\n  .setElasticNetParam(0.5)\n\nprint(\"Printing out the model Parameters:\")\nprint(\"-\"*20)\nprint(lrModel.explainParams())\nprint(\"-\"*20)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["* Use the `fit` method on `lrModel` to provide the `training` dataset for fitting. \n* Store the results in `lrFitted`."],"metadata":{}},{"cell_type":"code","source":["lrFitted = lrModel.fit(training)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["* Make a prediction by using the `transform` method on `lrFitted`, passing it the `test` dataset. \n* Store the results in `holdout`.\n* `transform` adds a new column called \"prediction\" to the data we passed into it.\n* Display the `prediction` and `count` from `holdout`"],"metadata":{}},{"cell_type":"code","source":["holdout = lrFitted.transform(test)\ndisplay(holdout.select(\"prediction\", \"count\"))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["The `transform` method shows us how many farmer's markets the `lrFitted` method predicts there will be in each zip code based on the features we provided.  The raw predictions are not rounded at all.  \n\n* Use a `selectExpr` to relabel `prediction` as `raw_prediction`.\n* `round` the `prediction` and call it `prediction` inside the expression\n* Select `count` for comparison purposes.\n* Create a column called `equal` that will let us know if the model predicted correctly."],"metadata":{}},{"cell_type":"code","source":["holdout = holdout.selectExpr(\\\n                             \"prediction as raw_prediction\", \\\n                             \"double(round(prediction)) as prediction\", \\\n                             \"count\", \\\n                             \"\"\"CASE double(round(prediction)) = count \n                                WHEN true then 1\n                                ELSE 0\n                                END as equal\"\"\")\ndisplay(holdout)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["* Use another `selectExpr` to `display` the proportion of predictions that were exactly correct."],"metadata":{}},{"cell_type":"code","source":["display(holdout.selectExpr(\"sum(equal)/sum(1)\"))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["* Use `RegressionMetrics` to get more insight into the model performance. NOTE: Regression metrics requires input formatted as tuples of `double`s where the first item is the `prediction` and the second item is the observation (in this case the observation is `count`).  Once you have `map`ped these values from `holdout` you can directly pass them to the `RegressionMetrics` constructor."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.evaluation import RegressionMetrics\n\nmapped = holdout.select(\"prediction\", \"count\").rdd.map(lambda x: (float(x[0]), float(x[1])))\nrm = RegressionMetrics(mapped)\n\nprint \"MSE: \", rm.meanSquaredError\nprint \"MAE: \", rm.meanAbsoluteError\nprint \"RMSE Squared: \", rm.rootMeanSquaredError\nprint \"R Squared: \", rm.r2\nprint \"Explained Variance: \", rm.explainedVariance"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["Because these results still aren't very good, rather than training a single-model, let's train several using a pipeline.\n\n* Use a `RandomForestRegressor` algorithm.  This algorithm has several `hyperparameters` that we can tune, rather than tune them individually, we will use a `ParamGridBuilder` to search the \"hyperparameter space\" for us.  This can take some time on small clusters, so be patient.\n\n* Use the `Pipeline` to feed the algorithm into a `CrossValidator` to help prevent \"overfitting\".\n* Use the `CrossValidator` uses a `RegressionEvaluator` to test the model results against a metric (default is RMSE).\n\n* NOTE: In production, using AWS EC2 compute-optimized instance speed this up -- 3 min (c3.4xlarge) vs 10 min (r3.xlarge)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nrfModel = RandomForestRegressor()\\\n  .setLabelCol(\"count\")\\\n  .setFeaturesCol(\"features\")\n  \nparamGrid = ParamGridBuilder()\\\n  .addGrid(rfModel.maxDepth, [5, 10])\\\n  .addGrid(rfModel.numTrees, [20, 60])\\\n  .build()\n\nsteps = [rfModel]\n\npipeline = Pipeline().setStages(steps)\n\ncv = CrossValidator()\\\n  .setEstimator(pipeline)\\\n  .setEstimatorParamMaps(paramGrid)\\\n  .setEvaluator(RegressionEvaluator().setLabelCol(\"count\"))\n\npipelineFitted = cv.fit(training)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["* Access the best model on the `pipelineFitted` object by accessing the first stage of the `bestModel` attribute."],"metadata":{}},{"cell_type":"code","source":["print(\"The Best Parameters:\\n--------------------\")\nprint(pipelineFitted.bestModel.stages[0])"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["* Use the `bestModel` to `transform` the `test` dataset.  \n* Use a `selectExpr` to show the raw prediction, rounded prediction, count, and whether or not the prediction exactly matched (hint: this is the same `selectExpr` you used on the previous model results).\n* Store the results in `holdout2`, then display."],"metadata":{}},{"cell_type":"code","source":["holdout2 = pipelineFitted.bestModel\\\n  .transform(test)\\\n  .selectExpr(\"prediction as raw_prediction\", \\\n    \"double(round(prediction)) as prediction\", \\\n    \"count\", \\\n    \"\"\"CASE double(round(prediction)) = count \n  WHEN true then 1\n  ELSE 0\nEND as equal\"\"\")\n  \ndisplay(holdout2)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["* Show the `RegressionMetrics` for the new model results."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.evaluation import RegressionMetrics\n\nmapped2 = holdout2.select(\"prediction\", \"count\").rdd.map(lambda x: (float(x[0]), float(x[1])))\nrm2 = RegressionMetrics(mapped2)\n\nprint \"MSE: \", rm2.meanSquaredError\nprint \"MAE: \", rm2.meanAbsoluteError\nprint \"RMSE Squared: \", rm2.rootMeanSquaredError\nprint \"R Squared: \", rm2.r2\nprint \"Explained Variance: \", rm2.explainedVariance"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["* See if there an improvement in the \"exactly right\" proportion."],"metadata":{}},{"cell_type":"code","source":["display(holdout2.selectExpr(\"sum(equal)/sum(1)\"))"],"metadata":{},"outputs":[],"execution_count":48}],"metadata":{"name":"8-SparkML","notebookId":3737261653513152},"nbformat":4,"nbformat_minor":0}
