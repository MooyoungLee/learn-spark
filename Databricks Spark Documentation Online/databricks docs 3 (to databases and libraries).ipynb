{"cells":[{"cell_type":"code","source":["%scala\ncase class MapEntry(key: String, value: Int)\nval largeSeries = for (x <- 1 to 5000) yield MapEntry(\"k_%04d\".format(x), x)\nval largeDataFrame = sc.parallelize(largeSeries).toDF()\nlargeDataFrame.registerTempTable(\"largeTable\")\ndisplay(sqlContext.sql(\"select * from largeTable\"))"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%scala\n// Click on the Plot Options Button...to see how this pivot table was configured.\n// NOTE how Pivot Tables are highlighted in green to distinguish them from regular charts.\ncase class PivotEntry(key: String, series_grouping: String, value: Int)\nval largePivotSeries = for (x <- 1 to 5000) yield PivotEntry(\"k_%03d\".format(x % 200),\"group_%01d\".format(x % 3), x)\nval largePivotDataFrame = sc.parallelize(largePivotSeries).toDF()\nlargePivotDataFrame.registerTempTable(\"table_to_be_pivoted\")\ndisplay(sqlContext.sql(\"select * from table_to_be_pivoted\"))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%sql select key, series_grouping, sum(value) from table_to_be_pivoted group by key, series_grouping order by key, series_grouping"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%scala\ncase class SalesEntry(category: String, product: String, year: Int, salesAmount: Double)\nval salesEntryDataFrame = sc.parallelize(\n  SalesEntry(\"fruits_and_vegetables\", \"apples\", 2012, 100.50) :: \n  SalesEntry(\"fruits_and_vegetables\", \"oranges\", 2012, 100.75) :: \n  SalesEntry(\"fruits_and_vegetables\", \"apples\", 2013, 200.25) :: \n  SalesEntry(\"fruits_and_vegetables\", \"oranges\", 2013, 300.65) :: \n  SalesEntry(\"fruits_and_vegetables\", \"apples\", 2014, 300.65) :: \n  SalesEntry(\"fruits_and_vegetables\", \"oranges\", 2015, 100.35) ::\n  SalesEntry(\"butcher_shop\", \"beef\", 2012, 200.50) :: \n  SalesEntry(\"butcher_shop\", \"chicken\", 2012, 200.75) :: \n  SalesEntry(\"butcher_shop\", \"pork\", 2013, 400.25) :: \n  SalesEntry(\"butcher_shop\", \"beef\", 2013, 600.65) :: \n  SalesEntry(\"butcher_shop\", \"beef\", 2014, 600.65) :: \n  SalesEntry(\"butcher_shop\", \"chicken\", 2015, 200.35) ::\n  SalesEntry(\"misc\", \"gum\", 2012, 400.50) :: \n  SalesEntry(\"misc\", \"cleaning_supplies\", 2012, 400.75) :: \n  SalesEntry(\"misc\", \"greeting_cards\", 2013, 800.25) :: \n  SalesEntry(\"misc\", \"kitchen_utensils\", 2013, 1200.65) :: \n  SalesEntry(\"misc\", \"cleaning_supplies\", 2014, 1200.65) :: \n  SalesEntry(\"misc\", \"cleaning_supplies\", 2015, 400.35) ::\n  Nil).toDF()\nsalesEntryDataFrame.registerTempTable(\"test_sales_table\")\ndisplay(sqlContext.sql(\"select * from test_sales_table\"))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql select * from test_sales_table"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sql select * from test_sales_table"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%scala\ncase class StateEntry(state: String, value: Int)\nval stateRDD = sc.parallelize(\n  StateEntry(\"MO\", 1) :: StateEntry(\"MO\", 10) ::\n  StateEntry(\"NH\", 4) ::\n  StateEntry(\"MA\", 8) ::\n  StateEntry(\"NY\", 4) ::\n  StateEntry(\"CA\", 7) ::  Nil).toDF()\nstateRDD.registerTempTable(\"test_state_table\")\ndisplay(sqlContext.sql(\"Select * from test_state_table\"))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%scala\n// Reminder: It's not a requirement to register this RDD as a temp table for Spark SQL - display can also be called directly on the RDD. \ncase class WorldEntry(country: String, value: Int)\nval worldRDD = sc.parallelize(\n  WorldEntry(\"USA\", 1000) ::\n  WorldEntry(\"JPN\", 23) ::\n  WorldEntry(\"GBR\", 23) ::\n  WorldEntry(\"FRA\", 21) ::\n  WorldEntry(\"TUR\", 3) ::\n  Nil).toDF()\ndisplay(worldRDD)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%scala\ncase class ScatterPlotEntry(key: String, a: Double, b: Double, c: Double)\nval scatterPlotRDD = sc.parallelize(\n  ScatterPlotEntry(\"k1\", 0.2, 120, 1) :: ScatterPlotEntry(\"k1\", 0.4, 140, 1) :: ScatterPlotEntry(\"k1\", 0.6, 160, 1) :: ScatterPlotEntry(\"k1\", 0.8, 180, 1) ::\n  ScatterPlotEntry(\"k2\", 0.2, 220, 1) :: ScatterPlotEntry(\"k2\", 0.4, 240, 1) :: ScatterPlotEntry(\"k2\", 0.6, 260, 1) :: ScatterPlotEntry(\"k2\", 0.8, 280, 1) ::\n  ScatterPlotEntry(\"k1\", 1.2, 120, 1) :: ScatterPlotEntry(\"k1\", 1.4, 140, 1) :: ScatterPlotEntry(\"k1\", 1.6, 160, 1) :: ScatterPlotEntry(\"k1\", 1.8, 180, 1) ::\n  ScatterPlotEntry(\"k2\", 1.2, 220, 2) :: ScatterPlotEntry(\"k2\", 1.4, 240, 2) :: ScatterPlotEntry(\"k2\", 1.6, 260, 2) :: ScatterPlotEntry(\"k2\", 1.8, 280, 2) ::\n  ScatterPlotEntry(\"k1\", 2.2, 120, 1) :: ScatterPlotEntry(\"k1\", 2.4, 140, 1) :: ScatterPlotEntry(\"k1\", 2.6, 160, 1) :: ScatterPlotEntry(\"k1\", 2.8, 180, 1) ::\n  ScatterPlotEntry(\"k2\", 2.2, 220, 3) :: ScatterPlotEntry(\"k2\", 2.4, 240, 3) :: ScatterPlotEntry(\"k2\", 2.6, 260, 3) :: ScatterPlotEntry(\"k2\", 2.8, 280, 3) ::\n  Nil).toDF()\ndisplay(scatterPlotRDD)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%scala\nval rng = new scala.util.Random(0)\nval points = sc.parallelize((0L until 1000L).map { x => (x/100.0, 4 * math.sin(x/100.0) + rng.nextGaussian()) }).toDF()\ndisplay(points)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%scala\ncase class HistogramEntry(key1: String, key2: String, value: Double)\nval HistogramRDD = sc.parallelize(\n  HistogramEntry(\"a\", \"x\", 0.2) :: HistogramEntry(\"a\", \"x\", 0.4) :: HistogramEntry(\"a\", \"x\", 0.6) :: HistogramEntry(\"a\", \"x\", 0.8) :: HistogramEntry(\"a\", \"x\", 1.0) ::\n  HistogramEntry(\"b\", \"z\", 0.2) :: HistogramEntry(\"b\", \"x\", 0.4) :: HistogramEntry(\"b\", \"x\", 0.6) :: HistogramEntry(\"b\", \"y\", 0.8) :: HistogramEntry(\"b\", \"x\", 1.0) ::\n  HistogramEntry(\"a\", \"x\", 0.2) :: HistogramEntry(\"a\", \"y\", 0.4) :: HistogramEntry(\"a\", \"x\", 0.6) :: HistogramEntry(\"a\", \"x\", 0.8) :: HistogramEntry(\"a\", \"x\", 1.0) ::\n  HistogramEntry(\"b\", \"x\", 0.2) :: HistogramEntry(\"b\", \"x\", 0.4) :: HistogramEntry(\"b\", \"x\", 0.6) :: HistogramEntry(\"b\", \"z\", 0.8) :: HistogramEntry(\"b\", \"x\", 1.0) ::\n  Nil).toDF()\ndisplay(HistogramRDD)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%scala\ncase class QuantileEntry(key: String, grouping: String, otherField: Int, value: Int)\nval quantileSeries = for (x <- 1 to 5000) yield QuantileEntry(\"key_%01d\".format(x % 4),\"group_%01d\".format(x % 3), x, x*x)\nval quantileSeriesRDD = sc.parallelize(quantileSeries).toDF()\ndisplay(quantileSeriesRDD)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%scala\ncase class QQPlotEntry(key: String, grouping: String, value: Int, value_squared: Int)\nval qqPlotSeries = for (x <- 1 to 5000) yield QQPlotEntry(\"k_%03d\".format(x % 5),\"group_%01d\".format(x % 3), x, x*x)\nval qqPlotRDD = sc.parallelize(qqPlotSeries).toDF()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%scala\ndisplay(qqPlotRDD)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%scala\ndisplay(qqPlotRDD)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%scala\nimport java.util.Random\ncase class BoxEntry(key: String, grouping: String, value: Int)\nval randomGenerator = new Random()\nval boxSeries = for (x <- 1 to 5000) yield BoxEntry(\"key_%01d\".format(x % 2),\"group_%01d\".format(x % 3), randomGenerator.nextInt(x).toInt)\nval boxSeriesRDD = sc.parallelize(boxSeries).toDF()\ndisplay(boxSeriesRDD)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 2*np.pi, 50)\ny = np.sin(x)\ny2 = y + 0.1 * np.random.normal(size=x.shape)\n\nfig, ax = plt.subplots()\nax.plot(x, y, 'k--')\nax.plot(x, y2, 'ro')\n\n# set ticks and tick labels\nax.set_xlim((0, 2*np.pi))\nax.set_xticks([0, np.pi, 2*np.pi])\nax.set_xticklabels(['0', '$\\pi$','2$\\pi$'])\nax.set_ylim((-1.5, 1.5))\nax.set_yticks([-1, 0, 1])\n\n# Only draw spine between the y-ticks\nax.spines['left'].set_bounds(-1, 1)\n# Hide the right and top spines\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n# Only show ticks on the left and bottom spines\nax.yaxis.set_ticks_position('left')\nax.xaxis.set_ticks_position('bottom')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["from ggplot import *\np = ggplot(meat, aes('date','beef')) + \\\n    geom_line(color='black') + \\\n    scale_x_date(breaks=date_breaks('7 years'), labels='%b %Y') + \\\n    scale_y_continuous(labels='comma') + theme_bw()\ndisplay(p)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["from plotly.offline import plot\nfrom plotly.graph_objs import *\nimport numpy as np\n\nx = np.random.randn(2000)\ny = np.random.randn(2000)\n\n# Instead of simply calling plot(...), store your plot as a variable and pass it to displayHTML().\n# Make sure to specify output_type='div' as a keyword argument.\n# (Note that if you call displayHTML() multiple times in the same cell, only the last will take effect.)\n\np = plot(\n  [\n    Histogram2dContour(x=x, y=y, contours=Contours(coloring='heatmap')),\n    Scatter(x=x, y=y, mode='markers', marker=Marker(color='white', size=3, opacity=0.3))\n  ],\n  output_type='div'\n)\n\ndisplayHTML(p)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%scala\nval sparkDF = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/ms7vg26h1498071578743/\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["sparkDF = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/ms7vg26h1498071578743/\")\n"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%r\nsparkDF <- read.df(sqlContext, source = \"csv\", path = \"/FileStore/tables/ms7vg26h1498071578743/\")\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%scala\nval rdd = sc.textFile(\"/FileStore/tables/ms7vg26h1498071578743/\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["rdd = sc.textFile(\"/FileStore/tables/ms7vg26h1498071578743/\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%r\ndf = read.csv(\"/dbfs/FileStore/tables/ms7vg26h1498071578743/airline.csv\", header = TRUE)\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["dbutils.fs.rm(\"dbfs:/FileStore/tables/ms7vg26h1498071578743/\", True)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sh\ncd ..\ncd ..\ncd dbfs/FileStore/tables/\nls"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":28}],"metadata":{"name":"databricks docs 3 (to databases and libraries)","notebookId":1529926929461238},"nbformat":4,"nbformat_minor":0}
