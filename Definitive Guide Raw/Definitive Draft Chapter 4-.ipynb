{"cells":[{"cell_type":"code","source":["%sh cd ..\ncd ..\ncd dbfs/FileStore/tables/tmdv7u711497636580369\nls"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%scala\nval df = spark.read.format(\"csv\")\n.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.load(\"/FileStore/tables/tmdv7u711497636580369/2010_12_01-ec65d.csv\")\ndf.printSchema()\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["df = spark.read.format(\"csv\")\\\n.option(\"header\", \"true\")\\\n.option(\"inferSchema\", \"true\")\\\n.load(\"/FileStore/tables/tmdv7u711497636580369/2010_12_01-ec65d.csv\")\ndf.printSchema()\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.col\ndf.where(col(\"InvoiceNo\").equalTo(536365))\n.select(\"InvoiceNo\", \"Description\")\n.show(5, false)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.col\ndf.where(col(\"InvoiceNo\") === 536365)\n.select(\"InvoiceNo\", \"Description\")\n.show(5, false)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf.where(col(\"InvoiceNo\") != 536365)\\\n.select(\"InvoiceNo\", \"Description\")\\\n.show(5, False)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%scala\nval priceFilter = col(\"UnitPrice\") > 600\nval descripFilter = col(\"Description\").contains(\"POSTAGE\")\ndf.where(col(\"StockCode\").isin(\"DOT\"))\n.where(priceFilter.or(descripFilter))\n.show(5)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql.functions import instr\npriceFilter = col(\"UnitPrice\") > 600\ndescripFilter = instr(df.Description, \"POSTAGE\") >= 1\ndf.where(df.StockCode.isin(\"DOT\"))\\\n.where(priceFilter | descripFilter)\\\n.show(5)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM dfTable\nWHERE\nStockCode in (\"DOT\") AND\n(UnitPrice > 600 OR\ninstr(Description, \"POSTAGE\") >= 1)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%scala\nval DOTCodeFilter = col(\"StockCode\") === \"DOT\"\nval priceFilter = col(\"UnitPrice\") > 600\nval descripFilter = col(\"Description\").contains(\"POSTAGE\")\ndf.withColumn(\"isExpensive\",\nDOTCodeFilter.and(priceFilter.or(descripFilter)))\n.where(\"isExpensive\")\n.select(\"unitPrice\", \"isExpensive\")\n.show(5)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.sql.functions import instr\nDOTCodeFilter = col(\"StockCode\") == \"DOT\"\npriceFilter = col(\"UnitPrice\") > 600\ndescripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\ndf.withColumn(\"isExpensive\",\nDOTCodeFilter & (priceFilter | descripFilter))\\\n.where(\"isExpensive\")\\\n.select(\"unitPrice\", \"isExpensive\")\\\n.show(5)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql\nSELECT\nUnitPrice,\n(StockCode = 'DOT' AND\n(UnitPrice > 600 OR\ninstr(Description, \"POSTAGE\") >= 1)) as isExpensive\nFROM dfTable\nWHERE\n(StockCode = 'DOT' AND\n(UnitPrice > 600 OR\ninstr(Description, \"POSTAGE\") >= 1))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{expr, not, col}\ndf.withColumn(\"isExpensive\", not(col(\"UnitPrice\").leq(250)))\n.filter(\"isExpensive\")\n.select(\"Description\", \"UnitPrice\").show(5)\ndf.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\n.filter(\"isExpensive\")\n.select(\"Description\", \"UnitPrice\").show(5)\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%python\nfrom pyspark.sql.functions import expr\ndf.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\\\n.where(\"isExpensive\")\\\n.select(\"Description\", \"UnitPrice\").show(5)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{expr, pow}\nval fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\ndf.select(\nexpr(\"CustomerId\"),\nfabricatedQuantity.alias(\"realQuantity\"))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.sql.functions import expr, pow\nfabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\ndf.select(\nexpr(\"CustomerId\"),\nfabricatedQuantity.alias(\"realQuantity\"))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%scala\ndf.selectExpr(\n\"CustomerId\",\n\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\")\n.show(2)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["df.selectExpr(\n\"CustomerId\",\n\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\")\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%sql\nSELECT\ncustomerId,\n(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\nFROM dfTable"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{round, bround}\ndf.select(\nround(col(\"UnitPrice\"), 1).alias(\"rounded\"),\ncol(\"UnitPrice\"))\n.show(5)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.lit\ndf.select(\nround(lit(\"2.5\")),\nbround(lit(\"2.5\")))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from pyspark.sql.functions import lit, round, bround\ndf.select(\nround(lit(\"2.5\")),\nbround(lit(\"2.5\")))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%sql\nSELECT\nround(2.5),\nbround(2.5)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{corr}\ndf.stat.corr(\"Quantity\", \"UnitPrice\")\ndf.select(corr(\"Quantity\", \"UnitPrice\")).show()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["from pyspark.sql.functions import corr\ndf.stat.corr(\"Quantity\", \"UnitPrice\")\ndf.select(corr(\"Quantity\", \"UnitPrice\")).show()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%sql\nSELECT\ncorr(Quantity, UnitPrice)\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%scala\ndf.describe().show()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["df.describe().show()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{count, mean, stddev_pop, min, max}"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from pyspark.sql.functions import count, mean, stddev_pop, min, max"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["%scala\nval colName = \"UnitPrice\"\nval quantileProbs = Array(0.5)\nval relError = 0.05\ndf.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["colName = \"UnitPrice\"\nquantileProbs = [0.5]\nrelError = 0.05\ndf.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["%scala\ndf.stat.crosstab(\"StockCode\", \"Quantity\").show()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["df.stat.crosstab(\"StockCode\", \"Quantity\").show()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["%scala\ndf.stat.freqItems(Seq(\"StockCode\", \"Quantity\")).show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["df.stat.freqItems([\"StockCode\", \"Quantity\"]).show()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{initcap}\ndf.select(initcap(col(\"Description\"))).show(2, false)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["from pyspark.sql.functions import initcap\ndf.select(initcap(col(\"Description\"))).show()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["%sql\nSELECT\ninitcap(Description)\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{lower, upper}\ndf.select(\ncol(\"Description\"),\nlower(col(\"Description\")),\nupper(lower(col(\"Description\"))))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["from pyspark.sql.functions import lower, upper\ndf.select(\ncol(\"Description\"),\nlower(col(\"Description\")),\nupper(lower(col(\"Description\"))))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["%sql\nSELECT\nDescription,\nlower(Description),\nUpper(lower(Description))\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}\ndf.select(\nltrim(lit(\" HELLO \")).as(\"ltrim\"),\nrtrim(lit(\" HELLO \")).as(\"rtrim\"),\ntrim(lit(\" HELLO \")).as(\"trim\"),\nlpad(lit(\"HELLO\"), 3, \" \").as(\"lp\"),\nrpad(lit(\"HELLO\"), 10, \" \").as(\"rp\"))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\ndf.select(\nltrim(lit(\" HELLO \")).alias(\"ltrim\"),\nrtrim(lit(\" HELLO \")).alias(\"rtrim\"),\ntrim(lit(\" HELLO \")).alias(\"trim\"),\nlpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\nrpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\"))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["%sql\nSELECT\nltrim(' HELLLOOOO '),\nrtrim(' HELLLOOOO '),\ntrim(' HELLLOOOO '),\nlpad('HELLOOOO ', 3, ' '),\nrpad('HELLOOOO ', 10, ' ')\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.regexp_replace\nval simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\nval regexString = simpleColors.map(_.toUpperCase).mkString(\"|\")\n// the | signifies `OR` in regular expression syntax\ndf.select(\nregexp_replace(col(\"Description\"), regexString, \"COLOR\")\n.alias(\"color_cleaned\"),\ncol(\"Description\"))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_replace\nregex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\ndf.select(\nregexp_replace(col(\"Description\"), regex_string, \"COLOR\")\n.alias(\"color_cleaned\"),\ncol(\"Description\"))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["%sql\nSELECT\nregexp_replace(Description, 'BLACK|WHITE|RED|GREEN|BLUE', 'COLOR') as color_cleaned,\nDescription\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.translate\ndf.select(\ntranslate(col(\"Description\"), \"LEET\", \"1337\"),\ncol(\"Description\"))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["from pyspark.sql.functions import translate\ndf.select(\ntranslate(col(\"Description\"), \"LEET\", \"1337\"),\ncol(\"Description\"))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["%sql\nSELECT\ntranslate(Description, 'LEET', '1337'),\nDescription\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.regexp_extract\nval regexString = simpleColors\n.map(_.toUpperCase)\n.mkString(\"(\", \"|\", \")\")\n// the | signifies OR in regular expression syntax\ndf.select(\nregexp_extract(col(\"Description\"), regexString, 1)\n.alias(\"color_cleaned\"),\ncol(\"Description\"))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_extract\nextract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\ndf.select(\nregexp_extract(col(\"Description\"), extract_str, 1)\n.alias(\"color_cleaned\"),\ncol(\"Description\"))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["%sql\nSELECT\nregexp_extract(Description, '(BLACK|WHITE|RED|GREEN|BLUE)', 1),\nDescription\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["%scala\nval containsBlack = col(\"Description\").contains(\"BLACK\")\nval containsWhite = col(\"DESCRIPTION\").contains(\"WHITE\")\ndf.withColumn(\"hasSimpleColor\", containsBlack.or(containsWhite))\n.filter(\"hasSimpleColor\")\n.select(\"Description\")\n.show(3, false)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["%python\nfrom pyspark.sql.functions import instr\ncontainsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\ncontainsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\ndf.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n.filter(\"hasSimpleColor\")\\\n.select(\"Description\")\\\n.show(3, False)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["%sql\nSELECT\nDescription\nFROM\ndfTable\nWHERE\ninstr(Description, 'BLACK') >= 1 OR\ninstr(Description, 'WHITE') >= 1"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["%scala\nval simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\nval selectedColumns = simpleColors.map(color => {\ncol(\"Description\")\n.contains(color.toUpperCase)\n.alias(s\"is_$color\")\n}):+expr(\"*\") // could also append this value\ndf\n.select(selectedColumns:_*)\n.where(col(\"is_white\").or(col(\"is_red\")))\n.select(\"Description\")\n.show(3, false)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["from pyspark.sql.functions import expr, locate\nsimpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\ndef color_locator(column, color_string):\n  return locate(color_string.upper(), column)\\\n.cast(\"boolean\")\\\n.alias(\"is_\" + c)\nselectedColumns = [color_locator(df.Description, c) for c in simpleColors]\nselectedColumns.append(expr(\"*\")) # has to a be Column type\ndf\\\n.select(*selectedColumns)\\\n.where(expr(\"is_white OR is_red\"))\\\n.select(\"Description\")\\\n.show(3, False)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{current_date, current_timestamp}\nval dateDF = spark.range(10)\n.withColumn(\"today\", current_date())\n.withColumn(\"now\", current_timestamp())\ndateDF.createOrReplaceTempView(\"dateTable\")"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["from pyspark.sql.functions import current_date, current_timestamp\ndateDF = spark.range(10)\\\n.withColumn(\"today\", current_date())\\\n.withColumn(\"now\", current_timestamp())\ndateDF.createOrReplaceTempView(\"dateTable\")\ndateDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{date_add, date_sub}\ndateDF\n.select(\ndate_sub(col(\"today\"), 5),\ndate_add(col(\"today\"), 5))\n.show(1)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["from pyspark.sql.functions import date_add, date_sub\ndateDF\\\n.select(\ndate_sub(col(\"today\"), 5),\ndate_add(col(\"today\"), 5))\\\n.show(1)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["%sql\nSELECT\ndate_sub(today, 5),\ndate_add(today, 5)\nFROM\ndateTable"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{datediff, months_between, to_date}\ndateDF\n.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n.select(datediff(col(\"week_ago\"), col(\"today\")))\n.show(1)\ndateDF\n.select(\nto_date(lit(\"2016-01-01\")).alias(\"start\"),\nto_date(lit(\"2017-05-22\")).alias(\"end\"))\n.select(months_between(col(\"start\"), col(\"end\")))\n.show(1)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["from pyspark.sql.functions import datediff, months_between, to_date\ndateDF\\\n.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n.select(datediff(col(\"week_ago\"), col(\"today\")))\\\n.show(1)\ndateDF\\\n.select(\nto_date(lit(\"2016-01-01\")).alias(\"start\"),\nto_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n.select(months_between(col(\"start\"), col(\"end\")))\\\n.show(1)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["%sql\nSELECT\nto_date('2016-01-01'),\nmonths_between('2016-01-01', '2017-01-01'),\ndatediff('2016-01-01', '2017-01-01')\nFROM\ndateTable"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{to_date, lit}\nspark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\n.select(to_date(col(\"date\")))\n.show(1)"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["%python\nfrom pyspark.sql.functions import to_date, lit\nspark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n.select(to_date(col(\"date\")))\\\n.show(1)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{unix_timestamp, from_unixtime}\nval dateFormat = \"yyyy-dd-MM\"\nval cleanDateDF = spark.range(1)\n.select(\nto_date(unix_timestamp(lit(\"2017-12-11\"), dateFormat).cast(\"timestamp\"))\n.alias(\"date\"),\nto_date(unix_timestamp(lit(\"2017-20-12\"), dateFormat).cast(\"timestamp\"))\n.alias(\"date2\"))\ncleanDateDF.createOrReplaceTempView(\"dateTable2\")"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["from pyspark.sql.functions import unix_timestamp, from_unixtime\ndateFormat = \"yyyy-dd-MM\"\ncleanDateDF = spark.range(1)\\\n.select(\nto_date(unix_timestamp(lit(\"2017-12-11\"), dateFormat).cast(\"timestamp\"))\\\n.alias(\"date\"),\nto_date(unix_timestamp(lit(\"2017-20-12\"), dateFormat).cast(\"timestamp\"))\\\n.alias(\"date2\"))\ncleanDateDF.createOrReplaceTempView(\"dateTable2\")"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["%sql\nSELECT\nto_date(cast(unix_timestamp(date, 'yyyy-dd-MM') as timestamp)),\nto_date(cast(unix_timestamp(date2, 'yyyy-dd-MM') as timestamp)),\nto_date(date)\nFROM\ndateTable2"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["%scala\ncleanDateDF\n.select(\nunix_timestamp(col(\"date\"), dateFormat).cast(\"timestamp\"))\n.show()"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["cleanDateDF\\\n.select(\nunix_timestamp(col(\"date\"), dateFormat).cast(\"timestamp\"))\\\n.show()"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["%scala\ncleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["cleanDateDF.filter(col(\"date2\") > \"'2017-12-12'\").show()"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["df.na.drop()\ndf.na.drop(\"any\")"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM\ndfTable\nWHERE\nDescription IS NOT NULL"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["df.na.drop(\"all\")"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["%scala\ndf.na.drop(\"all\", Seq(\"StockCode\", \"InvoiceNo\"))"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["df.na.fill(\"All Null values become this string\")"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["%scala\ndf.na.fill(5, Seq(\"StockCode\", \"InvoiceNo\"))"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["df.na.fill(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["%scala\nval fillColValues = Map(\n\"StockCode\" -> 5,\n\"Description\" -> \"No Value\"\n)\ndf.na.fill(fillColValues)"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["fill_cols_vals = {\n\"StockCode\": 5,\n\"Description\" : \"No Value\"\n}\ndf.na.fill(fill_cols_vals)"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["%scala\ndf.na.replace(\"Description\", Map(\"\" -> \"UNKNOWN\"))"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\")\ndf.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\")"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.struct\nval complexDF = df\n.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\ncomplexDF.createOrReplaceTempView(\"complexDF\")"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["from pyspark.sql.functions import struct\ncomplexDF = df\\\n.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\ncomplexDF.createOrReplaceTempView(\"complexDF\")"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["complexDF.select(\"complex.Description\")"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["complexDF.select(\"complex.*\")"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["%sql\nSELECT\ncomplex.*\nFROM\ncomplexDF"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.split\ndf.select(split(col(\"Description\"), \" \")).show(2)"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["from pyspark.sql.functions import split\ndf.select(split(col(\"Description\"), \" \")).show(2)"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["%sql\nSELECT\nsplit(Description, ' ')\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["%scala\ndf.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\n.selectExpr(\"array_col[0]\")\n.show(2)"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n.selectExpr(\"array_col[0]\")\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["%sql\nSELECT\nsplit(Description, ' ')[0]\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.array_contains\ndf.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["from pyspark.sql.functions import array_contains\ndf.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["%sql\nSELECT\narray_contains(split(Description, ' '), 'WHITE')\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{split, explode}\ndf.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\n.withColumn(\"exploded\", explode(col(\"splitted\")))\n.select(\"Description\", \"InvoiceNo\", \"exploded\")"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["from pyspark.sql.functions import split, explode\ndf.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n.withColumn(\"exploded\", explode(col(\"splitted\")))\\\n.select(\"Description\", \"InvoiceNo\", \"exploded\")\\\n"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.map\ndf.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n.selectExpr(\"complex_map['Description']\")"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["%sql\nSELECT\nmap(Description, InvoiceNo) as complex_map\nFROM\ndfTable\nWHERE\nDescription IS NOT NULL"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.map\ndf.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n.selectExpr(\"explode(complex_map)\")\n.take(5)"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["%scala\nval jsonDF = spark.range(1)\n.selectExpr(\"\"\"\n'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["jsonDF = spark.range(1)\\\n.selectExpr(\"\"\"\n'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{get_json_object, json_tuple}\njsonDF.select(\nget_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\"),\njson_tuple(col(\"jsonString\"), \"myJSONKey\"))\n.show()"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["from pyspark.sql.functions import get_json_object, json_tuple\njsonDF.select(\nget_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\"),\njson_tuple(col(\"jsonString\"), \"myJSONKey\"))\\\n.show()"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["%scala\njsonDF.selectExpr(\"json_tuple(jsonString, '$.myJSONKey.myJSONValue[1]') as res\")"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.to_json\ndf.selectExpr(\"(InvoiceNo, Description) as myStruct\")\n.select(to_json(col(\"myStruct\")))"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":["from pyspark.sql.functions import to_json\ndf.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n.select(to_json(col(\"myStruct\")))"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.from_json\nimport org.apache.spark.sql.types._\nval parseSchema = new StructType(Array(\nnew StructField(\"InvoiceNo\",StringType,true),\nnew StructField(\"Description\",StringType,true)))\ndf.selectExpr(\"(InvoiceNo, Description) as myStruct\")\n.select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\n.select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\"))"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":["from pyspark.sql.functions import from_json\nfrom pyspark.sql.types import *\nparseSchema = StructType((\nStructField(\"InvoiceNo\",StringType(),True),\nStructField(\"Description\",StringType(),True)))\ndf.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n.select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n.select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\"))\\"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"code","source":["%scala\nval udfExampleDF = spark.range(5).toDF(\"num\")\ndef power3(number:Double):Double = {\nnumber * number * number\n}\npower3(2.0)"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"code","source":["udfExampleDF = spark.range(5).toDF(\"num\")\ndef power3(double_value):\n  return double_value ** 3\npower3(2.0)"],"metadata":{},"outputs":[],"execution_count":121},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.udf\nval power3udf = udf(power3(_:Double):Double)"],"metadata":{},"outputs":[],"execution_count":122},{"cell_type":"code","source":["%scala\nudfExampleDF.select(power3udf(col(\"num\"))).show()"],"metadata":{},"outputs":[],"execution_count":123},{"cell_type":"code","source":["from pyspark.sql.functions import udf\npower3udf = udf(power3)"],"metadata":{},"outputs":[],"execution_count":124},{"cell_type":"code","source":["from pyspark.sql.functions import col\nudfExampleDF.select(power3udf(col(\"num\"))).show()"],"metadata":{},"outputs":[],"execution_count":125},{"cell_type":"code","source":["%scala\nspark.udf.register(\"power3\", power3(_:Double):Double)\nudfExampleDF.selectExpr(\"power3(num)\").show()"],"metadata":{},"outputs":[],"execution_count":126},{"cell_type":"code","source":["udfExampleDF.selectExpr(\"power3(num)\").show()"],"metadata":{},"outputs":[],"execution_count":127},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType, DoubleType\nspark.udf.register(\"power3py\", power3, DoubleType())\nudfExampleDF.selectExpr(\"power3py(num)\").show()"],"metadata":{},"outputs":[],"execution_count":128},{"cell_type":"code","source":["%sql\nSELECT\npower3py(12), -- doesn't work because of return type\npower3(12)"],"metadata":{},"outputs":[],"execution_count":129},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":130}],"metadata":{"name":"Definitive Draft Chapter 4-","notebookId":336718490210141},"nbformat":4,"nbformat_minor":0}
