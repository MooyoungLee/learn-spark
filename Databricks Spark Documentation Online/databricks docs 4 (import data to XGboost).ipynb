{"cells":[{"cell_type":"code","source":["%sh\ncd ..\ncd ..\ncd /dbfs/FileStore/tables/hx392d3y1498085786148\n\nls"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["sparkDF = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/hx392d3y1498085786148\")\nsparkDF.write.saveAsTable(\"tableName\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["sparkDF.createOrReplaceTempView(\"diamonds\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["dataPath = \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\"\nsqlContext.read.format(\"com.databricks.spark.csv\")\\\n  .option(\"header\",\"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(dataPath)\\\n  .createOrReplaceTempView(\"diamonds\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql\nSELECT * FROM diamonds"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["diamonds = sqlContext.sql(\"SELECT * FROM diamonds\")\ndisplay(diamonds.select(\"*\"))\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["diamonds = sqlContext.table(\"diamonds\")\ndisplay(diamonds.select(\"*\"))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%scala\nval diamonds = sqlContext.sql(\"SELECT * FROM diamonds\")\ndisplay(diamonds.select(\"*\"))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%scala\nval diamonds = sqlContext.table(\"diamonds\")\ndisplay(diamonds.select(\"*\"))\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%r\ndiamonds <- sql(sqlContext, \"SELECT * FROM diamonds\")\ndisplay(diamonds)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%sql REFRESH TABLE diamonds\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql DROP TABLE diamonds"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%sh\npip install --upgrade pip\n\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%sh\npip install --upgrade databricks-cli."],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["dbutils.fs.mkdirs(\"/foobar/\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["dbutils.fs.put(\"/foobar/baz.txt\", \"Hello, World!\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["dbutils.fs.head(\"/foobar/baz.txt\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%sh\ncd ..\ncd ..\ncd dbfs/foobar\nls"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["dbutils.fs.rm(\"/foobar/baz.txt\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["sc.parallelize(range(0, 100)).saveAsTextFile(\"/foobar/foo.txt\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(dbutils.fs.ls(\"dbfs:/foobar\"))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["dbutils.fs.ls(\"dbfs:/foobar\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%fs rm -r foobar"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["display(dbutils.fs.ls(\"dbfs:/\"))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["with open(\"/dbfs/tmp/test_dbfs.txt\", 'w') as f:\n  f.write(\"Apache Spark is awesome!\\n\")\n  f.write(\"End of example!\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["with open(\"/dbfs/tmp/test_dbfs.txt\", \"r\") as f_read:\n  for line in f_read:\n    print line"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%scala\nimport scala.io.Source\n\nval filename = \"/dbfs/tmp/test_dbfs.txt\"\nfor (line <- Source.fromFile(filename).getLines()) {\n  println(line)\n}"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["dbutils.fs.help()\n"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["dbutils.notebook.help()\n"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["dbutils.widgets.help()\n"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["dbutils.fs.put(\"/FileStore/my-stuff/my-file.txt\", \"Contents of my file\")"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["displayHTML(\"<img src =FileStore/plots/4bacd56a-6e2a-4b2a-9e2e-a35c49959d4e.png/>\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["display(dbutils.fs.ls(\"dbfs:/FileStore/plots\"))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["![my_test_image](dbfs:/FileStore/plots/4bacd56a-6e2a-4b2a-9e2e-a35c49959d4e.png)"],"metadata":{}},{"cell_type":"code","source":["dbfs_path = \"/FileStore/images/sparkui/\"\nmkdirs_payload = { 'path': dbfs_path }\nresp = client.post('/dbfs/mkdirs', json_params = mkdirs_payload).json()\nprint(resp)\n\nfiles = [f for f in os.listdir('.') if os.path.isfile(f)]\nfor f in files:\n    if \".png\" in f:\n        print(\"Sending png file ...\")\n        image = dbfs_path + f\n        print(image)\n        files = {'file': open(f, 'rb')}\n        put_payload = { 'path' : image, 'overwrite' : 'true' }\n        # push the images to DBFS\n        resp = client.post('/dbfs/put', json_params = put_payload, files_json = files).json()\n        print(resp)\n"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["import sys.process._"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["display(dbutils.fs.ls(\"file:/tmp/d3.v2.min.js\"))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%scala spark.conf.get(\"spark.ssl.enabled\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["import urllib2\nimport json\ninstance = json.loads(urllib2.urlopen('http://169.254.169.254/latest/dynamic/instance-identity/document').read())\ninstance\n"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["import urllib2\npublic_hostname = urllib2.urlopen('http://169.254.169.254/latest/meta-data/public-hostname').read()\npublic_hostname"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["import urllib2\nimport json\n\n# modify this to be the number of Workers you have running (in Clusters UI)\nnumWorkers = 10 \n\nworker_instance_ids = sc.parallelize(xrange(numWorkers)).map(lambda x: \n  (urllib2.urlopen('http://169.254.169.254/latest/meta-data/public-hostname').read(), urllib2.urlopen('http://169.254.169.254/latest/meta-data/public-ipv4').read())\n)\n\nprint worker_instance_ids.distinct().collect()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["import urllib2\nimport json\ninstance = json.loads(urllib2.urlopen('http://169.254.169.254/latest/dynamic/instance-identity/document').read())\ninstance\nSPARK_MASTER_HOSTNAME = urllib2.urlopen('http://169.254.169.254/latest/meta-data/public-hostname').read()\nSPARK_MASTER_HOSTNAME"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["%scala\nvar mySecretKey = \"This is a default log output of my secret key\"\n\ndef setMySecretKey():Unit = {\n  mySecretKey = \"My Actual Secret Key\"\n}\n\nsetMySecretKey()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["mySecretKey = \"This is a default log output of my secret key\"\n\ndef setMySecretKey():\n  mySecretKey = \"My Actual Secret Key\"\n\nsetMySecretKey()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["dbutils.fs.put(\"dbfs:/tmp/simple.c\",\n\"\"\"\n#include <stdio.h>\n\nint main (int argc, char *argv[]) {\n  char str[100];\n\n  while (1) {\n    if (!fgets(str, 100, stdin)) {\n      return 0;\n    }\n    printf(\"Hello, %s\", str);\n  }\n}\n\"\"\", True)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["print dbutils.fs.head(\"dbfs:/tmp/simple.c\")"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["dbutils.fs.cp(\"dbfs:/tmp/simple.c\", \"file:/tmp/simple.c\")"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["dbutils.fs.rm(\"file:/tmp/simple\")"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["import os\nos.system(\"/usr/bin/gcc -o /tmp/simple /tmp/simple.c\")"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["display(dbutils.fs.ls(\"file:/tmp/simple\"))"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["dbutils.fs.cp(\"file:/tmp/simple\", \"dbfs:/tmp/simple\")"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["import os\nimport shutil\n  \nnum_worker_nodes = 1\n\ndef copyFile(filepath):\n  shutil.copyfile(\"/dbfs%s\" % filepath, filepath)\n  os.system(\"chmod u+x %s\" % filepath)\n  \nsc.parallelize(range(0, 2 * (1 + num_worker_nodes))).map(lambda s: copyFile(\"/tmp/simple\")).count()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["names = sc.parallelize([\"Don\", \"Betty\", \"Sally\"])"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["piped = names.pipe(\"/tmp/simple\")"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["print \"test\""],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["piped.collect()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["dbutils.fs.rm(\"dbfs:/tmp/simple.c\")"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["dbutils.fs.rm(\"dbfs:/tmp/simple\")"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["%scala\ndbutils.fs.put(\"dbfs:/tmp/simple.c\",\n\"\"\"\n#include <stdio.h>\n\nint main (int argc, char *argv[]) {\n  char str[100];\n\n  while (1) {\n    if (!fgets(str, 100, stdin)) {\n      return 0;\n    }\n    printf(\"Hello, %s\", str);\n  }\n}\n\"\"\", true)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["%scala\ndbutils.fs.head(\"dbfs:/tmp/simple.c\")\n"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["%scala\ndbutils.fs.cp(\"dbfs:/tmp/simple.c\", \"file:/tmp/simple.c\")"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["%scala\ndbutils.fs.rm(\"file:/tmp/simple\")"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["%scala\nimport scala.sys.process._\n\nval compileOutput = \"/usr/bin/gcc -o /tmp/simple /tmp/simple.c\""],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["%scala\ndisplay(dbutils.fs.ls(\"file:/tmp/\"))"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["%scala\nval numWorkerNodes = 2\nsc.parallelize(1 to 2*numWorkerNodes).map(s => dbutils.fs.cp(\"dbfs:/tmp\", \"file:/tmp\")).count()"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["%scala\nval names = sc.parallelize(Seq(\"Don\", \"Betty\", \"Sally\"))"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["%scala\nval piped = names.pipe(Seq(\"/tmp/simple\"))"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["%scala\npiped.collect().map(println(_))"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["%scala\nimport java.nio.charset.Charset\nCharset.defaultCharset()"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["dbutils.fs.rm(\"dbfs:/tmp/simple.c\")"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["dbutils.fs.rm(\"dbfs:/tmp/simple\")"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["%r\ninstall.packages(\"sparklyr\")\n\n# Load sparklyr package.\nlibrary(sparklyr)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["%r\n# Using devtools to install sparklyr from github\ndevtools::install_github(\"rstudio/sparklyr\")\n\n# Load sparklyr package.\nlibrary(sparklyr)\n"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["%r\nsc <- spark_connect(method = \"databricks\")"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["%r\nlibrary(SparkR)"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["%r\ndetach(\"package:dplyr\")"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["%r\nif (!require(\"sparklyr\")) {\n  install.packages(\"sparklyr\")  \n}"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["%r\nsc <- spark_connect(method = \"databricks\")"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["%r\nlibrary(dplyr)"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["dataPath = \"/databricks-datasets/Rdatasets/data-001/csv/datasets/iris.csv\"\nsqlContext.read.format(\"com.databricks.spark.csv\")\\\n  .option(\"header\",\"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(dataPath)\\\n  .createOrReplaceTempView(\"iris\")"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["%scala\ndisplay(dbutils.fs.ls(\"dbfs:/databricks-datasets/Rdatasets/data-001/csv/datasets/\"))"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["%sql\nSELECT * FROM iris"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["%r\niris <- sql(sqlContext, \"iris\")\ndisplay(iris)"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["%r\niris <- sql(sqlContext, \"SELECT * FROM iris\")\ndisplay(iris)"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["\n\nsrc_tbls(sc)"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["%r\niris  %>% count"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["%r\noptions(repr.plot.height = 600)"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["%r\niris_summary <- iris %>% \n  mutate(Sepal_Width = ROUND(Sepal_Width * 2) / 2) %>% # Bucketizing Sepal_Width\n  group_by(Species, Sepal_Width) %>% \n  summarize(count = n(), Sepal_Length = mean(Sepal_Length), stdev = sd(Sepal_Length)) %>% collect"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["%r\nlibrary(ggplot2)\n\nggplot(iris_summary, aes(Sepal_Width, Sepal_Length, color = Species)) + \n  geom_line(size = 1.2) +\n  geom_errorbar(aes(ymin = Sepal_Length - stdev, ymax = Sepal_Length + stdev), width = 0.05) +\n  geom_text(aes(label = count), vjust = -0.2, hjust = 1.2, color = \"black\") +\n  theme(legend.position=\"top\")"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["%sh /home/ubuntu/databricks/python/bin/pip install xgboost --pre"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["%sh\nsudo apt-get update\nsudo apt-get install -y maven\nsudo git clone --recursive https://github.com/dmlc/xgboost\n\ncd xgboost\nsudo make -j4\n\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64/\n\ncd jvm-packages\nsudo mvn package"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["%sh\nscp -P 2200 your-instance-info.amazonaws.com:/home/ubuntu/xgboost/jvm-packages/xgboost4j-spark/target/xgboost4j-spark-0.7.jar .\n\nscp -P 2200 your-instance-info.amazonaws.com:/home/ubuntu/xgboost/jvm-packages/xgboost4j/target/xgboost4j-0.7.jar .\n"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["%sh \n\nsudo apt-get update\nsudo apt-get install -y maven\nsudo git clone --recursive https://github.com/dmlc/xgboost\n\ncd xgboost\nsudo make -j4\n\nexport JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64/\n\ncd jvm-packages\nsudo mvn package"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["%sh ls /databricks/driver/xgboost/jvm-packages/xgboost4j-spark/target/"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["%sh \n\nscp -P 2200 /databricks/driver/xgboost/jvm-packages/xgboost4j-spark/target/xgboost4j-spark-0.7.jar /dbfs/mnt/dbJars\n\nscp -P 2200 /databricks/driver/xgboost/jvm-packages/xgboost4j-spark/target/xgboost4j-spark-0.7-jar-with-dependencies.jar /dbfs/mnt/dbJars"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["%scala\nimport ml.dmlc.xgboost4j.scala.spark.{DataUtils, XGBoost}"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["%scala\nsql(\"drop table if exists power_plant\")\ncase class PowerPlantTable(AT: Double, V : Double, AP : Double, RH : Double, PE : Double)\nval powerPlantData = sc.textFile(\"dbfs:/databricks-datasets/power-plant/data/\")\n  .map(x => x.split(\"\\t\"))\n  .filter(line => line(0) != \"AT\")\n  .map(line => PowerPlantTable(line(0).toDouble, line(1).toDouble, line(2).toDouble, line(3).toDouble, line(4).toDouble))\n  .toDF\n  .write\n  .saveAsTable(\"power_plant\")\n\nval dataset = sqlContext.table(\"power_plant\")"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.VectorAssembler\n\nval assembler =  new VectorAssembler()\n  .setInputCols(Array(\"AT\", \"V\", \"AP\", \"RH\"))\n  .setOutputCol(\"features\")\n\nval vected = assembler.transform(dataset).withColumnRenamed(\"PE\", \"label\").drop(\"AT\",\"V\",\"AP\",\"RH\")"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["%scala\nval Array(split20, split80) = vected.randomSplit(Array(0.20, 0.80), 1800009193L)\nval testSet = split20.cache()\nval trainingSet = split80.cache()"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["%scala\nval paramMap = List(\n  \"eta\" -> 0.3,\n  \"max_depth\" -> 6,\n  \"objective\" -> \"reg:linear\",\n  \"early_stopping_rounds\" ->10).toMap\n\nval xgboostModel = XGBoost.trainWithDataFrame(trainingSet, paramMap, 30, 10, useExternalMemory=true)"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["%scala\nval predictions = xgboostModel.transform(testSet)\ndisplay(predictions)"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\n\nval evaluator = new RegressionEvaluator()\n  .setLabelCol(\"label\")\n  .setPredictionCol(\"prediction\")\n  .setMetricName(\"rmse\")\n\nval rmse = evaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["%scala\nprint (\"Root mean squared error: \" + rmse)"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["%scala\nxgboostModel.save(\"/tmp/myXgboostModel\")"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["%scala\nimport ml.dmlc.xgboost4j.scala.spark.XGBoostEstimator\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\n\nval xgboostEstimator = new XGBoostEstimator(\n        Map[String, Any](\"num_round\" -> 30, \"nWorkers\" -> 10, \"objective\" -> \"reg:linear\", \"eta\" -> 0.3, \"max_depth\" -> 6, \"early_stopping_rounds\" -> 10))\n\n// construct the pipeline       \nval pipeline = new Pipeline()\n      .setStages(Array(assembler, xgboostEstimator))"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["%scala\nval pipelineData = dataset.withColumnRenamed(\"PE\",\"label\")\nval pipelineModel = pipeline.fit(pipelineData)"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["%scala\ndisplay(pipelineModel.transform(pipelineData))"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder}\n\nval paramGrid = new ParamGridBuilder()\n      .addGrid(xgboostEstimator.maxDepth, Array(4, 7))\n      .addGrid(xgboostEstimator.eta, Array(0.1, 0.6))\n      .build()\n\nval cv = new CrossValidator()\n      .setEstimator(xgboostEstimator)\n      .setEvaluator(evaluator)\n      .setEstimatorParamMaps(paramGrid)\n      .setNumFolds(4)\n\nval cvModel = cv.fit(trainingSet)"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["%scala\ncvModel.bestModel.extractParamMap"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["%scala\nval results = cvModel.transform(testSet)\nevaluator.evaluate(results)"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["%scala\ncvModel.bestModel.asInstanceOf[ml.dmlc.xgboost4j.scala.spark.XGBoostModel].save(\"/tmp/xgboostTunedModel\")"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["%scala\npipelineModel.write.overwrite().save(\"/tmp/xgPipeline\")"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["%scala\npipelineModel.stages"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.PipelineModel\n\nval loadedPipeline = PipelineModel.load(\"/tmp/xgPipeline\")"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["%scala\nloadedPipeline"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["%fs ls /tmp/xgPipeline/stages"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":117}],"metadata":{"name":"databricks docs 4 (import data to XGboost)","notebookId":4289818323227945},"nbformat":4,"nbformat_minor":0}
