{"cells":[{"cell_type":"code","source":["%scala\nval df = spark.read.load(\"/FileStore/tables/m4i55ooq1497652004973/part_r_00000_8c34224c_f0ce_46e1_a271_791df66b7ae9_gz-ec310.parquet\")"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["df = spark.read.load(\"/FileStore/tables/m4i55ooq1497652004973/part_r_00000_8c34224c_f0ce_46e1_a271_791df66b7ae9_gz-ec310.parquet\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.regression.LinearRegression\nval lr = new LinearRegression()\n.setMaxIter(10)\n.setRegParam(0.3)\n.setElasticNetParam(0.8)\nval lrModel = lr.fit(df)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression\nlr = LinearRegression()\\\n.setMaxIter(10)\\\n.setRegParam(0.3)\\\n.setElasticNetParam(0.8)\nlrModel = lr.fit(df)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%scala\nval summary = lrModel.summary\nsummary.residuals.show()\nsummary.totalIterations\nsummary.objectiveHistory\nsummary.rootMeanSquaredError\nsummary.r2"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%python\nsummary = lrModel.summary\nsummary.residuals.show()\nsummary.totalIterations\nsummary.objectiveHistory\nsummary.rootMeanSquaredError\nsummary.r2"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.regression.GeneralizedLinearRegression\nval glr = new GeneralizedLinearRegression()\n.setFamily(\"gaussian\")\n.setLink(\"identity\")\n.setMaxIter(10)\n.setRegParam(0.3)\n.setLinkPredictionCol(\"linkOut\")\nval glrModel = glr.fit(df)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.ml.regression import GeneralizedLinearRegression\nglr = GeneralizedLinearRegression()\\\n.setFamily(\"gaussian\")\\\n.setLink(\"identity\")\\\n.setMaxIter(10)\\\n.setRegParam(0.3)\\\n.setLinkPredictionCol(\"linkOut\")\nglrModel = glr.fit(df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%scala\nval summary = glrModel.summary"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["glrModel.summary"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.regression.DecisionTreeRegressor"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.ml.regression.DecisionTreeRegressor"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.regression.RandomForestRegressor\nimport org.apache.spark.ml.regression.GBTRegressor"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.regression import GBTRegressor"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.regression.AFTSurvivalRegression\nval AFT = new AFTSurvivalRegression()\n.setFeaturesCol(\"features\")\n.setCensorCol(\"censor\")\n.setQuantileProbabilities(Array(0.5, 0.5))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["from pyspark.ml.regression import AFTSurvivalRegression\nAFT = AFTSurvivalRegression()\\\n.setFeaturesCol(\"features\")\\\n.setCensorCol(\"censor\")\\\n.setQuantileProbabilities([0.5, 0.5])"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.regression.IsotonicRegression\nval ir = new IsotonicRegression().setIsotonic(true)\nval model = ir.fit(df)\nprintln(s\"Boundaries in increasing order: ${model.boundaries}\\n\")\nprintln(s\"Predictions associated with the boundaries: ${model.predictions}\\n\")"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["from pyspark.ml.regression import IsotonicRegression\nir = IsotonicRegression().setIsotonic(True)\nmodel = ir.fit(df)\nmodel.boundaries\nmodel.predictions"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.evaluation.RegressionEvaluator"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%scala\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\nval out = lrModel.transform(df)\n.select(\"prediction\", \"label\")\n.rdd\n.map(x => (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))\nval metrics = new RegressionMetrics(out)\n// Squared error\nprintln(s\"MSE = ${metrics.meanSquaredError}\")\nprintln(s\"RMSE = ${metrics.rootMeanSquaredError}\")\n// R-squared\nprintln(s\"R-squared = ${metrics.r2}\")\n// Mean absolute error\nprintln(s\"MAE = ${metrics.meanAbsoluteError}\")\n// Explained variance\nprintln(s\"Explained variance = ${metrics.explainedVariance}\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from pyspark.mllib.evaluation import RegressionMetrics\nout = lrModel.transform(df)\\\n.select(\"prediction\", \"label\")\\\n.rdd\\\n.map(lambda x: (float(x[0]), float(x[1])))\nmetrics = RegressionMetrics(out)\n%python\nmetrics.meanSquaredError"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"Definitive Draft Chapter 18","notebookId":2060825016293912},"nbformat":4,"nbformat_minor":0}
