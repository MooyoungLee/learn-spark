{"cells":[{"cell_type":"code","source":["%scala\ncase class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%scala\nval flightsDF = spark.read.parquet(\"FileStore/tables/6jc7prea1497642653485/part_r_00000_1a9822ba_b8fb_4d8e_844a_ea30d0801b9e_gz-11168.parquet\")\nval flights = flightsDF.as[Flight]"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%scala\nflights.take(2)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%scala\nflights.first.DEST_COUNTRY_NAME"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%scala\ndef originIsDestination(flight_row: Flight): Boolean = {\nreturn flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME\n}"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%scala\nflights.filter(flight_row => originIsDestination(flight_row)).first()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%scala\nflights.collect().filter(flight_row => originIsDestination(flight_row))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%scala\nval destinations = flights.map(f => f.DEST_COUNTRY_NAME)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%scala\nval localDestinations = destinations.take(10)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%scala\ncase class FlightMetadata(count: BigInt, randomData: BigInt)\nval flightsMeta = spark.range(500)\n.map(x => (x, scala.util.Random.nextLong))\n.withColumnRenamed(\"_1\", \"count\")\n.withColumnRenamed(\"_2\", \"randomData\")\n.as[FlightMetadata]\nval flights2 = flights\n.joinWith(flightsMeta,\nflights.col(\"count\") === flightsMeta.col(\"count\"))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%scala\nflights2.selectExpr(\"_1.DEST_COUNTRY_NAME\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%scala\nflights2.take(2)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%scala\nval flights2 = flights.join(flightsMeta, Seq(\"count\"))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%scala\nval flights2 = flights.join(flightsMeta.toDF(), Seq(\"count\"))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%scala\ns\"${sc.uiWebUrl.get}/api/v1/applications/${sc.applicationId}\"\nflights.groupBy(\"DEST_COUNTRY_NAME\").count()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%scala\nflights.groupByKey(x => x.DEST_COUNTRY_NAME).count()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%scala\nflights.groupByKey(x => x.DEST_COUNTRY_NAME).count().explain"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%scala\ndef grpSum(countryName:String, values: Iterator[Flight]) = {\nvalues.dropWhile(_.count < 5).map(x => (countryName, x))\n}\nflights.groupByKey(x => x.DEST_COUNTRY_NAME).flatMapGroups(grpSum).take(5)\ndef grpSum2(f:Flight):Integer = {\n1\n}\nflights.groupByKey(x => x.DEST_COUNTRY_NAME).mapValues(grpSum2).count().take(5)\ndef sum2(left:Flight, right:Flight) = {\nFlight(left.DEST_COUNTRY_NAME, null, left.count + right.count)\n}\nflights.groupByKey(x => x.DEST_COUNTRY_NAME).reduceGroups((l, r) => sum2(l, r)).take(5)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%scala\nflights.groupBy(\"DEST_COUNTRY_NAME\").count().explain"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%scala\ncase class Transaction(customerId: BigInt, amount: Integer, unitCost:Double, itemId: BigInt)\ncase class Receipt(customerId: BigInt, totalCost: Double)\nval localTransactions = Seq(\nTransaction(1, 5, 5.5, 37),\nTransaction(1, 10, 8.24, 67),\nTransaction(1, 1, 3.5, 22)\n)\nval SparkTransactions = localTransactions.toDF().as[Transaction]\ndef isBigTransaction(transaction: Transaction) = {\n(transaction.amount * transaction.unitCost) > 15\n}"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%scala\nlocalTransactions.filter(isBigTransaction(_))\nSparkTransactions.filter(isBigTransaction(_))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%scala\nimport org.apache.spark.SparkConf\nval myConf = new SparkConf().setAppName(\"My Application\")\nspark.sparkContext"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%scala\nimport org.apache.spark.SparkContext\nval sc = SparkContext.getOrCreate()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["%scala\nval myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\".split(\" \")\nval words = spark.sparkContext.parallelize(myCollection, 2)\nwords.setName(\"myWords\")\nwords.name\nwords.distinct().count()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%scala\ndef startsWithS(individual:String) = {\nindividual.startsWith(\"S\")\n}\nval onlyS = words.filter(word => startsWithS(word))\nonlyS.collect()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%scala\nval words2 = words.map(word => (word, word(0), word.startsWith(\"S\")))\nwords2.filter(record => record._3).take(5)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%scala\nval characters = words.flatMap(word => word.toSeq)\ncharacters.take(5)\nwords.sortBy(word => word.length() * -1).take(2)\nval fiftyFiftySplit = words.randomSplit(Array[Double](0.5, 0.5))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%scala\nsc.parallelize(1 to 20).reduce(_ + _)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%scala\ndef wordLengthReducer(leftWord:String, rightWord:String): String = {\nif (leftWord.length >= rightWord.length)\nreturn leftWord\nelse\nreturn rightWord\n}\nwords.reduce(wordLengthReducer)\nwords.count()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%scala\nval confidence = 0.95\nval timeoutMilliseconds = 400\nwords.countApprox(timeoutMilliseconds, confidence)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["%scala\nwords.countApproxDistinct(0.05)\n"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["%scala\nwords.countApproxDistinct(4, 10)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["%scala\nwords.countByValue()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["%scala\nwords.countByValueApprox(1000, 0.95)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["%scala\nwords.first()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%scala\nsc.parallelize(1 to 20).max()\nsc.parallelize(1 to 20).min()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["%scala\nwords.take(5)\nwords.takeOrdered(5)\nwords.top(5)\nval withReplacement = true\nval numberToTake = 6\nval randomSeed = 100L\nwords.takeSample(withReplacement, numberToTake, randomSeed)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%scala\nwords.saveAsTextFile(\"file:/tmp/bookTitle\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["%scala\nimport org.apache.hadoop.io.compress.BZip2Codec\nwords.saveAsTextFile(\"file:/tmp/bookTitleCompressed\", classOf[BZip2Codec])"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["%scala\nwords.saveAsObjectFile(\"file:/tmp/my/sequenceFilePath\")"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["%scala\nwords.cache()\nwords.getStorageLevel"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["%scala\nspark.range(10).rdd\nspark.range(10).toDF().rdd\nspark.range(10).toDF().rdd.map(rowObject => rowObject.getLong(0))\nspark.range(10).rdd.toDF()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["spark.range(10).rdd.toDF()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":44}],"metadata":{"name":"Definitive Draft Chapter 9-11","notebookId":2060825016293633},"nbformat":4,"nbformat_minor":0}
