{"cells":[{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.recommendation.ALS\nval ratings = spark.read.textFile(\"/FileStore/tables/0y03ep1o1497652390466/sample.txt\")\n.selectExpr(\"split(value , '::') as col\")\n.selectExpr(\n\"cast(col[0] as int) as userId\",\n\"cast(col[1] as int) as movieId\",\n\"cast(col[1] as float) as rating\",\n\"cast(col[1] as long) as timestamp\")\nval Array(training, test) = ratings.randomSplit(Array(0.8, 0.2))\nval als = new ALS()\n.setMaxIter(5)\n.setRegParam(0.01)\n.setUserCol(\"userId\")\n.setItemCol(\"movieId\")\n.setRatingCol(\"rating\")\nval alsModel = als.fit(training)\nval predictions = alsModel.transform(test)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\nfrom pyspark.sql import Row\nratings = spark.read.text(\"/FileStore/tables/0y03ep1o1497652390466/sample.txt\")\\\n.rdd.toDF()\\\n.selectExpr(\"split(value , '::') as col\")\\\n.selectExpr(\n\"cast(col[0] as int) as userId\",\n\"cast(col[1] as int) as movieId\",\n\"cast(col[1] as float) as rating\",\n\"cast(col[1] as long) as timestamp\")\ntraining, test = ratings.randomSplit([0.8, 0.2])\nals = ALS()\\\n.setMaxIter(5)\\\n.setRegParam(0.01)\\\n.setUserCol(\"userId\")\\\n.setItemCol(\"movieId\")\\\n.setRatingCol(\"rating\")\nalsModel = als.fit(training)\npredictions = alsModel.transform(test)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nval evaluator = new RegressionEvaluator()\n.setMetricName(\"rmse\")\n.setLabelCol(\"rating\")\n.setPredictionCol(\"prediction\")\nval rmse = evaluator.evaluate(predictions)\nprintln(s\"Root-mean-square error = $rmse\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\nevaluator = RegressionEvaluator()\\\n.setMetricName(\"rmse\")\\\n.setLabelCol(\"rating\")\\\n.setPredictionCol(\"prediction\")\nrmse = evaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%scala\nimport org.apache.spark.mllib.evaluation.{\nRankingMetrics,\nRegressionMetrics}\nval regComparison = predictions.select(\"rating\", \"prediction\")\n.rdd\n.map(x => (\nx(0).asInstanceOf[Float].toDouble,\nx(1).asInstanceOf[Float].toDouble))\nval metrics = new RegressionMetrics(regComparison)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.mllib.evaluation import RegressionMetrics\nregComparison = predictions.select(\"rating\", \"prediction\")\\\n.rdd\\\n.map(lambda x: (float(x(0)), float(x(1))))\nmetrics = RegressionMetrics(regComparison)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%scala\nimport org.apache.spark.mllib.evaluation.{RankingMetrics, RegressionMetrics}\nimport org.apache.spark.sql.functions.{col, expr}\nval perUserActual = predictions\n.where(\"rating > 2.5\")\n.groupBy(\"userId\")\n.agg(expr(\"collect_set(movieId) as movies\"))\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.mllib.evaluation import RankingMetrics, RegressionMetrics\nfrom pyspark.sql.functions import col, expr\nperUserActual = predictions\\\n.where(\"rating > 2.5\")\\\n.groupBy(\"userId\")\\\n.agg(expr(\"collect_set(movieId) as movies\"))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%scala\nval perUserPredictions = predictions\n.orderBy(col(\"userId\"), col(\"prediction\").desc)\n.groupBy(\"userId\")\n.agg(expr(\"collect_list(movieId) as movies\"))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["perUserPredictions = predictions\\\n.orderBy(col(\"userId\"), expr(\"prediction DESC\"))\\\n.groupBy(\"userId\")\\\n.agg(expr(\"collect_list(movieId) as movies\"))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%scala\nval perUserActualvPred = perUserActual.join(perUserPredictions, Seq(\"userId\"))\n.map(row => (\nrow(1).asInstanceOf[Seq[Integer]].toArray,\nrow(2).asInstanceOf[Seq[Integer]].toArray.take(15)\n))\nval ranks = new RankingMetrics(perUserActualvPred.rdd)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["perUserActualvPred = perUserActual.join(perUserPredictions, [\"userId\"]).rdd\\\n.map(lambda row: (row[1], row[2][:15]))\nranks = RankingMetrics(perUserActualvPred)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%scala\nranks.meanAveragePrecision\nranks.precisionAt(5)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["ranks.meanAveragePrecision\nranks.precisionAt(2)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"name":"Definitive Draft Chapter 19","notebookId":2060825016293937},"nbformat":4,"nbformat_minor":0}
