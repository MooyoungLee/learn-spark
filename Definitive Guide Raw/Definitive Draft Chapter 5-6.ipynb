{"cells":[{"cell_type":"code","source":["%sh cd ..\ncd ..\ncd dbfs/FileStore/tables/1x42p66f1497639007910\nls"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%scala\nval df = spark.read.format(\"csv\")\n.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.load(\"/FileStore/tables/1x42p66f1497639007910/online_retail_dataset-92e8e.csv\")\n.coalesce(5)\ndf.cache()\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["df = spark.read.format(\"csv\")\\\n.option(\"header\", \"true\")\\\n.option(\"inferSchema\", \"true\")\\\n.load(\"/FileStore/tables/1x42p66f1497639007910/online_retail_dataset-92e8e.csv\")\\\n.coalesce(5)\ndf.cache()\ndf.createOrReplaceTempView(\"dfTable\")\ndf.count()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.count\ndf.select(count(\"StockCode\")).collect()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql.functions import count\ndf.select(count(\"StockCode\")).collect()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sql\nSELECT COUNT(*) FROM dfTable"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.countDistinct\ndf.select(countDistinct(\"StockCode\")).collect()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\ndf.select(countDistinct(\"StockCode\")).collect()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sql\nSELECT COUNT(DISTINCT *) FROM DFTABLE"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.approx_count_distinct\ndf.select(approx_count_distinct(\"StockCode\", 0.1)).collect()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.sql.functions import approx_count_distinct\ndf.select(approx_count_distinct(\"StockCode\", 0.1)).collect()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql\nSELECT approx_count_distinct(StockCode, 0.1) FROM DFTABLE"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{first, last}\ndf.select(first(\"StockCode\"), last(\"StockCode\")).collect()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql.functions import first, last\ndf.select(first(\"StockCode\"), last(\"StockCode\")).collect()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%sql\nSELECT first(StockCode), last(StockCode) FROM dfTable"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{min, max}\ndf.select(min(\"Quantity\"), max(\"Quantity\")).collect()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql.functions import min, max\ndf.select(min(\"Quantity\"), max(\"Quantity\")).collect()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%sql\nSELECT min(Quantity), max(Quantity) FROM dfTable"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.sum\ndf.select(sum(\"Quantity\")).show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["from pyspark.sql.functions import sum\ndf.select(sum(\"Quantity\")).show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%sql\nSELECT sum(Quantity) FROM dfTable"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.sumDistinct\ndf.select(sumDistinct(\"Quantity\")).show()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["from pyspark.sql.functions import sumDistinct\ndf.select(sumDistinct(\"Quantity\")).show()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["%sql\nSELECT SUM(DISTINCT Quantity) FROM dfTable"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{sum, count, avg, expr}\ndf.select(\ncount(\"Quantity\").alias(\"total_transactions\"),\nsum(\"Quantity\").alias(\"total_purchases\"),\navg(\"Quantity\").alias(\"avg_purchases\"),\nexpr(\"mean(Quantity)\").alias(\"mean_purchases\"))\n.selectExpr(\n\"total_purchases/total_transactions\",\n\"avg_purchases\",\n\"mean_purchases\")\n.collect()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from pyspark.sql.functions import sum, count, avg, expr\ndf.select(\ncount(\"Quantity\").alias(\"total_transactions\"),\nsum(\"Quantity\").alias(\"total_purchases\"),\navg(\"Quantity\").alias(\"avg_purchases\"),\nexpr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n.selectExpr(\n\"total_purchases/total_transactions\",\n\"avg_purchases\",\n\"mean_purchases\")\\\n.collect()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{var_pop, stddev_pop}\nimport org.apache.spark.sql.functions.{var_samp, stddev_samp}\ndf.select(\n  var_pop(\"Quantity\"),\nvar_samp(\"Quantity\"),\nstddev_pop(\"Quantity\"),\nstddev_samp(\"Quantity\"))\n.collect()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from pyspark.sql.functions import var_pop, stddev_pop\nfrom pyspark.sql.functions import var_samp, stddev_samp\ndf.select(\nvar_pop(\"Quantity\"),\nvar_samp(\"Quantity\"),\nstddev_pop(\"Quantity\"),\nstddev_samp(\"Quantity\"))\\\n.collect()"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%sql\nSELECT\nvar_pop(Quantity),\nvar_samp(Quantity),\nstddev_pop(Quantity),\nstddev_samp(Quantity)\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{skewness, kurtosis}\ndf.select(\nskewness(\"Quantity\"),\nkurtosis(\"Quantity\"))\n.collect()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["from pyspark.sql.functions import skewness, kurtosis\ndf.select(\nskewness(\"Quantity\"),\nkurtosis(\"Quantity\"))\\\n.collect()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["%sql\nSELECT\nskewness(Quantity),\nkurtosis(Quantity)\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{corr, covar_pop, covar_samp}\ndf.select(\ncorr(\"InvoiceNo\", \"Quantity\"),\ncovar_samp(\"InvoiceNo\", \"Quantity\"),\ncovar_pop(\"InvoiceNo\", \"Quantity\"))\n.show()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["from pyspark.sql.functions import corr, covar_pop, covar_samp\ndf.select(\ncorr(\"InvoiceNo\", \"Quantity\"),\ncovar_samp(\"InvoiceNo\", \"Quantity\"),\ncovar_pop(\"InvoiceNo\", \"Quantity\"))\\\n.show()"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["%sql\nSELECT\ncorr(InvoiceNo, Quantity),\ncovar_samp(InvoiceNo, Quantity),\ncovar_pop(InvoiceNo, Quantity)\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{collect_set, collect_list}\ndf.agg(\ncollect_set(\"Country\"),\ncollect_list(\"Country\"))\n.show()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["from pyspark.sql.functions import collect_set, collect_list\ndf.agg(\ncollect_set(\"Country\"),\ncollect_list(\"Country\"))\\\n.show()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%sql\nSELECT\ncollect_set(Country),\ncollect_set(Country)\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["df.groupBy(\"invoiceNo\").count().show()"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["%scala\ndf.groupBy(\"InvoiceNo\", \"CustomerId\")\n.count()\n.show()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["%sql\nSELECT\ncount(*)\nFROM\ndfTable\nGROUP BY\nInvoiceNo, CustomerId"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.count\ndf.groupBy(\"InvoiceNo\")\n.agg(\ncount(\"Quantity\").alias(\"quan\"),\nexpr(\"count(Quantity)\"))\n.show()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["from pyspark.sql.functions import count\ndf.groupBy(\"InvoiceNo\")\\\n.agg(\ncount(\"Quantity\").alias(\"quan\"),\nexpr(\"count(Quantity)\"))\\\n.show()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["%scala\ndf.groupBy(\"InvoiceNo\")\n.agg(\n\"Quantity\" ->\"avg\",\n\"Quantity\" -> \"stddev_pop\")\n.show()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["df.groupBy(\"InvoiceNo\")\\\n.agg(expr(\"avg(Quantity)\"),\nexpr(\"stddev_pop(Quantity)\"))\\\n.show()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%sql\nSELECT\navg(Quantity),\nstddev_pop(Quantity),\nInvoiceNo\nFROM\ndfTable\nGROUP BY\nInvoiceNo"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.col\nval dfWithDate = df.withColumn(\"date\", col(\"InvoiceDate\").cast(\"date\"))\ndfWithDate.createOrReplaceTempView(\"dfWithDate\")"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndfWithDate = df.withColumn(\"date\", col(\"InvoiceDate\").cast(\"date\"))\ndfWithDate.createOrReplaceTempView(\"dfWithDate\")"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions.col\nval windowSpec = Window\n.partitionBy(\"CustomerId\", \"date\")\n.orderBy(col(\"Quantity\").desc)\n.rowsBetween(Window.unboundedPreceding, Window.currentRow)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["from pyspark.sql.window import Window\nfrom pyspark.sql.functions import desc\nwindowSpec = Window\\\n.partitionBy(\"CustomerId\", \"date\")\\\n.orderBy(desc(\"Quantity\"))\\\n.rowsBetween(Window.unboundedPreceding, Window.currentRow)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.max\nval maxPurchaseQuantity = max(col(\"Quantity\"))\n.over(windowSpec)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["from pyspark.sql.functions import max\nmaxPurchaseQuantity = max(col(\"Quantity\"))\\\n.over(windowSpec)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{dense_rank, rank}\nval purchaseDenseRank = dense_rank()\n.over(windowSpec)\nval purchaseRank = rank()\n.over(windowSpec)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["from pyspark.sql.functions import dense_rank, rank\npurchaseDenseRank = dense_rank()\\\n.over(windowSpec)\npurchaseRank = rank()\\\n.over(windowSpec)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.col\ndfWithDate\n.where(\"CustomerId IS NOT NULL\")\n.orderBy(\"CustomerId\")\n.select(\ncol(\"CustomerId\"),\ncol(\"date\"),\ncol(\"Quantity\"),\npurchaseRank.alias(\"quantityRank\"),\npurchaseDenseRank.alias(\"quantityDenseRank\"),\nmaxPurchaseQuantity.alias(\"maxPurchaseQuantity\"))\n.show()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndfWithDate\\\n.where(\"CustomerId IS NOT NULL\")\\\n.orderBy(\"CustomerId\")\\\n.select(\ncol(\"CustomerId\"),\ncol(\"date\"),\ncol(\"Quantity\"),\npurchaseRank.alias(\"quantityRank\"),\npurchaseDenseRank.alias(\"quantityDenseRank\"),\nmaxPurchaseQuantity.alias(\"maxPurchaseQuantity\"))\\\n.show()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["%sql\nSELECT\nCustomerId,\ndate,\nQuantity,\nrank(Quantity) OVER (PARTITION BY CustomerId, date\nORDER BY Quantity DESC NULLS LAST\nROWS BETWEEN\nUNBOUNDED PRECEDING AND\nCURRENT ROW) as rank,\ndense_rank(Quantity) OVER (PARTITION BY CustomerId, date\nORDER BY Quantity DESC NULLS LAST\nROWS BETWEEN\nUNBOUNDED PRECEDING AND\nCURRENT ROW) as dRank,\nmax(Quantity) OVER (PARTITION BY CustomerId, date\nORDER BY Quantity DESC NULLS LAST\nROWS BETWEEN\nUNBOUNDED PRECEDING AND\nCURRENT ROW) as maxPurchase\nFROM\ndfWithDate\nWHERE\nCustomerId IS NOT NULL\nORDER BY\nCustomerId"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["%scala\nval rolledUpDF = dfWithDate.rollup(\"Date\", \"Country\")\n.agg(sum(\"Quantity\"))\n.selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\n.orderBy(\"Date\")\nrolledUpDF.show(20)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["rolledUpDF = dfWithDate.rollup(\"Date\", \"Country\")\\\n.agg(sum(\"Quantity\"))\\\n.selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\\\n.orderBy(\"Date\")\nrolledUpDF.show(20)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["rolledUpDF.where(\"Country IS NULL\").show()\nrolledUpDF.where(\"Date IS NULL\").show()"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["%scala\ndfWithDate.cube(\"Date\", \"Country\")\n.agg(sum(col(\"Quantity\")))\n.select(\"Date\", \"Country\", \"sum(Quantity)\")\n.orderBy(\"Date\")\n.show(20)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["dfWithDate.cube(\"Date\", \"Country\")\\\n.agg(sum(col(\"Quantity\")))\\\n.select(\"Date\", \"Country\", \"sum(Quantity)\")\\\n.orderBy(\"Date\")\\\n.show(20)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["%scala\nval pivoted = dfWithDate\n.groupBy(\"date\")\n.pivot(\"Country\")\n.agg(\"quantity\" -> \"sum\")"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["pivoted = dfWithDate\\\n.groupBy(\"date\")\\\n.pivot(\"Country\")\\\n.agg({\"quantity\":\"sum\"})"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["pivoted.columns\npivoted.where(\"date > '2011-12-05'\").select(\"USA\").show()"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.expressions.MutableAggregationBuffer\nimport org.apache.spark.sql.expressions.UserDefinedAggregateFunction\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\nclass BoolAnd extends UserDefinedAggregateFunction {\ndef inputSchema: org.apache.spark.sql.types.StructType =\n  StructType(StructField(\"value\", BooleanType) :: Nil)\ndef bufferSchema: StructType = StructType(\n  StructField(\"result\", BooleanType) :: Nil\n)\ndef dataType: DataType = BooleanType\ndef deterministic: Boolean = true\ndef initialize(buffer: MutableAggregationBuffer): Unit = {\n  buffer(0) = true\n}\ndef update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n  buffer(0) = buffer.getAs[Boolean](0) && input.getAs[Boolean](0)\n}\ndef merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n  buffer1(0) = buffer1.getAs[Boolean](0) && buffer2.getAs[Boolean](0)\n}\ndef evaluate(buffer: Row): Any = {\n  buffer(0)\n}\n}\nval ba = new BoolAnd\nspark.udf.register(\"booland\", ba)\nimport org.apache.spark.sql.functions._\nspark.range(1)\n.selectExpr(\"explode(array(TRUE, TRUE, TRUE)) as t\")\n.selectExpr(\"explode(array(TRUE, FALSE, TRUE)) as f\", \"t\")\n.select(ba(col(\"t\")), expr(\"booland(f)\"))\n.show()"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["%scala\nval person = Seq(\n(0, \"Bill Chambers\", 0, Seq(100)),\n(1, \"Matei Zaharia\", 1, Seq(500, 250, 100)),\n(2, \"Michael Armbrust\", 1, Seq(250, 100)))\n.toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\nval graduateProgram = Seq(\n(0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n(2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n(1, \"Ph.D.\", \"EECS\", \"UC Berkeley\"))\n.toDF(\"id\", \"degree\", \"department\", \"school\")\nval sparkStatus = Seq(\n(500, \"Vice President\"),\n(250, \"PMC Member\"),\n(100, \"Contributor\"))\n.toDF(\"id\", \"status\")"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["person = spark.createDataFrame([\n(0, \"Bill Chambers\", 0, [100]),\n(1, \"Matei Zaharia\", 1, [500, 250, 100]),\n(2, \"Michael Armbrust\", 1, [250, 100])])\\\n.toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\ngraduateProgram = spark.createDataFrame([\n(0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n(2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n(1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n.toDF(\"id\", \"degree\", \"department\", \"school\")\nsparkStatus = spark.createDataFrame([\n(500, \"Vice President\"),\n(250, \"PMC Member\"),\n(100, \"Contributor\")])\\\n.toDF(\"id\", \"status\")\nperson.createOrReplaceTempView(\"person\")\ngraduateProgram.createOrReplaceTempView(\"graduateProgram\")\nsparkStatus.createOrReplaceTempView(\"sparkStatus\")"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["%scala\nval joinExpression = person.col(\"graduate_program\") === graduateProgram.col(\"id\")"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["joinExpression = person[\"graduate_program\"] == graduateProgram['id']"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["%scala\nval wrongJoinExpression = person.col(\"name\") === graduateProgram.col(\"school\")"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["wrongJoinExpression = person[\"name\"] == graduateProgram[\"school\"]\nperson.join(graduateProgram, joinExpression).show()"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM\nperson\nJOIN graduateProgram\nON person.graduate_program = graduateProgram.id"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["%scala\nvar joinType = \"inner\""],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["joinType = \"inner\"\nperson.join(graduateProgram, joinExpression, joinType).show()"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM\nperson\nINNER JOIN graduateProgram\nON person.graduate_program = graduateProgram.id"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["joinType = \"outer\"\nperson.join(graduateProgram, joinExpression, joinType).show()"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM\nperson\nOUTER JOIN graduateProgram\nON graduate_program = graduateProgram.id"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["joinType = \"left_outer\"\ngraduateProgram.join(person, joinExpression, joinType).show()"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM\ngraduateProgram\nJOIN person\nON person.graduate_program = graduateProgram.id"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["joinType = \"right_outer\"\nperson.join(graduateProgram, joinExpression, joinType).show()"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM\nperson\nRIGHT OUTER JOIN graduateProgram\nON person.graduate_program = graduateProgram.id"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["joinType = \"left_semi\"\ngraduateProgram.join(person, joinExpression, joinType).show()"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["%scala\nval gradProgram2 = graduateProgram\n.union(Seq(\n(0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")).toDF())\ngradProgram2.createOrReplaceTempView(\"gradProgram2\")"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["gradProgram2 = graduateProgram\\\n.union(spark.createDataFrame([\n(0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))\ngradProgram2.createOrReplaceTempView(\"gradProgram2\")\ngradProgram2.join(person, joinExpression, joinType).show()"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["%sql\nSELECT *\nFROM\ngradProgram2\nLEFT SEMI JOIN person\nON gradProgram2.id = person.graduate_program"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["joinType = \"left_anti\"\ngraduateProgram.join(person, joinExpression, joinType).show()"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM\ngraduateProgram\nLEFT ANTI JOIN person\nON graduateProgram.id = person.graduate_program"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["joinType = \"cross\"\ngraduateProgram.join(person, joinExpression, joinType).show()"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM\ngraduateProgram\nCROSS JOIN person\nON graduateProgram.id = person.graduate_program"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["person.crossJoin(graduateProgram).show()"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM\ngraduateProgram\nCROSS JOIN person"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.expr\nperson\n.withColumnRenamed(\"id\", \"personId\")\n.join(sparkStatus, expr(\"array_contains(spark_status, id)\"))\n.take(5)"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["from pyspark.sql.functions import expr\nperson\\\n.withColumnRenamed(\"id\", \"personId\")\\\n.join(sparkStatus, expr(\"array_contains(spark_status, id)\"))\\\n.take(5)"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM\n(select id as personId, name, graduate_program, spark_status FROM person)\nINNER JOIN sparkStatus\non array_contains(spark_status, id)\n"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["%scala\nval gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\nval joinExpr = gradProgramDupe.col(\"graduate_program\") === person.col(\"graduate_program\")\nperson.join(gradProgramDupe, joinExpr).show()\n"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["%scala\nperson\n.join(gradProgramDupe, joinExpr)\n.select(\"graduate_program\")\n.show()\nperson\n.join(gradProgramDupe, \"graduate_program\")\n.select(\"graduate_program\")\n.show()\n"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["%scala\nperson\n.join(gradProgramDupe, joinExpr)\n.drop(person.col(\"graduate_program\"))\n.select(\"graduate_program\")\n.show()\nval joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\nperson\n.join(graduateProgram, joinExpr)\n.drop(graduateProgram.col(\"id\"))\n.show()"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["%scala\nval gradProgram3 = graduateProgram\n.withColumnRenamed(\"id\", \"grad_id\")\nval joinExpr = person.col(\"graduate_program\") === gradProgram3.col(\"grad_id\")\nperson\n.join(gradProgram3, joinExpr)\n.show()"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["%scala\nval joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\nperson\n.join(graduateProgram, joinExpr)\n.explain()"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.broadcast\nval joinExpr = person.col(\"graduate_program\") === graduateProgram.col(\"id\")\nperson\n.join(broadcast(graduateProgram), joinExpr)\n.explain()"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":102}],"metadata":{"name":"Definitive Draft Chapter 5-6","notebookId":336718490210273},"nbformat":4,"nbformat_minor":0}
