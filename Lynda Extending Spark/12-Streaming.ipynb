{"cells":[{"cell_type":"markdown","source":["# Structured Streaming\n\n* Use DataFrame API to build Structured Streaming applications. \n* Compute real-time metrics like running counts and windowed counts on a stream of timestamped actions (e.g. Open, Close, etc).\n* Use cluster version \"Spark 2.0 (Scala 2.10)\"."],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## Sample Data\nWe have some sample action data as files in `/databricks-datasets/structured-streaming/events/` which we are going to use to build this application. There are about 50 JSON files in the directory. Let's see an example of what each JSON file contains."],"metadata":{}},{"cell_type":"code","source":["%fs head /databricks-datasets/structured-streaming/events/file-0.json"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Each line in the file contains JSON record with two fields - `time` and `action`. Let's try to analyze these files interactively."],"metadata":{}},{"cell_type":"markdown","source":["## Batch/Interactive Processing\nThe usual first step in attempting to process the data is to interactively query the data. Let's define a static DataFrame on the files, and give it a table name.  Since we know the data format already, we can define the schema upfront to speed up processing."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ninputPath = \"/databricks-datasets/structured-streaming/events/\"\n\njsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n\nstaticInputDF = (\n  spark\n    .read\n    .schema(jsonSchema)\n    .json(inputPath)\n)\n\ndisplay(staticInputDF)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Now we can compute the number of \"open\" and \"close\" actions with one hour windows. To do this, we will group by the `action` column and 1 hour windows over the `time` column.  We can register the results as a temp table."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import window\n\nstaticCountsDF = (\n  staticInputDF\n    .groupBy(\n       staticInputDF.action, \n       window(staticInputDF.time, \"1 hour\"))    \n    .count()\n)\nstaticCountsDF.cache()\n\nstaticCountsDF.createOrReplaceTempView(\"static_counts\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Now we can directly use SQL to query the table. For example, here are the total counts across all the hours."],"metadata":{}},{"cell_type":"code","source":["%sql \nSELECT action, SUM(count) AS total_count \nFROM static_counts \nGROUP BY action"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Or we can make a timeline of windowed counts.  Note the two ends of the graph. Some time passes between open actions and close actions, so there are more \"opens\" in the beginning and more \"closes\" in the end."],"metadata":{}},{"cell_type":"code","source":["%sql \nSELECT action, date_format(window.end, \"MMM-dd HH:mm\") AS time, count \nFROM static_counts \nORDER BY time, action"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Stream Processing \nWe can convert this to a streaming query that continuously updates as data comes. We can use our static set of files to emulate a real stream by reading them one at a time.  Note that we use `readStream` instead of `read` when constructing our query.\n\nWe use the `option` method to instruct Spark to read one file at a time, emulating a stream.  Once we have emulated the stream, the query is the same as the static query."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(jsonSchema)              \n    .option(\"maxFilesPerTrigger\", 1)  \n    .json(inputPath)\n)\n\nstreamingCountsDF = (                 \n  streamingInputDF\n    .groupBy(\n      streamingInputDF.action, \n      window(streamingInputDF.time, \"1 hour\"))\n    .count()\n)\n\nprint \"Is this DF actually a streaming DF?\", (\"Yes\" if streamingCountsDF.isStreaming else \"No\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["To start streaming computation define a sink and starting it."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\") \n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")       \n    .queryName(\"counts\")     \n    .outputMode(\"complete\") \n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["`query` is a handle to the streaming query that is running in the background. This query is continuously picking up files and updating the windowed counts. \n\nNote the status of query in the above cell. Both the `Status: ACTIVE` and the progress bar shows that the query is active. \nFurthermore, if you expand the `>Details` above, you will find the number of files they have already processed. \n\nWait a few seconds for a few files to be processed and then interactively query the in-memory `counts` table. Wait a few seconds more, then run the query again."],"metadata":{}},{"cell_type":"code","source":["%sql \nSELECT action, date_format(window.end, \"MMM-dd HH:mm\") AS time, count \nFROM counts \nORDER BY time, action"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["We can also see the total number of \"opens\" and \"closes\"."],"metadata":{}},{"cell_type":"code","source":["%sql \nSELECT action, SUM(count) AS total_count \nFROM counts \nGROUP BY action \nORDER BY action"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["If you don't see any updates as you re-run the queries, try restarting the streaming query.  There are only a few files and once they are all processed, there won't be any more updates in the stream.\n\nFinally, you can stop the query running in the background, either by clicking on the 'Cancel' link in the cell of the query, or by executing `query.stop()`."],"metadata":{}},{"cell_type":"code","source":["query.stop()"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"12-Streaming","notebookId":3737261653513287},"nbformat":4,"nbformat_minor":0}
