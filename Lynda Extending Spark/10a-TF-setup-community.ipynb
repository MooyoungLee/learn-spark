{"cells":[{"cell_type":"markdown","source":["# Install TensorFlow on a single node\n\nWe can install TensorFlow on a single node using the following script. \n\nThis should work fine on the community edition because the driver and the worker are on the same node. \n\nRun the script below."],"metadata":{}},{"cell_type":"code","source":["%sh\n\ntfBinaryUrl=\"https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp27-none-linux_x86_64.whl\"\n\nset -ex\n\necho \"**** Installing CPU-enabled TensorFlow *****\"\n\npip install ${tfBinaryUrl}"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["With python, it is normally only necessary to install a module before you can start using it.  However, when working in a notebook, python is already “running”.  So, python will not notice that the TensorFlow module is available unless you “restart” python.  \n\nThis is simpler than it sounds.  Simply use the cluster menu to `Detach` from your cluster.  Then use the menu again to `Attach` to the cluster.  The notebook will be executed again with a fresh instance of python, and you should be able to import TensorFlow as you normally would.\n\nTry importing `tensorflow` now as `tf`, and `print` `__version__`."],"metadata":{}},{"cell_type":"code","source":["import tensorflow as tf\nprint tf.__version__"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["Now you should have a working single-node Spark cluster.  This means that not only will python work, but Spark will work too."],"metadata":{}},{"cell_type":"markdown","source":["# Continue to Notebook 6\n\nIf your community cluster terminates you will need to run this notebook again to install tensorflow.  After running this notebook, you can do the exercises in Notebook 11-UseTF.py."],"metadata":{}}],"metadata":{"name":"10a-TF-setup-community","notebookId":3737261653513228},"nbformat":4,"nbformat_minor":0}
