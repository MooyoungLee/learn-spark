{"cells":[{"cell_type":"code","source":["%scala\nval staticDataFrame = spark.read.format(\"csv\")\n.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.load(\"/FileStore/tables/1x42p66f1497639007910/*.csv\")\nstaticDataFrame.createOrReplaceTempView(\"retail_data\")\nval staticSchema = staticDataFrame.schema"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["staticDataFrame = spark.read.format(\"csv\")\\\n.option(\"header\", \"true\")\\\n.option(\"inferSchema\", \"true\")\\\n.load(\"/FileStore/tables/1x42p66f1497639007910/*.csv\")\nstaticDataFrame.createOrReplaceTempView(\"retail_data\")\nstaticSchema = staticDataFrame.schema"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{window, column, desc, col}\nstaticDataFrame\n.selectExpr(\n\"CustomerId\",\n\"(UnitPrice * Quantity) as total_cost\",\n\"InvoiceDate\")\n.groupBy(\ncol(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\n.sum(\"total_cost\")\n.orderBy(desc(\"sum(total_cost)\"))\n.take(5)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.functions import window, column, desc, col\nstaticDataFrame\\\n.selectExpr(\n\"CustomerId\",\n\"(UnitPrice * Quantity) as total_cost\" ,\n\"InvoiceDate\" )\\\n.groupBy(\ncol(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n.sum(\"total_cost\")\\\n.orderBy(desc(\"sum(total_cost)\"))\\\n.take(5)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql\nSELECT\nsum(total_cost),\nCustomerId,\nto_date(InvoiceDate)\nFROM\n(SELECT\nCustomerId,\n(UnitPrice * Quantity) as total_cost,\nInvoiceDate\nFROM\nretail_data)\nGROUP BY\nCustomerId, to_date(InvoiceDate)\nORDER BY\nsum(total_cost) DESC"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%scala\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\nval streamingDataFrame = spark.readStream\n.schema(staticSchema)\n.option(\"maxFilesPerTrigger\", 1)\n.format(\"csv\")\n.option(\"header\", \"true\")\n.load(\"/FileStore/tables/1x42p66f1497639007910/*.csv\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["streamingDataFrame = spark.readStream\\\n.schema(staticSchema)\\\n.option(\"maxFilesPerTrigger\", 1)\\\n.format(\"csv\")\\\n.option(\"header\", \"true\")\\\n.load(\"dbfs:/mnt/defg/retail-data/by-day/*.csv\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%scala\nstreamingDataFrame.isStreaming // returns true"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%scala\nval purchaseByCustomerPerHour = streamingDataFrame\n.selectExpr(\n\"CustomerId\",\n\"(UnitPrice * Quantity) as total_cost\",\n\"InvoiceDate\")\n.groupBy(\n$\"CustomerId\", window($\"InvoiceDate\", \"1 day\"))\n.sum(\"total_cost\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["purchaseByCustomerPerHour = streamingDataFrame\\\n.selectExpr(\n\"CustomerId\",\n\"(UnitPrice * Quantity) as total_cost\" ,\n\"InvoiceDate\" )\\\n.groupBy(\ncol(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n.sum(\"total_cost\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%scala\npurchaseByCustomerPerHour.writeStream\n.format(\"memory\") // memory = store in-memory table\n.queryName(\"customer_purchases\") // counts = name of the in-memory table\n.outputMode(\"complete\") // complete = all the counts should be in the table\n.start()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["purchaseByCustomerPerHour.writeStream\\\n.format(\"memory\")\\\n.queryName(\"customer_purchases\")\\\n.outputMode(\"complete\")\\\n.start()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%scala\nspark.sql(\"\"\"\nSELECT *\nFROM customer_purchases\nORDER BY `sum(total_cost)` DESC\n\"\"\")\n.take(5)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["spark.sql(\"\"\"\nSELECT *\nFROM customer_purchases\nORDER BY `sum(total_cost)` DESC\n\"\"\")\\\n.take(5)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["staticDataFrame.printSchema()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.date_format\nval preppedDataFrame = staticDataFrame\n.na.fill(0)\n.withColumn(\"day_of_week\", date_format($\"InvoiceDate\", \"EEEE\"))\n.coalesce(5)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql.functions import date_format, col\npreppedDataFrame = staticDataFrame\\\n.na.fill(0)\\\n.withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n.coalesce(5)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%scala\nval trainDataFrame = preppedDataFrame\n.where(\"InvoiceDate < '2011-07-01'\")\nval testDataFrame = preppedDataFrame\n.where(\"InvoiceDate >= '2011-07-01'\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["trainDataFrame = preppedDataFrame\\\n.where(\"InvoiceDate < '2011-07-01'\")\ntestDataFrame = preppedDataFrame\\\n.where(\"InvoiceDate >= '2011-07-01'\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["trainDataFrame.count()\ntestDataFrame.count()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.StringIndexer\nval indexer = new StringIndexer()\n.setInputCol(\"day_of_week\")\n.setOutputCol(\"day_of_week_index\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\nindexer = StringIndexer()\\\n.setInputCol(\"day_of_week\")\\\n.setOutputCol(\"day_of_week_index\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.OneHotEncoder\nval encoder = new OneHotEncoder()\n.setInputCol(\"day_of_week_index\")\n.setOutputCol(\"day_of_week_encoded\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\nencoder = OneHotEncoder()\\\n.setInputCol(\"day_of_week_index\")\\\n.setOutputCol(\"day_of_week_encoded\")"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.VectorAssembler\nval vectorAssembler = new VectorAssembler()\n.setInputCols(Array(\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"))\n.setOutputCol(\"features\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nvectorAssembler = VectorAssembler()\\\n.setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n.setOutputCol(\"features\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.Pipeline\nval transformationPipeline = new Pipeline()\n.setStages(Array(indexer, encoder, vectorAssembler))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from pyspark.ml import Pipeline\ntransformationPipeline = Pipeline()\\\n.setStages([indexer, encoder, vectorAssembler])"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%scala\nval fittedPipeline = transformationPipeline.fit(trainDataFrame)\n"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["fittedPipeline = transformationPipeline.fit(trainDataFrame)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["%scala\nval transformedTraining = fittedPipeline.transform(trainDataFrame)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["transformedTraining = fittedPipeline.transform(trainDataFrame)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["transformedTraining.cache()"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.clustering.KMeans\nval kmeans = new KMeans()\n.setK(20)\n.setSeed(1L)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\nkmeans = KMeans()\\\n.setK(20)\\\n.setSeed(1L)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%scala\nval kmModel = kmeans.fit(transformedTraining)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["kmModel = kmeans.fit(transformedTraining)\nkmModel.computeCost(transformedTraining)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%scala\nval transformedTest = fittedPipeline.transform(testDataFrame)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["transformedTest = fittedPipeline.transform(testDataFrame)\nkmModel.computeCost(transformedTest)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["%scala\nval bikeStations = spark.read.format(\"csv\")\n.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.load(\"/FileStore/tables/ltn1r4x11497653150836/201508_station_data.csv\")\nval bikeTrips = spark.read.format(\"csv\")\n.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.load(\"/FileStore/tables/ltn1r4x11497653150836/201508_trip_data.csv\")"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["bikeStations = spark.read.format(\"csv\")\\\n.option(\"header\", \"true\")\\\n.option(\"inferSchema\", \"true\")\\\n.load(\"/FileStore/tables/ltn1r4x11497653150836/201508_station_data.csv\")\nbikeTrips = spark.read.format(\"csv\")\\\n.option(\"header\", \"true\")\\\n.option(\"inferSchema\", \"true\")\\\n.load(\"/FileStore/tables/ltn1r4x11497653150836/201508_trip_data.csv\")"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["%scala\nval stationVertices = bikeStations\n.withColumnRenamed(\"name\", \"id\")\n.distinct()\nval tripEdges = bikeTrips\n.withColumnRenamed(\"Start Station\", \"src\")\n.withColumnRenamed(\"End Station\", \"dst\")"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["stationVertices = bikeStations\\\n.withColumnRenamed(\"name\", \"id\")\\\n.distinct()\ntripEdges = bikeTrips\\\n.withColumnRenamed(\"Start Station\", \"src\")\\\n.withColumnRenamed(\"End Station\", \"dst\")"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["%scala\nimport org.graphframes.GraphFrame\nval stationGraph = GraphFrame(stationVertices, tripEdges)\ntripEdges.cache()\nstationVertices.cache()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["from graphframes import GraphFrame\nstationGraph = GraphFrame(stationVertices, tripEdges)\ntripEdges.cache()\nstationVertices.cache()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{desc, col}\nval ranks = stationGraph.pageRank.resetProbability(0.15).maxIter(10).run()\nranks.vertices.orderBy(desc(\"pagerank\")).take(5)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["from pyspark.sql.functions import desc\nranks = stationGraph.pageRank(maxIter=10).resetProbability(0.15).run()\nranks.vertices.orderBy(desc(\"pagerank\")).take(5)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["%scala\nstationGraph\n.edges\n.groupBy(\"src\", \"dst\")\n.count()\n.orderBy(desc(\"count\"))\n.limit(10)\n.show()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["stationGraph\\\n.edges\\\n.groupBy(\"src\", \"dst\")\\\n.count()\\\n.orderBy(desc(\"count\"))\\\n.limit(10)\\\n.show()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["%scala\nval df = spark.range(500).toDF(\"number\")\ndf.select(df.col(\"number\") + 10)\n// org.apache.spark.sql.DataFrame = [(number + 10): bigint]"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["df = spark.range(500).toDF(\"number\")\ndf.select(df[\"number\"] + 10)\n# DataFrame[(number + 10): bigint]"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["%scala\nspark.range(2).toDF().collect()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["spark.range(2).collect()"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.types._\nval b = ByteType()"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["import org.apache.spark.sql.types.DataTypes;\nByteType x = DataTypes.ByteType();"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["from pyspark.sql.types import *\nb = byteType()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["%scala\nval df = spark.read.format(\"json\")\n.load(\"/FileStore/tables/8su3wraj1497632771405/2015_summary-ebaee.json\")"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["df = spark.read.format(\"json\")\\\n.load(\"/FileStore/tables/8su3wraj1497632771405/2015_summary-ebaee.json\")"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["%scala\nspark.read.format(\"json\")\n.load(\"/FileStore/tables/8su3wraj1497632771405/2015_summary-ebaee.json\")\n.schema"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["spark.read.format(\"json\")\\\n.load(\"/FileStore/tables/8su3wraj1497632771405/2015_summary-ebaee.json\")\\\n.schema"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\nval myManualSchema = new StructType(Array(\nnew StructField(\"DEST_COUNTRY_NAME\", StringType, true),\nnew StructField(\"ORIGIN_COUNTRY_NAME\", StringType, true),\nnew StructField(\"count\", LongType, false) // just to illustrate flipping\n\n))\nval df = spark.read.format(\"json\")\n.schema(myManualSchema)\n.load(\"/FileStore/tables/8su3wraj1497632771405/2015_summary-ebaee.json\")"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StructType, StringType, LongType\nmyManualSchema = StructType([\nStructField(\"DEST_COUNTRY_NAME\", StringType(), True),\nStructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\nStructField(\"count\", LongType(), False)\n])\ndf = spark.read.format(\"json\")\\\n.schema(myManualSchema)\\\n.load(\"/FileStore/tables/8su3wraj1497632771405/2015_summary-ebaee.json\")"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{col, column}\ncol(\"someColumnName\")\ncolumn(\"someColumnName\")"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["from pyspark.sql.functions import col, column\ncol(\"someColumnName\")\ncolumn(\"someColumnName\")"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["%scala\n$\"myColumn\"\n'myColumn"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["%scala\ndf.col(\"count\")"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{expr, col}\n(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["from pyspark.sql.functions import expr\nexpr(\"(((someCol + 5) * 200) - 6) < otherCol\")"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["%scala\nspark.read.format(\"json\")\n.load(\"/FileStore/tables/8su3wraj1497632771405/2015_summary-ebaee.json\")\n.columns"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["%scala\ndf.first()"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["df.first()"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.Row\nval myRow = Row(\"Hello\", null, 1, false)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["from pyspark.sql import Row\nmyRow = Row(\"Hello\", None, 1, False)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["%scala\nmyRow(0) // type Any\nmyRow(0).asInstanceOf[String] // String\nmyRow.getString(0) // String\nmyRow.getInt(2) // String"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["myRow[0]\nmyRow[2]"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["%scala\nval df = spark.read.format(\"json\")\n.load(\"/FileStore/tables/8su3wraj1497632771405/2015_summary-ebaee.json\")\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["df = spark.read.format(\"json\")\\\n.load(\"/FileStore/tables/8su3wraj1497632771405/2015_summary-ebaee.json\")\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types.{StructField, StructType,\nStringType, LongType}\nval myManualSchema = new StructType(Array(\nnew StructField(\"some\", StringType, true),\nnew StructField(\"col\", StringType, true),\nnew StructField(\"names\", LongType, false) // just to illustrate flipping\n))\nval myRows = Seq(Row(\"Hello\", null, 1L))\nval myRDD = spark.sparkContext.parallelize(myRows)\nval myDf = spark.createDataFrame(myRDD, myManualSchema)\nmyDf.show()"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["%scala\nval myDF = Seq((\"Hello\", 2, 1L)).toDF()"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["from pyspark.sql import Row\nfrom pyspark.sql.types import StructField, StructType,\\\nStringType, LongType\nmyManualSchema = StructType([\nStructField(\"some\", StringType(), True),\nStructField(\"col\", StringType(), True),\nStructField(\"names\", LongType(), False)\n])\nmyRow = Row(\"Hello\", None, 1)\nmyDf = spark.createDataFrame([myRow], myManualSchema)\nmyDf.show()\n"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["%scala\ndf.select(\"DEST_COUNTRY_NAME\").show(2)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["df.select(\"DEST_COUNTRY_NAME\").show(2)"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["%sql\nSELECT DEST_COUNTRY_NAME\nFROM dfTable\nLIMIT 2"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["%scala\ndf.select(\n\"DEST_COUNTRY_NAME\",\n\"ORIGIN_COUNTRY_NAME\")\n.show(2)"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["df.select(\n\"DEST_COUNTRY_NAME\",\n\"ORIGIN_COUNTRY_NAME\" )\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["%sql\nSELECT\nDEST_COUNTRY_NAME,\nORIGIN_COUNTRY_NAME\nFROM\ndfTable\nLIMIT 2"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{expr, col, column}\ndf.select(\ndf.col(\"DEST_COUNTRY_NAME\"),\ncol(\"DEST_COUNTRY_NAME\"),\ncolumn(\"DEST_COUNTRY_NAME\"),\n'DEST_COUNTRY_NAME,\n$\"DEST_COUNTRY_NAME\",\nexpr(\"DEST_COUNTRY_NAME\")\n).show(2)"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["from pyspark.sql.functions import expr, col, column\ndf.select(\nexpr(\"DEST_COUNTRY_NAME\"),\ncol(\"DEST_COUNTRY_NAME\"),\ncolumn(\"DEST_COUNTRY_NAME\"))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\")"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["%scala\ndf.select(expr(\"DEST_COUNTRY_NAME AS destination\"))"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["df.select(expr(\"DEST_COUNTRY_NAME AS destination\"))"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["%sql\nSELECT\nDEST_COUNTRY_NAME as destination\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["%scala\ndf.select(\nexpr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")\n)"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["df.select(\nexpr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")\n)"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["%scala\ndf.selectExpr(\n\"DEST_COUNTRY_NAME as newColumnName\",\n\"DEST_COUNTRY_NAME\"\n).show(2)"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["df.selectExpr(\n\"DEST_COUNTRY_NAME as newColumnName\",\n\"DEST_COUNTRY_NAME\"\n).show(2)"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["%scala\ndf.selectExpr(\n\"*\", // all original columns\n\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\"\n).show(2)"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["df.selectExpr(\n\"*\", # all original columns\n\"(DEST_C\n  OUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\" )\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["%sql\nSELECT\n*,\n(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["%scala\ndf.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["%sql\nSELECT\navg(count),\ncount(distinct(DEST_COUNTRY_NAME))\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.lit\ndf.select(\nexpr(\"*\"),\nlit(1).as(\"something\")\n).show(2)"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["from pyspark.sql.functions import lit\ndf.select(\nexpr(\"*\"),\nlit(1).alias(\"One\")\n).show(2)"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["%sql\nSELECT\n*,\n1 as One\nFROM\ndfTable\nLIMIT 2"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["%scala\ndf.withColumn(\"numberOne\", lit(1)).show(2)"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["df.withColumn(\"numberOne\", lit(1)).show(2)"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["%sql\nSELECT\n1 as numberOne\nFROM\ndfTable\nLIMIT 2"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["%scala\ndf.withColumn(\n\"withinCountry\",\nexpr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")\n).show(2)"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["df.withColumn(\n\"withinCountry\",\nexpr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["%scala\ndf.withColumn(\n\"Destination\",\ndf.col(\"DEST_COUNTRY_NAME\"))\n.columns"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["%scala\ndf.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns|"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.expr\nval dfWithLongColName = df\n.withColumn(\n\"This Long Column-Name\",\nexpr(\"ORIGIN_COUNTRY_NAME\"))"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["dfWithLongColName = df\\\n.withColumn(\n\"This Long Column-Name\",\nexpr(\"ORIGIN_COUNTRY_NAME\"))"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":["%scala\ndfWithLongColName\n.selectExpr(\n\"`This Long Column-Name`\",\n\"`This Long Column-Name` as `new col`\")\n.show(2)"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["dfWithLongColName\\\n.selectExpr(\n\"`This Long Column-Name`\",\n\"`This Long Column-Name` as `new col`\" )\\\n.show(2)\ndfWithLongColName.createOrReplaceTempView(\"dfTableLong\")"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":["%sql\nSELECT `This Long Column-Name` FROM dfTableLong"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"code","source":["%scala\ndfWithLongColName.select(col(\"This Long Column-Name\")).columns"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"code","source":["dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns"],"metadata":{},"outputs":[],"execution_count":121},{"cell_type":"code","source":["df.drop(\"ORIGIN_COUNTRY_NAME\").columns"],"metadata":{},"outputs":[],"execution_count":122},{"cell_type":"code","source":["dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\")"],"metadata":{},"outputs":[],"execution_count":123},{"cell_type":"code","source":["df.printSchema()\ndf.withColumn(\"count\", col(\"count\").cast(\"int\")).printSchema()"],"metadata":{},"outputs":[],"execution_count":124},{"cell_type":"code","source":["%sql\nSELECT\ncast(count as int)\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":125},{"cell_type":"code","source":["%scala\nval colCondition = df.filter(col(\"count\") < 2).take(2)\nval conditional = df.where(\"count < 2\").take(2)"],"metadata":{},"outputs":[],"execution_count":126},{"cell_type":"code","source":["colCondition = df.filter(col(\"count\") < 2).take(2)\nconditional = df.where(\"count < 2\").take(2)"],"metadata":{},"outputs":[],"execution_count":127},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM dfTable\nWHERE\ncount < 2"],"metadata":{},"outputs":[],"execution_count":128},{"cell_type":"code","source":["%scala\ndf.where(col(\"count\") < 2)\n.where(col(\"ORIGIN_COUNTRY_NAME\") =!= \"Croatia\")\n.show(2)"],"metadata":{},"outputs":[],"execution_count":129},{"cell_type":"code","source":["df.where(col(\"count\") < 2)\\\n.where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":130},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM dfTable\nWHERE\ncount < 2 AND\nORIGIN_COUNTRY_NAME != \"Croatia\""],"metadata":{},"outputs":[],"execution_count":131},{"cell_type":"code","source":["%scala\ndf.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").count()"],"metadata":{},"outputs":[],"execution_count":132},{"cell_type":"code","source":["df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").count()"],"metadata":{},"outputs":[],"execution_count":133},{"cell_type":"code","source":["%sql\nSELECT\nCOUNT(DISTINCT ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)\nFROM dfTable"],"metadata":{},"outputs":[],"execution_count":134},{"cell_type":"code","source":["%scala\ndf.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"],"metadata":{},"outputs":[],"execution_count":135},{"cell_type":"code","source":["df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"],"metadata":{},"outputs":[],"execution_count":136},{"cell_type":"code","source":["%sql\nSELECT\nCOUNT(DISTINCT ORIGIN_COUNTRY_NAME)\nFROM dfTable"],"metadata":{},"outputs":[],"execution_count":137},{"cell_type":"code","source":["%scala\nval seed = 5\nval withReplacement = false\nval fraction = 0.5\ndf.sample(withReplacement, fraction, seed).count()"],"metadata":{},"outputs":[],"execution_count":138},{"cell_type":"code","source":["seed = 5\nwithReplacement = False\nfraction = 0.5\ndf.sample(withReplacement, fraction, seed).count()"],"metadata":{},"outputs":[],"execution_count":139},{"cell_type":"code","source":["%scala\nval dataFrames = df.randomSplit(Array(0.25, 0.75), seed)\ndataFrames(0).count() > dataFrames(1).count()"],"metadata":{},"outputs":[],"execution_count":140},{"cell_type":"code","source":["dataFrames = df.randomSplit([0.25, 0.75], seed)\ndataFrames[0].count() > dataFrames[1].count()"],"metadata":{},"outputs":[],"execution_count":141},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.Row\nval schema = df.schema\nval newRows = Seq(\nRow(\"New Country\", \"Other Country\", 5L),\nRow(\"New Country 2\", \"Other Country 3\", 1L)\n)\nval parallelizedRows = spark.sparkContext.parallelize(newRows)\nval newDF = spark.createDataFrame(parallelizedRows, schema)\ndf.union(newDF)\n.where(\"count = 1\")\n.where($\"ORIGIN_COUNTRY_NAME\" =!= \"United States\")\n.show() // get all of them and we'll see our new rows at the end"],"metadata":{},"outputs":[],"execution_count":142},{"cell_type":"code","source":["from pyspark.sql import Row\nschema = df.schema\nnewRows = [\nRow(\"New Country\", \"Other Country\", 5L),\nRow(\"New Country 2\", \"Other Country 3\", 1L)\n]\nparallelizedRows = spark.sparkContext.parallelize(newRows)\nnewDF = spark.createDataFrame(parallelizedRows, schema)"],"metadata":{},"outputs":[],"execution_count":143},{"cell_type":"code","source":["\ndf.union(newDF)\\\n.where(\"count = 1\")\\\n.where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n.show()"],"metadata":{},"outputs":[],"execution_count":144},{"cell_type":"code","source":["%scala\ndf.sort(\"count\").show(5)\ndf.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\ndf.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"],"metadata":{},"outputs":[],"execution_count":145},{"cell_type":"code","source":["df.sort(\"count\").show(5)\ndf.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\ndf.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"],"metadata":{},"outputs":[],"execution_count":146},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{desc, asc}\ndf.orderBy(expr(\"count desc\")).show(2)\ndf.orderBy(desc(\"count\"), asc(\"DEST_COUNTRY_NAME\")).show(2)"],"metadata":{},"outputs":[],"execution_count":147},{"cell_type":"code","source":["from pyspark.sql.functions import desc, asc\ndf.orderBy(expr(\"count desc\")).show(2)\ndf.orderBy(desc(col(\"count\")), asc(col(\"DEST_COUNTRY_NAME\"))).show(2)"],"metadata":{},"outputs":[],"execution_count":148},{"cell_type":"code","source":["%sql\nSELECT *\nFROM dfTable\nORDER BY count DESC, DEST_COUNTRY_NAME ASC"],"metadata":{},"outputs":[],"execution_count":149},{"cell_type":"code","source":["%scala\nspark.read.format(\"json\")\n.load(\"/FileStore/tables/nrptmpui1497642469139/2010_summary-506d8.json\")\n.sortWithinPartitions(\"count\")"],"metadata":{},"outputs":[],"execution_count":150},{"cell_type":"code","source":["spark.read.format(\"json\")\\\n.load(\"/FileStore/tables/nrptmpui1497642469139/2010_summary-506d8.json\")\\\n.sortWithinPartitions(\"count\")"],"metadata":{},"outputs":[],"execution_count":151},{"cell_type":"code","source":["%scala\ndf.limit(5).show()"],"metadata":{},"outputs":[],"execution_count":152},{"cell_type":"code","source":["df.limit(5).show()"],"metadata":{},"outputs":[],"execution_count":153},{"cell_type":"code","source":["%scala\ndf.orderBy(expr(\"count desc\")).limit(6).show()"],"metadata":{},"outputs":[],"execution_count":154},{"cell_type":"code","source":["df.orderBy(expr(\"count desc\")).limit(6).show()"],"metadata":{},"outputs":[],"execution_count":155},{"cell_type":"code","source":["%sql\nSELECT *\nFROM dfTable\nLIMIT 6"],"metadata":{},"outputs":[],"execution_count":156},{"cell_type":"code","source":["%scala\ndf.rdd.getNumPartitions"],"metadata":{},"outputs":[],"execution_count":157},{"cell_type":"code","source":["df.rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":158},{"cell_type":"code","source":["%scala\ndf.repartition(5)"],"metadata":{},"outputs":[],"execution_count":159},{"cell_type":"code","source":["df.repartition(5)"],"metadata":{},"outputs":[],"execution_count":160},{"cell_type":"code","source":["%scala\ndf.repartition(col(\"DEST_COUNTRY_NAME\"))"],"metadata":{},"outputs":[],"execution_count":161},{"cell_type":"code","source":["df.repartition(col(\"DEST_COUNTRY_NAME\"))"],"metadata":{},"outputs":[],"execution_count":162},{"cell_type":"code","source":["%scala\ndf.repartition(5, col(\"DEST_COUNTRY_NAME\"))"],"metadata":{},"outputs":[],"execution_count":163},{"cell_type":"code","source":["df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"],"metadata":{},"outputs":[],"execution_count":164},{"cell_type":"code","source":["%scala\ndf.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"],"metadata":{},"outputs":[],"execution_count":165},{"cell_type":"code","source":["\ndf.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"],"metadata":{},"outputs":[],"execution_count":166},{"cell_type":"code","source":["%scala\nval collectDF = df.limit(10)\ncollectDF.take(5) // take works with an Integer count\ncollectDF.show() // this prints it out nicely\ncollectDF.show(5, false)\ncollectDF.collect()"],"metadata":{},"outputs":[],"execution_count":167},{"cell_type":"code","source":["collectDF = df.limit(10)\ncollectDF.take(5) # take works with an Integer count\ncollectDF.show() # this prints it out nicely\ncollectDF.show(5, False)\ncollectDF.collect()"],"metadata":{},"outputs":[],"execution_count":168},{"cell_type":"code","source":["%scala\nval df = spark.read.format(\"csv\")\n.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.load(\"/FileStore/tables/tmdv7u711497636580369/2010_12_01-ec65d.csv\")\ndf.printSchema()\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{},"outputs":[],"execution_count":169},{"cell_type":"code","source":["df = spark.read.format(\"csv\")\\\n.option(\"header\", \"true\")\\\n.option(\"inferSchema\", \"true\")\\\n.load(\"/FileStore/tables/tmdv7u711497636580369/2010_12_01-ec65d.csv\")\ndf.printSchema()\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{},"outputs":[],"execution_count":170},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.col\ndf.where(col(\"InvoiceNo\").equalTo(536365))\n.select(\"InvoiceNo\", \"Description\")\n.show(5, false)"],"metadata":{},"outputs":[],"execution_count":171},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.col\ndf.where(col(\"InvoiceNo\") === 536365)\n.select(\"InvoiceNo\", \"Description\")\n.show(5, false)"],"metadata":{},"outputs":[],"execution_count":172},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndf.where(col(\"InvoiceNo\") != 536365)\\\n.select(\"InvoiceNo\", \"Description\")\\\n.show(5, False)"],"metadata":{},"outputs":[],"execution_count":173},{"cell_type":"code","source":["%scala\nval priceFilter = col(\"UnitPrice\") > 600\nval descripFilter = col(\"Description\").contains(\"POSTAGE\")\ndf.where(col(\"StockCode\").isin(\"DOT\"))\n.where(priceFilter.or(descripFilter))\n.show(5)"],"metadata":{},"outputs":[],"execution_count":174},{"cell_type":"code","source":["from pyspark.sql.functions import instr\npriceFilter = col(\"UnitPrice\") > 600\ndescripFilter = instr(df.Description, \"POSTAGE\") >= 1\ndf.where(df.StockCode.isin(\"DOT\"))\\\n.where(priceFilter | descripFilter)\\\n.show(5)"],"metadata":{},"outputs":[],"execution_count":175},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM dfTable\nWHERE\nStockCode in (\"DOT\") AND\n(UnitPrice > 600 OR\ninstr(Description, \"POSTAGE\") >= 1)"],"metadata":{},"outputs":[],"execution_count":176},{"cell_type":"code","source":["%scala\nval DOTCodeFilter = col(\"StockCode\") === \"DOT\"\nval priceFilter = col(\"UnitPrice\") > 600\nval descripFilter = col(\"Description\").contains(\"POSTAGE\")\ndf.withColumn(\"isExpensive\",\nDOTCodeFilter.and(priceFilter.or(descripFilter)))\n.where(\"isExpensive\")\n.select(\"unitPrice\", \"isExpensive\")\n.show(5)"],"metadata":{},"outputs":[],"execution_count":177},{"cell_type":"code","source":["from pyspark.sql.functions import instr\nDOTCodeFilter = col(\"StockCode\") == \"DOT\"\npriceFilter = col(\"UnitPrice\") > 600\ndescripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\ndf.withColumn(\"isExpensive\",\nDOTCodeFilter & (priceFilter | descripFilter))\\\n.where(\"isExpensive\")\\\n.select(\"unitPrice\", \"isExpensive\")\\\n.show(5)"],"metadata":{},"outputs":[],"execution_count":178},{"cell_type":"code","source":["%sql\nSELECT\nUnitPrice,\n(StockCode = 'DOT' AND\n(UnitPrice > 600 OR\ninstr(Description, \"POSTAGE\") >= 1)) as isExpensive\nFROM dfTable\nWHERE\n(StockCode = 'DOT' AND\n(UnitPrice > 600 OR\ninstr(Description, \"POSTAGE\") >= 1))"],"metadata":{},"outputs":[],"execution_count":179},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{expr, not, col}\ndf.withColumn(\"isExpensive\", not(col(\"UnitPrice\").leq(250)))\n.filter(\"isExpensive\")\n.select(\"Description\", \"UnitPrice\").show(5)\ndf.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\n.filter(\"isExpensive\")\n.select(\"Description\", \"UnitPrice\").show(5)"],"metadata":{},"outputs":[],"execution_count":180},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{expr, not, col}\ndf.withColumn(\"isExpensive\", not(col(\"UnitPrice\").leq(250)))\n.filter(\"isExpensive\")\n.select(\"Description\", \"UnitPrice\").show(5)\ndf.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\"))\n.filter(\"isExpensive\")\n.select(\"Description\", \"UnitPrice\").show(5)"],"metadata":{},"outputs":[],"execution_count":181},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{expr, pow}\nval fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\ndf.select(\nexpr(\"CustomerId\"),\nfabricatedQuantity.alias(\"realQuantity\"))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":182},{"cell_type":"code","source":["from pyspark.sql.functions import expr, pow\nfabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\ndf.select(\nexpr(\"CustomerId\"),\nfabricatedQuantity.alias(\"realQuantity\"))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":183},{"cell_type":"code","source":["%scala\ndf.selectExpr(\n\"CustomerId\",\n\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\")\n.show(2)"],"metadata":{},"outputs":[],"execution_count":184},{"cell_type":"code","source":["df.selectExpr(\n\"CustomerId\",\n\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\" )\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":185},{"cell_type":"code","source":["%sql\nSELECT\ncustomerId,\n(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\nFROM dfTable"],"metadata":{},"outputs":[],"execution_count":186},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{round, bround}\ndf.select(\nround(col(\"UnitPrice\"), 1).alias(\"rounded\"),\ncol(\"UnitPrice\"))\n.show(5)"],"metadata":{},"outputs":[],"execution_count":187},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.lit\ndf.select(\nround(lit(\"2.5\")),\nbround(lit(\"2.5\")))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":188},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.lit\ndf.select(\nround(lit(\"2.5\")),\nbround(lit(\"2.5\")))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":189},{"cell_type":"code","source":["%sql\nSELECT\nround(2.5),\nbround(2.5)"],"metadata":{},"outputs":[],"execution_count":190},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{corr}\ndf.stat.corr(\"Quantity\", \"UnitPrice\")\ndf.select(corr(\"Quantity\", \"UnitPrice\")).show()"],"metadata":{},"outputs":[],"execution_count":191},{"cell_type":"code","source":["from pyspark.sql.functions import corr\ndf.stat.corr(\"Quantity\", \"UnitPrice\")\ndf.select(corr(\"Quantity\", \"UnitPrice\")).show()"],"metadata":{},"outputs":[],"execution_count":192},{"cell_type":"code","source":["%sql\nSELECT\ncorr(Quantity, UnitPrice)\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":193},{"cell_type":"code","source":["%scala\ndf.describe().show()"],"metadata":{},"outputs":[],"execution_count":194},{"cell_type":"code","source":["df.describe().show()"],"metadata":{},"outputs":[],"execution_count":195},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{count, mean, stddev_pop, min, max}"],"metadata":{},"outputs":[],"execution_count":196},{"cell_type":"code","source":["from pyspark.sql.functions import count, mean, stddev_pop, min, max"],"metadata":{},"outputs":[],"execution_count":197},{"cell_type":"code","source":["%scala\nval colName = \"UnitPrice\"\nval quantileProbs = Array(0.5)\nval relError = 0.05\ndf.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)"],"metadata":{},"outputs":[],"execution_count":198},{"cell_type":"code","source":["colName = \"UnitPrice\"\nquantileProbs = [0.5]\nrelError = 0.05\ndf.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)"],"metadata":{},"outputs":[],"execution_count":199},{"cell_type":"code","source":["%scala\ndf.stat.crosstab(\"StockCode\", \"Quantity\").show()"],"metadata":{},"outputs":[],"execution_count":200},{"cell_type":"code","source":["df.stat.crosstab(\"StockCode\", \"Quantity\").show()"],"metadata":{},"outputs":[],"execution_count":201},{"cell_type":"code","source":["%scala\ndf.stat.freqItems(Seq(\"StockCode\", \"Quantity\")).show()"],"metadata":{},"outputs":[],"execution_count":202},{"cell_type":"code","source":["df.stat.freqItems([\"StockCode\", \"Quantity\"]).show()"],"metadata":{},"outputs":[],"execution_count":203},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{initcap}\ndf.select(initcap(col(\"Description\"))).show(2, false)"],"metadata":{},"outputs":[],"execution_count":204},{"cell_type":"code","source":["from pyspark.sql.functions import initcap\ndf.select(initcap(col(\"Description\"))).show()"],"metadata":{},"outputs":[],"execution_count":205},{"cell_type":"code","source":["%sql\nSELECT\ninitcap(Description)\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":206},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{lower, upper}\ndf.select(\ncol(\"Description\"),\nlower(col(\"Description\")),\nupper(lower(col(\"Description\"))))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":207},{"cell_type":"code","source":["from pyspark.sql.functions import lower, upper\ndf.select(\ncol(\"Description\"),\nlower(col(\"Description\")),\nupper(lower(col(\"Description\"))))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":208},{"cell_type":"code","source":["%sql\nSELECT\nDescription,\nlower(Description),\nUpper(lower(Description))\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":209},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}\ndf.select(\nltrim(lit(\" HELLO \")).as(\"ltrim\"),\nrtrim(lit(\" HELLO \")).as(\"rtrim\"),\ntrim(lit(\" HELLO \")).as(\"trim\"),\nlpad(lit(\"HELLO\"), 3, \" \").as(\"lp\"),\nrpad(lit(\"HELLO\"), 10, \" \").as(\"rp\"))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":210},{"cell_type":"code","source":["from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\ndf.select(\nltrim(lit(\" HELLO \")).alias(\"ltrim\"),\nrtrim(lit(\" HELLO \")).alias(\"rtrim\"),\ntrim(lit(\" HELLO \")).alias(\"trim\"),\nlpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\nrpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\"))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":211},{"cell_type":"code","source":["%sql\nSELECT\nltrim(' HELLLOOOO '),\nrtrim(' HELLLOOOO '),\ntrim(' HELLLOOOO '),\nlpad('HELLOOOO ', 3, ' '),\nrpad('HELLOOOO ', 10, ' ')\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":212},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.regexp_replace\nval simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\nval regexString = simpleColors.map(_.toUpperCase).mkString(\" \")\n// the   signifies `OR` in regular expression syntax\ndf.select(\nregexp_replace(col(\"Description\"), regexString, \"COLOR\")\n.alias(\"color_cleaned\"),\ncol(\"Description\"))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":213},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_replace\nregex_string = \"BLACK WHITE RED GREEN BLUE\"\ndf.select(\nregexp_replace(col(\"Description\"), regex_string, \"COLOR\")\n.alias(\"color_cleaned\"),\ncol(\"Description\"))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":214},{"cell_type":"code","source":["%sql\nSELECT\nregexp_replace(Description, 'BLACK WHITE RED GREEN BLUE', 'COLOR') as\ncolor_cleaned,\nDescription\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":215},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.translate\ndf.select(\ntranslate(col(\"Description\"), \"LEET\", \"1337\"),\ncol(\"Description\"))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":216},{"cell_type":"code","source":["from pyspark.sql.functions import translate\ndf.select(\ntranslate(col(\"Description\"), \"LEET\", \"1337\"),\ncol(\"Description\"))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":217},{"cell_type":"code","source":["%sql\nSELECT\ntranslate(Description, ‘LEET’, ‘1337’),\nDescription\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":218},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.regexp_extract\nval regexString = simpleColors\n.map(_.toUpperCase)\n.mkString(\"(\", \" \", \")\")\n// the   signifies OR in regular expression syntax\ndf.select(\nregexp_extract(col(\"Description\"), regexString, 1)\n.alias(\"color_cleaned\"),\ncol(\"Description\"))\n.show(2)"],"metadata":{},"outputs":[],"execution_count":219},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_extract\nextract_str = \"(BLACK WHITE RED GREEN BLUE)\"\ndf.select(\nregexp_extract(col(\"Description\"), extract_str, 1)\n.alias(\"color_cleaned\"),\ncol(\"Description\"))\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":220},{"cell_type":"code","source":["%sql\nSELECT\nregexp_extract(Description, '(BLACK WHITE RED GREEN BLUE)', 1),\nDescription\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":221},{"cell_type":"code","source":["%scala\nval containsBlack = col(\"Description\").contains(\"BLACK\")\nval containsWhite = col(\"DESCRIPTION\").contains(\"WHITE\")\ndf.withColumn(\"hasSimpleColor\", containsBlack.or(containsWhite))\n.filter(\"hasSimpleColor\")\n.select(\"Description\")\n.show(3, false)"],"metadata":{},"outputs":[],"execution_count":222},{"cell_type":"code","source":["from pyspark.sql.functions import instr\ncontainsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\ncontainsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\ndf.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n.filter(\"hasSimpleColor\")\\\n.select(\"Description\")\\\n.show(3, False)"],"metadata":{},"outputs":[],"execution_count":223},{"cell_type":"code","source":["%sql\nSELECT\nDescription\nFROM\ndfTable\nWHERE\ninstr(Description, 'BLACK') >= 1 OR\ninstr(Description, 'WHITE') >= 1"],"metadata":{},"outputs":[],"execution_count":224},{"cell_type":"code","source":["%scala\nval simpleColors = Seq(\"black\", \"white\", \"red\", \"green\", \"blue\")\nval selectedColumns = simpleColors.map(color => {\ncol(\"Description\")\n.contains(color.toUpperCase)\n.alias(s\"is_$color\")\n}):+expr(\"*\") // could also append this value\ndf\n.select(selectedColumns:_*)\n.where(col(\"is_white\").or(col(\"is_red\")))\n.select(\"Description\")\n.show(3, false)"],"metadata":{},"outputs":[],"execution_count":225},{"cell_type":"code","source":["from pyspark.sql.functions import expr, locate\nsimpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\ndef color_locator(column, color_string):\n  return locate(color_string.upper(), column)\\\n.cast(\"boolean\")\\\n.alias(\"is_\" + c)\nselectedColumns = [color_locator(df.Description, c) for c in simpleColors]\nselectedColumns.append(expr(\"*\")) # has to a be Column type\ndf\\\n.select(*selectedColumns)\\\n.where(expr(\"is_white OR is_red\"))\\\n.select(\"Description\")\\\n.show(3, False)"],"metadata":{},"outputs":[],"execution_count":226},{"cell_type":"code","source":["\ndf.printSchema()"],"metadata":{},"outputs":[],"execution_count":227},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{current_date, current_timestamp}\nval dateDF = spark.range(10)\n.withColumn(\"today\", current_date())\n.withColumn(\"now\", current_timestamp())\ndateDF.createOrReplaceTempView(\"dateTable\")"],"metadata":{},"outputs":[],"execution_count":228},{"cell_type":"code","source":["from pyspark.sql.functions import current_date, current_timestamp\ndateDF = spark.range(10)\\\n.withColumn(\"today\", current_date())\\\n.withColumn(\"now\", current_timestamp())\ndateDF.createOrReplaceTempView(\"dateTable\")\ndateDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":229},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{date_add, date_sub}\ndateDF\n.select(\ndate_sub(col(\"today\"), 5),\ndate_add(col(\"today\"), 5))\n.show(1)"],"metadata":{},"outputs":[],"execution_count":230},{"cell_type":"code","source":["from pyspark.sql.functions import date_add, date_sub\ndateDF\\\n.select(\ndate_sub(col(\"today\"), 5),\ndate_add(col(\"today\"), 5))\\\n.show(1)"],"metadata":{},"outputs":[],"execution_count":231},{"cell_type":"code","source":["%sql\nSELECT\ndate_sub(today, 5),\ndate_add(today, 5)\nFROM\ndateTable"],"metadata":{},"outputs":[],"execution_count":232},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{datediff, months_between, to_date}\ndateDF\n.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\n.select(datediff(col(\"week_ago\"), col(\"today\")))\n.show(1)\ndateDF\n.select(\n  to_date(lit(\"2016-01-01\")).alias(\"start\"),\nto_date(lit(\"2017-05-22\")).alias(\"end\"))\n.select(months_between(col(\"start\"), col(\"end\")))\n.show(1)"],"metadata":{},"outputs":[],"execution_count":233},{"cell_type":"code","source":["from pyspark.sql.functions import datediff, months_between, to_date\ndateDF\\\n.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n.select(datediff(col(\"week_ago\"), col(\"today\")))\\\n.show(1)\ndateDF\\\n.select(\nto_date(lit(\"2016-01-01\")).alias(\"start\"),\nto_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n.select(months_between(col(\"start\"), col(\"end\")))\\\n.show(1)"],"metadata":{},"outputs":[],"execution_count":234},{"cell_type":"code","source":["%sql\nSELECT\nto_date('2016-01-01'),\nmonths_between('2016-01-01', '2017-01-01'),\ndatediff('2016-01-01', '2017-01-01')\nFROM\ndateTable"],"metadata":{},"outputs":[],"execution_count":235},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{to_date, lit}\nspark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\n.select(to_date(col(\"date\")))\n.show(1)"],"metadata":{},"outputs":[],"execution_count":236},{"cell_type":"code","source":["%python\nfrom pyspark.sql.functions import to_date, lit\nspark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n.select(to_date(col(\"date\")))\\\n.show(1)"],"metadata":{},"outputs":[],"execution_count":237},{"cell_type":"code","source":["dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)"],"metadata":{},"outputs":[],"execution_count":238},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{unix_timestamp, from_unixtime}\nval dateFormat = \"yyyy-dd-MM\"\nval cleanDateDF = spark.range(1)\n.select(\nto_date(unix_timestamp(lit(\"2017-12-11\"), dateFormat).cast(\"timestamp\"))\n.alias(\"date\"),\nto_date(unix_timestamp(lit(\"2017-20-12\"), dateFormat).cast(\"timestamp\"))\n.alias(\"date2\"))\ncleanDateDF.createOrReplaceTempView(\"dateTable2\")"],"metadata":{},"outputs":[],"execution_count":239},{"cell_type":"code","source":["from pyspark.sql.functions import unix_timestamp, from_unixtime\ndateFormat = \"yyyy-dd-MM\"\ncleanDateDF = spark.range(1)\\\n.select(\nto_date(unix_timestamp(lit(\"2017-12-11\"), dateFormat).cast(\"timestamp\"))\\\n.alias(\"date\"),\nto_date(unix_timestamp(lit(\"2017-20-12\"), dateFormat).cast(\"timestamp\"))\\\n.alias(\"date2\"))\ncleanDateDF.createOrReplaceTempView(\"dateTable2\")"],"metadata":{},"outputs":[],"execution_count":240},{"cell_type":"code","source":["%sql\nSELECT\nto_date(cast(unix_timestamp(date, 'yyyy-dd-MM') as timestamp)),\nto_date(cast(unix_timestamp(date2, 'yyyy-dd-MM') as timestamp)),\nto_date(date)\nFROM\ndateTable2"],"metadata":{},"outputs":[],"execution_count":241},{"cell_type":"code","source":["%scala\ncleanDateDF\n.select(\nunix_timestamp(col(\"date\"), dateFormat).cast(\"timestamp\"))\n.show()"],"metadata":{},"outputs":[],"execution_count":242},{"cell_type":"code","source":["cleanDateDF\\\n.select(\nunix_timestamp(col(\"date\"), dateFormat).cast(\"timestamp\"))\\\n.show()"],"metadata":{},"outputs":[],"execution_count":243},{"cell_type":"code","source":["cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()"],"metadata":{},"outputs":[],"execution_count":244},{"cell_type":"code","source":["df.na.drop()\ndf.na.drop(\"any\")"],"metadata":{},"outputs":[],"execution_count":245},{"cell_type":"code","source":["%sql\nSELECT\n*\nFROM\ndfTable\nWHERE\nDescription IS NOT NULL"],"metadata":{},"outputs":[],"execution_count":246},{"cell_type":"code","source":["df.na.drop(\"all\")"],"metadata":{},"outputs":[],"execution_count":247},{"cell_type":"code","source":["%scala\ndf.na.drop(\"all\", Seq(\"StockCode\", \"InvoiceNo\"))"],"metadata":{},"outputs":[],"execution_count":248},{"cell_type":"code","source":["df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"],"metadata":{},"outputs":[],"execution_count":249},{"cell_type":"code","source":["df.na.fill(\"All Null values become this string\")"],"metadata":{},"outputs":[],"execution_count":250},{"cell_type":"code","source":["%scala\ndf.na.fill(5, Seq(\"StockCode\", \"InvoiceNo\"))"],"metadata":{},"outputs":[],"execution_count":251},{"cell_type":"code","source":["df.na.fill(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"],"metadata":{},"outputs":[],"execution_count":252},{"cell_type":"code","source":["%scala\nval fillColValues = Map(\n\"StockCode\" -> 5,\n\"Description\" -> \"No Value\"\n)\ndf.na.fill(fillColValues)"],"metadata":{},"outputs":[],"execution_count":253},{"cell_type":"code","source":["fill_cols_vals = {\n\"StockCode\": 5,\n\"Description\" : \"No Value\"\n}\ndf.na.fill(fill_cols_vals)"],"metadata":{},"outputs":[],"execution_count":254},{"cell_type":"code","source":["%scala\ndf.na.replace(\"Description\", Map(\"\" -> \"UNKNOWN\"))"],"metadata":{},"outputs":[],"execution_count":255},{"cell_type":"code","source":["df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")"],"metadata":{},"outputs":[],"execution_count":256},{"cell_type":"code","source":["df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\")\ndf.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\")"],"metadata":{},"outputs":[],"execution_count":257},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.struct\nval complexDF = df\n.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\ncomplexDF.createOrReplaceTempView(\"complexDF\")"],"metadata":{},"outputs":[],"execution_count":258},{"cell_type":"code","source":["from pyspark.sql.functions import struct\ncomplexDF = df\\\n.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\ncomplexDF.createOrReplaceTempView(\"complexDF\")"],"metadata":{},"outputs":[],"execution_count":259},{"cell_type":"code","source":["complexDF.select(\"complex.Description\")"],"metadata":{},"outputs":[],"execution_count":260},{"cell_type":"code","source":["complexDF.select(\"complex.*\")"],"metadata":{},"outputs":[],"execution_count":261},{"cell_type":"code","source":["%sql\nSELECT\ncomplex.*\nFROM\ncomplexDF"],"metadata":{},"outputs":[],"execution_count":262},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.split\ndf.select(split(col(\"Description\"), \" \")).show(2)"],"metadata":{},"outputs":[],"execution_count":263},{"cell_type":"code","source":["from pyspark.sql.functions import split\ndf.select(split(col(\"Description\"), \" \")).show(2)"],"metadata":{},"outputs":[],"execution_count":264},{"cell_type":"code","source":["%sql\nSELECT\nsplit(Description, ' ')\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":265},{"cell_type":"code","source":["%scala\ndf.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\n.selectExpr(\"array_col[0]\")\n.show(2)"],"metadata":{},"outputs":[],"execution_count":266},{"cell_type":"code","source":["df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n.selectExpr(\"array_col[0]\")\\\n.show(2)"],"metadata":{},"outputs":[],"execution_count":267},{"cell_type":"code","source":["%sql\nSELECT\nsplit(Description, ' ')[0]\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":268},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.array_contains\ndf.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"],"metadata":{},"outputs":[],"execution_count":269},{"cell_type":"code","source":["from pyspark.sql.functions import array_contains\ndf.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"],"metadata":{},"outputs":[],"execution_count":270},{"cell_type":"code","source":["%sql\nSELECT\narray_contains(split(Description, ' '), 'WHITE')\nFROM\ndfTable"],"metadata":{},"outputs":[],"execution_count":271},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{split, explode}\ndf.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\n.withColumn(\"exploded\", explode(col(\"splitted\")))\n.select(\"Description\", \"InvoiceNo\", \"exploded\")"],"metadata":{},"outputs":[],"execution_count":272},{"cell_type":"code","source":["from pyspark.sql.functions import split, explode\ndf.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n.withColumn(\"exploded\", explode(col(\"splitted\")))\\\n.select(\"Description\", \"InvoiceNo\", \"exploded\")"],"metadata":{},"outputs":[],"execution_count":273},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.map\ndf.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n.selectExpr(\"complex_map['Description']\")"],"metadata":{},"outputs":[],"execution_count":274},{"cell_type":"code","source":["%sql\nSELECT\nmap(Description, InvoiceNo) as complex_map\nFROM\ndfTable\nWHERE\nDescription IS NOT NULL"],"metadata":{},"outputs":[],"execution_count":275},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.map\ndf.select(map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\n.selectExpr(\"explode(complex_map)\")\n.take(5)"],"metadata":{},"outputs":[],"execution_count":276},{"cell_type":"code","source":["%scala\nval jsonDF = spark.range(1)\n.selectExpr(\"\"\"\n'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":277},{"cell_type":"code","source":["jsonDF = spark.range(1)\\\n.selectExpr(\"\"\"\n'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":278},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.{get_json_object, json_tuple}\njsonDF.select(\nget_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\"),\njson_tuple(col(\"jsonString\"), \"myJSONKey\"))\n.show()"],"metadata":{},"outputs":[],"execution_count":279},{"cell_type":"code","source":["from pyspark.sql.functions import get_json_object, json_tuple\njsonDF.select(\nget_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\"),\njson_tuple(col(\"jsonString\"), \"myJSONKey\"))\\\n.show()"],"metadata":{},"outputs":[],"execution_count":280},{"cell_type":"code","source":["jsonDF.selectExpr(\"json_tuple(jsonString, '$.myJSONKey.myJSONValue[1]') as res\")"],"metadata":{},"outputs":[],"execution_count":281},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.to_json\ndf.selectExpr(\"(InvoiceNo, Description) as myStruct\")\n.select(to_json(col(\"myStruct\")))"],"metadata":{},"outputs":[],"execution_count":282},{"cell_type":"code","source":["from pyspark.sql.functions import to_json\ndf.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n.select(to_json(col(\"myStruct\")))"],"metadata":{},"outputs":[],"execution_count":283},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.from_json\nimport org.apache.spark.sql.types._\nval parseSchema = new StructType(Array(\nnew StructField(\"InvoiceNo\",StringType,true),\nnew StructField(\"Description\",StringType,true)))\ndf.selectExpr(\"(InvoiceNo, Description) as myStruct\")\n.select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\n.select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\"))"],"metadata":{},"outputs":[],"execution_count":284},{"cell_type":"code","source":["from pyspark.sql.functions import from_json\nfrom pyspark.sql.types import *\nparseSchema = StructType((\nStructField(\"InvoiceNo\",StringType(),True),\nStructField(\"Description\",StringType(),True)))\ndf.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n.select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n.select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\"))\\"],"metadata":{},"outputs":[],"execution_count":285},{"cell_type":"code","source":["%scala\nval udfExampleDF = spark.range(5).toDF(\"num\")\ndef power3(number:Double):Double = {\nnumber * number * number\n}\npower3(2.0)"],"metadata":{},"outputs":[],"execution_count":286},{"cell_type":"code","source":["udfExampleDF = spark.range(5).toDF(\"num\")\ndef power3(double_value):\n  return double_value ** 3\npower3(2.0)"],"metadata":{},"outputs":[],"execution_count":287},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.udf\nval power3udf = udf(power3(_:Double):Double)\n"],"metadata":{},"outputs":[],"execution_count":288},{"cell_type":"code","source":["%scala\nudfExampleDF.select(power3udf(col(\"num\"))).show()"],"metadata":{},"outputs":[],"execution_count":289},{"cell_type":"code","source":["from pyspark.sql.functions import udf\npower3udf = udf(power3)"],"metadata":{},"outputs":[],"execution_count":290},{"cell_type":"code","source":["from pyspark.sql.functions import col\nudfExampleDF.select(power3udf(col(\"num\"))).show()"],"metadata":{},"outputs":[],"execution_count":291},{"cell_type":"code","source":["%scala\nspark.udf.register(\"power3\", power3(_:Double):Double)\nudfExampleDF.selectExpr(\"power3(num)\").show()"],"metadata":{},"outputs":[],"execution_count":292},{"cell_type":"code","source":["udfExampleDF.selectExpr(\"power3(num)\").show()"],"metadata":{},"outputs":[],"execution_count":293},{"cell_type":"code","source":["%python\nfrom pyspark.sql.types import IntegerType, DoubleType\nspark.udf.register(\"power3py\", power3, DoubleType())\nudfExampleDF.selectExpr(\"power3py(num)\").show()"],"metadata":{},"outputs":[],"execution_count":294},{"cell_type":"code","source":["%sql\nSELECT\npower3py(12), -- doesn’t work because of return type\npower3(12)"],"metadata":{},"outputs":[],"execution_count":295},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":296}],"metadata":{"name":"Definitive-Guide-Excerpts","notebookId":2060825016294018},"nbformat":4,"nbformat_minor":0}
